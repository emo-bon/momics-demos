{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parametrized co-occurrence networks from taxonomy\n",
    "1. Load data\n",
    "2. Remove high taxa (non-identified sequences)\n",
    "3. Pivot table from sampling events to taxa.\n",
    "4. Remove low abundance taxa\n",
    "5. Rarefy, or normalize\n",
    "6. Remove replicates\n",
    "7. Split to groups on chosen factor\n",
    "8. Calculate associations (Bray-curits dissimilarity, Spearmanâ€™s correlation, etc.)\n",
    "9. False discovery rate correction\n",
    "10. Build and analyse network per group"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform dependent part\n",
    "- Resolve platform setup\n",
    "- the difference to local imports should be resolved by setting the VRE packages well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import logging\n",
    "from IPython import get_ipython\n",
    "logger = logging.getLogger(name=\"Co-occurrence network analysis\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Setting Google colab, you will need a ngrok account to make the dashboard display over the tunnel. \\\n",
    "    https://ngrok.com/')\n",
    "    # clone the momics-demos repository to use it to load data\n",
    "    try:\n",
    "        os.system('git clone https://github.com/palec87/momics-demos.git')\n",
    "        logger.info(f\"Repository cloned\")\n",
    "    except OSError as e:\n",
    "        logger.info(f\"An error occurred while cloning the repository: {e}\")\n",
    "\n",
    "    sys.path.insert(0,'/content/momics-demos')\n",
    "\n",
    "    # this step takes time beacause of many dependencies\n",
    "    os.system('pip install marine-omics')\n",
    "\n",
    "from momics.utils import (\n",
    "    memory_load, reconfig_logger,\n",
    "    init_setup, get_notebook_environment,\n",
    ")\n",
    "\n",
    "# Set up logging\n",
    "reconfig_logger()\n",
    "\n",
    "# Determine the notebook environment\n",
    "env = get_notebook_environment()\n",
    "\n",
    "init_setup()\n",
    "logger.info(f\"Environment: {env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "import networkx as nx\n",
    "import holoviews as hv\n",
    "from typing import Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr, pearsonr\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "from skbio.stats import subsample_counts\n",
    "from skbio.diversity import beta_diversity\n",
    "\n",
    "from mgo.udal import UDAL\n",
    "\n",
    "# All low level functions are imported from the momics package\n",
    "from momics.loader import load_parquets_udal\n",
    "from momics.metadata import get_metadata_udal, enhance_metadata\n",
    "from momics.taxonomy import (\n",
    "    remove_high_taxa,\n",
    "    pivot_taxonomic_data,\n",
    "    prevalence_cutoff,\n",
    "    rarefy_table,\n",
    "    split_metadata,\n",
    "    split_taxonomic_data_pivoted,\n",
    "    fill_taxonomy_placeholders,\n",
    ")\n",
    "from momics.networks import interaction_to_graph, interaction_to_graph_with_pvals, pairwise_jaccard_lower_triangle\n",
    "\n",
    "import momics.plotting as pl\n",
    "from momics.panel_utils import serve_app, close_server\n",
    "from momics.loader import bytes_to_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True  # enable stdout logging\n",
    "global full_metadata\n",
    "udal = UDAL()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    root_folder = os.path.abspath(os.path.join('/content/momics-demos'))\n",
    "else:\n",
    "    root_folder = os.path.abspath(os.path.join('../'))\n",
    "\n",
    "\n",
    "assets_folder = os.path.join(root_folder, 'assets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.cache()\n",
    "def get_data():\n",
    "    return load_parquets_udal()\n",
    "\n",
    "# Load and merge metadata\n",
    "@pn.cache()\n",
    "def get_full_metadata():\n",
    "    return get_metadata_udal()\n",
    "\n",
    "@pn.cache()\n",
    "def get_valid_samples():\n",
    "    df_valid = pd.read_csv(\n",
    "        os.path.join(root_folder, 'data/shipment_b1b2_181.csv')\n",
    "    )\n",
    "    return df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "full_metadata = get_full_metadata()\n",
    "\n",
    "# filter the metadata only for valid 181 samples\n",
    "valid_samples = get_valid_samples()\n",
    "full_metadata = enhance_metadata(full_metadata, valid_samples)\n",
    "\n",
    "mgf_parquet_dfs = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert all object columns to categorical for metadata\n",
    "factors = []\n",
    "\n",
    "# Identify object columns\n",
    "object_cols = full_metadata.select_dtypes(include='object').columns\n",
    "\n",
    "to_remove = ['source_mat_id', 'source_mat_id_orig', 'samp_description',]\n",
    "object_cols = [col for col in object_cols if col not in to_remove]\n",
    "\n",
    "\n",
    "# Convert them all at once to category\n",
    "full_metadata = full_metadata.astype({col: 'category' for col in object_cols})\n",
    "\n",
    "# Track which were converted\n",
    "factors.extend(object_cols)\n",
    "\n",
    "if not isinstance(full_metadata['season'].dtype, pd.CategoricalDtype):\n",
    "        raise ValueError(f\"Column 'season' is not categorical (object dtype).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT_TAXONOMY = {}\n",
    "SPEARMAN_TAXA = {}\n",
    "TAXONOMY_RANKS = ['superkingdom', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Side panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension(\"tabulator\", \"mathjax\")\n",
    "\n",
    "ACCENT = \"teal\"\n",
    "\n",
    "styles = {\n",
    "    \"box-shadow\": \"rgba(50, 50, 93, 0.25) 0px 6px 12px -2px, rgba(0, 0, 0, 0.3) 0px 3px 7px -3px\",\n",
    "    \"border-radius\": \"4px\",\n",
    "    \"padding\": \"10px\",\n",
    "}\n",
    "\n",
    "md_prepare_table = pn.pane.Markdown(\n",
    "\"\"\"\n",
    "**Button triggers:**\n",
    "- Removes high taxa\n",
    "- Removes low prevalence taxa\n",
    "- Rarefies/normalizes the table\n",
    "- Removes replicates\n",
    "- split to group by factor\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "tables = {\n",
    "    \"LSU\": mgf_parquet_dfs['lsu'].copy(),\n",
    "    \"SSU\": mgf_parquet_dfs['ssu'].copy(),\n",
    "}\n",
    "\n",
    "del mgf_parquet_dfs\n",
    "\n",
    "select_table = pn.widgets.Select(\n",
    "    name=\"Select table\",\n",
    "    options= list(tables.keys()),\n",
    "    description=\"Select a table for network analysis\",\n",
    ")\n",
    "\n",
    "select_factor = pn.widgets.Select(\n",
    "        name=\"Select factor\",\n",
    "        options=factors,\n",
    ")\n",
    "\n",
    "select_high_taxon = pn.widgets.Select(\n",
    "    name=\"Select high taxa to remove\",\n",
    "    options=['None', 'phylum', 'class', 'order', 'family', 'genus'],\n",
    "    value='phylum',\n",
    "    description=\"Taxa identified at this level or higher will be removed from the analysis\",\n",
    ")\n",
    "\n",
    "mapping = pn.widgets.Checkbox(\n",
    "    name=\"strict mapping to selected taxonomic level (takes time)\",\n",
    "    value=True,\n",
    ")\n",
    "\n",
    "low_prevalence_cutoff = pn.widgets.FloatInput(\n",
    "    name='Low prevalence cutoff [%]',\n",
    "    value=10, step=1, start=0, end=100,\n",
    "    description=\"Percentage of samples in which the taxon must be present not to be removed.\",\n",
    ")\n",
    "\n",
    "button_prepare_table = pn.widgets.Button(\n",
    "    name=\"Process taxonomy\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_taxonomy(table, factor, high_taxon, mapping, prevalence_cutoff_value):\n",
    "    \"\"\"\n",
    "    Preprocess the taxonomy data.\n",
    "    \"\"\"\n",
    "    global SPLIT_TAXONOMY\n",
    "    SPLIT_TAXONOMY.clear()\n",
    "    df_filt = tables[table]\n",
    "\n",
    "    df_filt = fill_taxonomy_placeholders(df_filt, TAXONOMY_RANKS)\n",
    "\n",
    "    logger.info(\"Preprocessing taxonomy...\")\n",
    "    if high_taxon != 'None':\n",
    "        bef = df_filt.shape[0]\n",
    "        df_filt = remove_high_taxa(df_filt, TAXONOMY_RANKS, tax_level=high_taxon, strict=mapping)\n",
    "        aft = df_filt.shape[0]\n",
    "        logger.info(f\"Removed {bef - aft} high taxa at level: {high_taxon}\")\n",
    "    \n",
    "    # pivot table\n",
    "    df_filt = pivot_taxonomic_data(df_filt, normalize=None, rarefy_depth=None)\n",
    "\n",
    "    # low prevalence cutoff\n",
    "    df_filt = prevalence_cutoff(df_filt, percent=prevalence_cutoff_value, skip_columns=2)\n",
    "\n",
    "    # rarefy table\n",
    "    df_rarefied = df_filt.copy()\n",
    "    df_rarefied.iloc[:, 2:] = rarefy_table(df_filt.iloc[:, 2:], depth=None, axis=1)\n",
    "\n",
    "    # process metadata\n",
    "    metadata = full_metadata.copy()\n",
    "    filtered_metadata = metadata.drop_duplicates(subset='replicate_info', keep='first')\n",
    "\n",
    "    groups = split_metadata(\n",
    "        filtered_metadata,\n",
    "        factor,\n",
    "    )\n",
    "    # remove groups which have less than 2 members (bad for your statistics :)\n",
    "    for groups_key in list(groups.keys()):\n",
    "        print(f\"{groups_key}: {len(groups[groups_key])} samples\")\n",
    "        if len(groups[groups_key]) < 3:\n",
    "            del groups[groups_key]\n",
    "            print(f\"Warning: {groups_key} has less than 3 samples, therefore removed.\")\n",
    "\n",
    "    SPLIT_TAXONOMY = split_taxonomic_data_pivoted(\n",
    "        df_rarefied,\n",
    "        groups\n",
    "    )\n",
    "\n",
    "\n",
    "button_prepare_table.on_click(\n",
    "    lambda event: preprocess_taxonomy(\n",
    "        select_table.value,\n",
    "        select_factor.value,\n",
    "        select_high_taxon.value,\n",
    "        mapping.value,\n",
    "        low_prevalence_cutoff.value\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Association tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_associations = pn.pane.Markdown(\n",
    "\"\"\"\n",
    "**General hints:**\n",
    "- Calculate associations between the selected factor and the taxonomic data.\n",
    "- Perform FDR correction on the p-values.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "hist_fdr = pn.pane.HoloViews(\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    name=\"Associations visualization\",\n",
    "    )\n",
    "\n",
    "viz_tab = pn.Column(\n",
    "    hist_fdr,\n",
    ")\n",
    "\n",
    "pval_cutoff = pn.widgets.FloatInput(\n",
    "    name='P-value cutoff',\n",
    "    value=0.05, step=0.01, start=0, end=1,\n",
    "    description=\"P-value cutoff to identify significant associations.\",\n",
    ")\n",
    "\n",
    "histogram_plot = pn.pane.HoloViews(\n",
    "    height=500,\n",
    "    name=\"Histogram\",\n",
    ")\n",
    "\n",
    "fdr_plot = pn.pane.HoloViews(\n",
    "    height=500,\n",
    "    name=\"FDR Plot\",\n",
    ")\n",
    "\n",
    "button_associations = pn.widgets.Button(\n",
    "    name=\"Calculate associations\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this has to be updated in the momics-methods\n",
    "def fdr_pvals(p_spearman_df: pd.DataFrame, pval_cutoff: float) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Apply FDR correction to the p-values DataFrame using Benjamini/Hochberg (non-negative)\n",
    "    method. This function extracts the upper triangle of the p-values DataFrame.\n",
    "\n",
    "    Args:\n",
    "        p_spearman_df (pd.DataFrame): DataFrame containing p-values.\n",
    "        pval_cutoff (float): P-value cutoff for FDR correction.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with FDR corrected p-values.\n",
    "    \"\"\"\n",
    "    # Extract upper triangle p-values\n",
    "    pval_array = (\n",
    "        p_spearman_df.where(np.triu(np.ones(p_spearman_df.shape), k=1).astype(bool))\n",
    "        .stack()\n",
    "        .values\n",
    "    )\n",
    "\n",
    "    # Apply FDR correction\n",
    "    _rejected, pvals_corrected, _, _ = multipletests(\n",
    "        pval_array, alpha=pval_cutoff, method=\"fdr_bh\"\n",
    "    )\n",
    "    logger.info(f\"FDR corrected p-values: {pvals_corrected.shape}, {p_spearman_df.shape}\")\n",
    "\n",
    "    # Map corrected p-values back to a DataFrame\n",
    "    pvals_fdr = p_spearman_df.copy()\n",
    "    pvals_fdr.values[np.triu_indices_from(p_spearman_df, k=1)] = pvals_corrected\n",
    "    pvals_fdr.values[np.tril_indices_from(p_spearman_df, k=0)] = (\n",
    "        np.nan\n",
    "    )  # Optional: keep only upper triangle\n",
    "    logger.info(f\"FDR corrected p-values DataFrame shape: {pvals_fdr.shape}\")\n",
    "    return pvals_fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panel.widgets import Tqdm\n",
    "\n",
    "tqdm1 = Tqdm()\n",
    "\n",
    "def values_below_diagonal_series(df: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Extract values under the main diagonal from a square DataFrame\n",
    "    and return them as a flattened pandas Series.\n",
    "    \"\"\"\n",
    "    idx = np.tril_indices_from(df, k=-1)\n",
    "    return pd.Series(df.values[idx])\n",
    "\n",
    "\n",
    "def plot_associations(pval_cutoff: float):\n",
    "    hists = []\n",
    "    for factor, d in SPEARMAN_TAXA.items():\n",
    "        values = pd.Series(d['correlation'].values.flatten())\n",
    "        logger.info(f\"Plotting histogram for factor: {factor} with {len(values)} values\")\n",
    "        hist = values.hvplot.hist(\n",
    "            bins=50,\n",
    "            alpha=0.5,\n",
    "            label=factor,\n",
    "            xlabel=\"Correlation\",\n",
    "            ylabel=\"Frequency\",\n",
    "            title=\"Histogram of Correlation Values\"\n",
    "        )\n",
    "        hists.append(hist)\n",
    "    # Overlay all histograms\n",
    "    histograms = hists[0]\n",
    "    for h in hists[1:]:\n",
    "        histograms *= h\n",
    "    histogram_plot.object = histograms.opts(\n",
    "        show_legend=True,\n",
    "        legend_position='top_left',\n",
    "    )\n",
    "\n",
    "    fdrs = []\n",
    "    for factor, d in SPEARMAN_TAXA.items():\n",
    "        if d['p_vals_fdr'] is None:\n",
    "            logger.warning(f\"Skipping factor {factor} due to missing FDR p-values.\")\n",
    "            continue\n",
    "        df_pvals = pd.DataFrame({\n",
    "            'raw_pval': SPEARMAN_TAXA[factor]['p_vals'].values.flatten()[::10],  # downsample for better visibility and speed\n",
    "            'fdr_pval': SPEARMAN_TAXA[factor]['p_vals_fdr'].values.flatten()[::10],  # downsample for better visibility and speed\n",
    "        })\n",
    "\n",
    "        fdr_scatter = df_pvals.hvplot.scatter(\n",
    "            x='raw_pval',\n",
    "            y='fdr_pval',\n",
    "            alpha=0.5,\n",
    "            label=factor,\n",
    "            xlabel=\"Raw p-value\",\n",
    "            ylabel=\"FDR-corrected p-value\",\n",
    "        )\n",
    "        fdrs.append(fdr_scatter)\n",
    "    \n",
    "    # Overlay all scatter plots\n",
    "    fdr_scatter = fdrs[0]\n",
    "    for f in fdrs[1:]:\n",
    "        fdr_scatter *= f\n",
    "    # Add horizontal and vertical lines at pval_cutoff\n",
    "    hline = hv.HLine(pval_cutoff).opts(color='black', line_dash='dashed', line_width=2)\n",
    "    vline = hv.VLine(pval_cutoff).opts(color='gray', line_dash='dashed', line_width=2)\n",
    "    \n",
    "    # Overlay the lines on the scatter plot\n",
    "    fdr_plot.object = (fdr_scatter * hline * vline).opts(\n",
    "        show_legend=True,\n",
    "        legend_position='bottom_right',\n",
    "    )\n",
    "\n",
    "\n",
    "def calculate_associations(pval_cutoff):\n",
    "    global SPEARMAN_TAXA\n",
    "    SPEARMAN_TAXA.clear()\n",
    "    global SPLIT_TAXONOMY\n",
    "    logger.info(\"Calculating associations...\")\n",
    "    logger.info(f\"number of items: {len(SPLIT_TAXONOMY.items())}\")\n",
    "    tqdm1.reset()\n",
    "    count = 0\n",
    "    for factor, df in tqdm1(SPLIT_TAXONOMY.items()):\n",
    "        corr, p_spearman = spearmanr(df.iloc[:, 2:].T)\n",
    "        assert corr.shape == p_spearman.shape, \"Spearman correlation and p-values must have the same shape.\"\n",
    "        corr_df = pd.DataFrame(\n",
    "            corr,\n",
    "            index=df['ncbi_tax_id'],\n",
    "            columns=df['ncbi_tax_id']\n",
    "        )\n",
    "        logger.info(f\"{corr_df.shape}\")\n",
    "        p_spearman_df = pd.DataFrame(\n",
    "            p_spearman,\n",
    "            index=df['ncbi_tax_id'],\n",
    "            columns=df['ncbi_tax_id']\n",
    "        )\n",
    "        d = {\n",
    "            'correlation': corr_df,\n",
    "            'p_vals': p_spearman_df\n",
    "        }\n",
    "        SPEARMAN_TAXA[factor] = d\n",
    "        assert SPEARMAN_TAXA[factor]['correlation'].shape == SPEARMAN_TAXA[factor]['p_vals'].shape, \"Spearman correlation and p-values must have the same shape.\"\n",
    "        # FDR correction\n",
    "        try:\n",
    "            pvals_fdr = fdr_pvals(SPEARMAN_TAXA[factor]['p_vals'], pval_cutoff=pval_cutoff)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Error occurred while calculating FDR p-values for factor: {factor}. Error: {e}\")\n",
    "            SPEARMAN_TAXA[factor]['p_vals_fdr'] = None\n",
    "            count += 1\n",
    "            tqdm1.update(count)\n",
    "            continue\n",
    "        SPEARMAN_TAXA[factor]['p_vals_fdr'] = pvals_fdr\n",
    "        count += 1\n",
    "        tqdm1.value = count\n",
    "\n",
    "    # plot associations\n",
    "    logger.info(\"Plotting associations...\")\n",
    "    plot_associations(pval_cutoff)\n",
    "\n",
    "\n",
    "def run_tqdm_loop(*events):\n",
    "    \"\"\"\n",
    "    Run a loop with tqdm progress bar.\n",
    "    \"\"\"\n",
    "    calculate_associations(pval_cutoff.value)\n",
    "\n",
    "button_associations.on_click(\n",
    "    lambda event: calculate_associations(\n",
    "        pval_cutoff.value\n",
    "    )\n",
    "    # lambda event: run_tqdm_loop()\n",
    "    # run_tqdm_loop\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_tab = pn.Column(\n",
    "    md_associations,\n",
    "    pval_cutoff,\n",
    "    pn.Row(\n",
    "        button_associations,\n",
    "        tqdm1,\n",
    "    ),\n",
    "    pn.Row(\n",
    "        histogram_plot,\n",
    "        fdr_plot,\n",
    "    ),\n",
    "    scroll=True,\n",
    "    sizing_mode=\"stretch_both\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_network = pn.pane.Markdown(\n",
    "\"\"\"\n",
    "**General hints:**\n",
    "- Same p-value will be used as in previous section.\n",
    "- You select the thresholds of the positive and negative associations, respectively.\n",
    "- To add the edge to the network, the absolute value of the correlation must be above/below the threshold and p-value needs to be lower than the cutoff.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "pos_corr_cutoff = pn.widgets.FloatInput(\n",
    "    name='Positive Correlation cutoff',\n",
    "    value=0.75, step=0.05, start=0, end=1,\n",
    "    description=\"Significant Positive Correlation Cutoff\",\n",
    ")\n",
    "\n",
    "neg_corr_cutoff = pn.widgets.FloatInput(\n",
    "    name='Negative Correlation cutoff',\n",
    "    value=-0.70, step=0.05, start=-1, end=0,\n",
    "    description=\"Significant Negative Correlation Cutoff\",\n",
    ")\n",
    "\n",
    "button_network = pn.widgets.Button(\n",
    "    name=\"Create and evaluate network\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    ")\n",
    "\n",
    "overall_network_df = pn.widgets.Tabulator()\n",
    "jaccard_pos = pn.widgets.Tabulator()\n",
    "jaccard_neg = pn.widgets.Tabulator()\n",
    "\n",
    "network_plot = pn.pane.Matplotlib(\n",
    "    name=\"Network plot\",\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    sizing_mode=\"stretch_both\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pairwise Jaccard similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_jaccard_lower_triangle(network_results, edge_type='edges_pos'):\n",
    "    \"\"\"\n",
    "    Calculate pairwise Jaccard similarity for the lower triangle of all group comparisons.\n",
    "    Returns a DataFrame with columns: group1, group2, jaccard_similarity.\n",
    "    \"\"\"\n",
    "    # Extract all group names\n",
    "    groups = list(network_results.keys())\n",
    "\n",
    "    # define empty DataFrame with groups as index and columns\n",
    "    jaccard_df = pd.DataFrame(index=groups, columns=groups)\n",
    "    # Iterate over all unique pairs (lower triangle, i < j)\n",
    "    for g1, g2 in itertools.combinations(groups, 2):\n",
    "        edges1 = set(network_results[g1][edge_type])\n",
    "        edges2 = set(network_results[g2][edge_type])\n",
    "\n",
    "        intersection = edges1 & edges2\n",
    "        union = edges1 | edges2\n",
    "\n",
    "        jaccard = len(intersection) / len(union) if len(union) > 0 else float('nan')\n",
    "        jaccard_df.loc[g2, g1] = round(jaccard, 4)\n",
    "    return jaccard_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "tqdm2 = Tqdm()\n",
    "\n",
    "def calculate_network(pos_corr_cutoff, neg_corr_cutoff, pval_cutoff):\n",
    "    global SPLIT_TAXONOMY\n",
    "    global SPEARMAN_TAXA\n",
    "    global full_metadata\n",
    "\n",
    "    network_results = {}\n",
    "    count = 0\n",
    "    tqdm2.reset()\n",
    "    for factor, dict_df in tqdm2(SPEARMAN_TAXA.items()):\n",
    "        if dict_df['p_vals_fdr'] is None:\n",
    "            logger.warning(f\"Skipping factor {factor} due to missing FDR p-values.\")\n",
    "            continue\n",
    "        logger.info(f\"Calculating network for factor: {factor}\")\n",
    "        start_time = time.time()\n",
    "        nodes, edges_pos, edges_neg = interaction_to_graph_with_pvals(\n",
    "            dict_df['correlation'],\n",
    "            dict_df['p_vals_fdr'],\n",
    "            pos_cutoff=pos_corr_cutoff,\n",
    "            neg_cutoff=neg_corr_cutoff,\n",
    "            p_val_cutoff=pval_cutoff,\n",
    "        )\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.info(f\"Network calculation for factor {factor} took {elapsed_time:.2f} seconds.\")\n",
    "        logger.info(f\"Number of nodes: {len(nodes)}\")\n",
    "        logger.info(f\"Number of positive edges: {len(edges_pos)}\")\n",
    "        logger.info(f\"Number of negative edges: {len(edges_neg)}\")\n",
    "        G = nx.Graph(\n",
    "            mode = factor,\n",
    "        )\n",
    "\n",
    "        G.add_nodes_from(nodes)\n",
    "        G.add_edges_from(edges_pos, color='green')\n",
    "        G.add_edges_from(edges_neg, color='red')\n",
    "\n",
    "        network_results[factor] = {\n",
    "            \"graph\": G,\n",
    "            \"nodes\": nodes,\n",
    "            \"edges_pos\": edges_pos,\n",
    "            \"edges_neg\": edges_neg\n",
    "        }\n",
    "\n",
    "        degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "        network_results[factor]['degree_centrality'] = sorted(degree_centrality.items(),\n",
    "                                                            key=lambda x: x[1],\n",
    "                                                            reverse=True)[:10]\n",
    "        \n",
    "        betweenness = nx.betweenness_centrality(G)\n",
    "        network_results[factor]['top_betweenness'] = sorted(betweenness.items(),\n",
    "                                                        key=lambda x: x[1],\n",
    "                                                        reverse=True)[:10]\n",
    "        network_results[factor]['bottom_betweenness'] = sorted(betweenness.items(),\n",
    "                                                            key=lambda x: x[1])[:10]\n",
    "        network_results[factor]['total_nodes'] = G.number_of_nodes()\n",
    "        network_results[factor]['total_edges'] = G.number_of_edges()\n",
    "        count += 1\n",
    "        tqdm2.value = count\n",
    "\n",
    "    DF = pd.DataFrame(columns=[select_factor.value, 'centrality', 'top_betweenness', 'bottom_betweenness', 'total_nodes', 'total_edges'])\n",
    "    factors = []\n",
    "    for factor, dict_results in network_results.items():\n",
    "        DF = pd.concat([DF, pd.DataFrame([{\n",
    "            select_factor.value: factor,\n",
    "            'centrality': dict_results['degree_centrality'],\n",
    "            'top_betweenness': dict_results['top_betweenness'],\n",
    "            'bottom_betweenness': dict_results['bottom_betweenness'],\n",
    "            'total_nodes': dict_results['total_nodes'],\n",
    "            'total_edges': dict_results['total_edges']\n",
    "        }])], ignore_index=True)\n",
    "        factors.append(factor)\n",
    "\n",
    "    overall_network_df.value = DF\n",
    "\n",
    "    # Calculate Jaccard similarity for the networks\n",
    "    jaccard_pos.value = pairwise_jaccard_lower_triangle(network_results, edge_type='edges_pos')\n",
    "    jaccard_neg.value = pairwise_jaccard_lower_triangle(network_results, edge_type='edges_neg')\n",
    "\n",
    "    # plot the networks\n",
    "    fig, axes = plt.subplots(1, len(factors), figsize=(6*len(factors), 6))\n",
    "\n",
    "    for ax, factor in zip(axes, factors):\n",
    "        G = network_results[factor]['graph']\n",
    "        colors = nx.get_edge_attributes(G, 'color')\n",
    "        pos = nx.spring_layout(G, k=0.2, iterations=50, seed=42)\n",
    "        nx.draw_networkx_nodes(G, pos, ax=ax, alpha=0.2, node_color='grey', node_size=15)\n",
    "        nx.draw_networkx_edges(G, pos, ax=ax, alpha=0.2, edge_color=list(colors.values()))\n",
    "        ax.set_title(factor)\n",
    "        ax.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.close(fig)\n",
    "    network_plot.object = fig\n",
    "\n",
    "button_network.on_click(\n",
    "    lambda event: calculate_network(\n",
    "        pos_corr_cutoff.value,\n",
    "        neg_corr_cutoff.value,\n",
    "        pval_cutoff.value\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "network_tab = pn.Column(\n",
    "    md_network,\n",
    "    pn.Row(\n",
    "        pos_corr_cutoff,\n",
    "        neg_corr_cutoff,\n",
    "        button_network,\n",
    "        tqdm1,\n",
    "    ),\n",
    "    overall_network_df,\n",
    "    pn.Row(\"## Jaccard similarity (positive)\", jaccard_pos),\n",
    "    pn.Row(\"## Jaccard similarity (negative)\", jaccard_neg),\n",
    "    network_plot,\n",
    "    scroll=True,\n",
    "    sizing_mode=\"stretch_both\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = pn.Tabs(\n",
    "    (\"Associations\", association_tab),\n",
    "    (\"Network analysis\", network_tab),\n",
    "    dynamic=True,\n",
    "    styles=styles,\n",
    "    sizing_mode=\"stretch_both\",\n",
    "    margin=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension(\"tabulator\", \"mathjax\")\n",
    "hv.extension(\"bokeh\", \"plotly\")\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    pn.extension(comms='colab')\n",
    "\n",
    "def app():\n",
    "    template = pn.template.FastListTemplate(\n",
    "        title=\"Taxonomic co-occurrence networks\",\n",
    "        sidebar=[md_prepare_table, \n",
    "                pn.layout.Divider(margin=(-20, 0, 0, 0)),\n",
    "                pn.Column(\n",
    "                    select_table,\n",
    "                    select_factor,\n",
    "                    select_high_taxon,\n",
    "                    mapping,\n",
    "                    low_prevalence_cutoff,\n",
    "                ),\n",
    "                button_prepare_table,\n",
    "                ],\n",
    "        main=[pn.Column(\n",
    "            tabs,\n",
    "            scroll=True,\n",
    "        )],\n",
    "        main_layout=None,\n",
    "        accent=ACCENT,\n",
    "    )\n",
    "    return template\n",
    "\n",
    "template = app()\n",
    "logger.info(\"Template created\")\n",
    "\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):  \n",
    "    s = serve_app(template, env=env, name=\"co_occurrence_networks\")\n",
    "else:\n",
    "    template.servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment this if running if running ngrok tunnel which you want to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use for the ngrok tunnel in GColab\n",
    "# close_server(s, env=env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momics-demos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
