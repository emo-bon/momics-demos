{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize taxonomy and alpha/beta diversities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform dependent part\n",
    "- Resolve platform setup\n",
    "- the difference to local imports should be resolved by setting the Blue Cloud VRE well, Colab will still be an issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "binder\n",
      "Platform: local Windows\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import io\n",
    "import gc\n",
    "import logging\n",
    "from IPython import get_ipython\n",
    "logger = logging.getLogger(name=\"Diversity analysis app\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # clone the momics-demos repository to use the utils module from there\n",
    "    # TODO: eventually utils from momics will be used for that\n",
    "    try:\n",
    "        os.system('git clone https://github.com/palec87/momics-demos.git')\n",
    "        logger.info(f\"Repository cloned\")\n",
    "    except OSError as e:\n",
    "        logger.info(f\"An error occurred while cloning the repository: {e}\")\n",
    "\n",
    "    sys.path.insert(0,'/content/momics-demos')\n",
    "elif \"zmqshell\" in str(get_ipython()):\n",
    "    logger.info(\"Binder\")\n",
    "    print('binder')\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "else:\n",
    "    logger.info(\"Local\")\n",
    "    print('local')\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # local utils, to be removed in the future\n",
    "\n",
    "from utils import init_setup, get_notebook_environment\n",
    "# Determine the notebook environment\n",
    "env = get_notebook_environment()\n",
    "\n",
    "init_setup()\n",
    "logger.info(f\"Environment: {env}\")\n",
    "\n",
    "# if path exists add sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../../marine-omics')))  # local momics package, to be removed too\n",
    "local_momics_path = os.path.abspath(os.path.join(os.getcwd(), '../../../marine-omics'))\n",
    "if os.path.exists(local_momics_path):\n",
    "    sys.path.append(local_momics_path)\n",
    "    logger.info(f\"Added local momics path: {local_momics_path}\")\n",
    "    print(f\"Added local momics path: {local_momics_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be repeated here for the Pannel dashboard to work, WEIRD\n",
    "# TODO: report as possible bug\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import warnings\n",
    "import psutil\n",
    "\n",
    "\n",
    "from functools import partial\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "import panel as pn\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "from skbio.diversity import beta_diversity\n",
    "from skbio.stats.ordination import pcoa\n",
    "\n",
    "# All low level functions are imported from the momics package\n",
    "from momics.loader import load_parquets, process_collection_date, extract_season\n",
    "import momics.plotting as pl\n",
    "from momics.panel_utils import (\n",
    "    diversity_select_widgets, create_indicators_diversity,\n",
    "    serve_app, close_server,\n",
    ")\n",
    "from momics.utils import memory_load, reconfig_logger\n",
    "from momics.taxonomy import (\n",
    "    pivot_taxonomic_data,\n",
    "    separate_taxonomy)\n",
    "\n",
    "# Note: This is breaking the panel preview functionality\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | root | Logging.basicConfig completed successfully\n"
     ]
    }
   ],
   "source": [
    "DEBUG = True  # enable stdout logging\n",
    "\n",
    "# Set up logging\n",
    "reconfig_logger()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_na_for_object_columns(df):\n",
    "    \"\"\"\n",
    "    Fill NA values with 'NA' for object columns in the dataframe.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input dataframe.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The dataframe with NA values filled for object columns.\n",
    "    \"\"\"\n",
    "    # Apply fillna only to object columns\n",
    "    df[df.select_dtypes(include=['object']).columns] = df.select_dtypes(include=['object']).apply(lambda col: col.fillna('NA'))\n",
    "    return df\n",
    "\n",
    "@pn.cache()\n",
    "def get_data(folder):\n",
    "    return load_parquets(folder)\n",
    "\n",
    "@pn.cache()\n",
    "def get_metadata(folder):\n",
    "    # Load metadata\n",
    "    sample_metadata = pd.read_csv(\n",
    "        os.path.join(folder, \"Batch1and2_combined_logsheets_2024-11-12.csv\")\n",
    "    )\n",
    "\n",
    "    observatory_metadata = pd.read_csv(\n",
    "        os.path.join(folder, \"Observatory_combined_logsheets_validated.csv\")\n",
    "    )\n",
    "\n",
    "    # Merge metadata\n",
    "    full_metadata = pd.merge(\n",
    "        sample_metadata,\n",
    "        observatory_metadata,\n",
    "        on=[\"obs_id\", \"env_package\"],  # Matching conditions\n",
    "        how=\"inner\"  # Inner join\n",
    "    )\n",
    "\n",
    "    # Sort the merged dataframe by 'ref_code' column in ascending order\n",
    "    full_metadata = full_metadata.sort_values(by=\"ref_code\", ascending=True)\n",
    "\n",
    "    # first convert some of the boolean cols\n",
    "    full_metadata[\"failure\"] = full_metadata[\"failure\"].astype(str)\n",
    "    # replace the 'nan' values with 'NA'\n",
    "    full_metadata[\"failure\"] = full_metadata[\"failure\"].replace(\"nan\", \"NA\")\n",
    "\n",
    "\n",
    "    # adding replacement for the missing values for object type columns\n",
    "    full_metadata = fill_na_for_object_columns(full_metadata)\n",
    "    \n",
    "    return full_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet files\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    root_folder = os.path.abspath(os.path.join('/content/momics-demos'))\n",
    "else:\n",
    "    root_folder = os.path.abspath(os.path.join('../'))\n",
    "\n",
    "\n",
    "data_folder = os.path.join(root_folder, 'data/parquet_files')\n",
    "assets_folder = os.path.join(root_folder, 'assets')\n",
    "\n",
    "\n",
    "mgf_parquet_dfs = get_data(data_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enhance matadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TODO: enhace metadata here\n",
    "# Load and merge metadata\n",
    "full_metadata = get_metadata(os.path.join(root_folder, 'data'))\n",
    "full_metadata = process_collection_date(full_metadata)\n",
    "full_metadata = extract_season(full_metadata)\n",
    "\n",
    "# fiter the metadata only for valid 181 samples\n",
    "df_valid = pd.read_csv(\n",
    "    os.path.join(root_folder, 'data/shipment_b1b2_181.csv')\n",
    ")\n",
    "\n",
    "# Filter the full_metadata on the 'ref_code' only for entries that are in df_valid\n",
    "full_metadata = full_metadata[full_metadata['ref_code'].isin(df_valid['ref_code'])]\n",
    "\n",
    "missing = df_valid[~df_valid['ref_code'].isin(full_metadata['ref_code'])]\n",
    "assert len(missing) == 0, \"Missing samples in the metadata\"\n",
    "assert len(full_metadata) == len(df_valid), \"Filtered metadata does not match the valid samples\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Diversity analysis app | Data table names are:\n",
      "dict_keys(['go', 'go_slim', 'ips', 'ko', 'LSU', 'pfam', 'SSU'])\n",
      "INFO | Diversity analysis app | Categorical metadata columns are:\n",
      "['ENA_accession_number_project', 'ENA_accession_number_umbrella', 'ammonium_method', 'arr_date_hq', 'arr_date_seq', 'chlorophyll_method', 'conduc_method', 'contact_email', 'contact_name', 'contact_orcid', 'density_method', 'diss_oxygen_method', 'env_broad_biome', 'env_local', 'env_material', 'env_package', 'extra_site_info', 'failure', 'failure_comment', 'geo_loc_name', 'investigation_type', 'loc_broad_ocean', 'loc_loc', 'loc_regional', 'month_name', 'nitrate_method', 'nitrite_method', 'obs_id', 'organism_count', 'organism_count_method', 'organization', 'organization_country', 'organization_edmoid', 'other_person', 'other_person_orcid', 'ph_method', 'phaeopigments_method', 'phosphate_method', 'pigments', 'pigments_method', 'pressure_method', 'project_name', 'replicate', 'samp_collect_device', 'samp_mat_process', 'samp_mat_process_dev', 'samp_store_date', 'samp_store_loc', 'sampl_person', 'sampl_person_orcid', 'sampling_event', 'scientific_name', 'sea_subsurf_salinity_method', 'sea_subsurf_temp_method', 'sea_surf_salinity_method', 'sea_surf_temp_method', 'season', 'ship_date', 'ship_date_seq', 'silicate_method', 'size_frac', 'store_person', 'store_person_orcid', 'tidal_stage', 'time_fi', 'turbidity_method', 'wa_id']\n",
      "INFO | Diversity analysis app | Numerical metadata columns are:\n",
      "['ENA_accession_number_sample', 'alkalinity', 'alkalinity_method', 'ammonium', 'bac_prod', 'bac_prod_method', 'biomass', 'biomass_method', 'chem_administration', 'chlorophyll', 'conduc', 'day', 'density', 'depth', 'diss_carb_dioxide', 'diss_carb_dioxide_method', 'diss_inorg_carb', 'diss_inorg_carb_method', 'diss_org_carb', 'diss_org_carb_method', 'diss_org_nitro', 'diss_org_nitro_method', 'diss_oxygen', 'down_par', 'down_par_method', 'latitude', 'loc_broad_ocean_mrgid', 'loc_loc_mrgid', 'loc_regional_mrgid', 'long_store', 'longitude', 'membr_cut', 'month', 'n_alkanes', 'n_alkanes_method', 'nitrate', 'nitrite', 'noteworthy_env_cond', 'part_org_carb', 'part_org_carb_method', 'part_org_nitro', 'part_org_nitro_method', 'petroleum_hydrocarb', 'petroleum_hydrocarb_method', 'ph', 'phaeopigments', 'phosphate', 'pressure', 'primary_prod', 'primary_prod_method', 'samp_size_vol', 'samp_store_temp', 'sea_subsurf_salinity', 'sea_subsurf_temp', 'sea_surf_salinity', 'sea_surf_temp', 'silicate', 'size_frac_low', 'size_frac_up', 'store_temp_hq', 'sulfate', 'sulfate_method', 'sulfide', 'sulfide_method', 'tax_id', 'tot_depth_water_col', 'turbidity', 'water_current', 'water_current_method', 'year']\n"
     ]
    }
   ],
   "source": [
    "# select categorical columns from metadata\n",
    "categorical_columns = sorted(full_metadata.select_dtypes(include=['object', \"boolean\"]).columns)\n",
    "cat_to_remove = [\"ref_code\", \"samp_description\", \"source_mat_id\", \"source_mat_id_orig\"]\n",
    "categorical_columns = [k for k in categorical_columns if k not in cat_to_remove]\n",
    "\n",
    "# select numerical columns from metadata\n",
    "numerical_columns = sorted(full_metadata.select_dtypes(include=['int64', 'float64']).columns)\n",
    "\n",
    "assert (\n",
    "    len(full_metadata.columns) == len(numerical_columns) + len(categorical_columns) + len(cat_to_remove), # + for removed cats\n",
    "\"i have wrong number orf columns in the metadata\",\n",
    ")\n",
    "\n",
    "if DEBUG:\n",
    "    logger.info(f\"Data table names are:\\n{mgf_parquet_dfs.keys()}\")\n",
    "    logger.info(f\"Categorical metadata columns are:\\n{categorical_columns}\")\n",
    "    logger.info(f\"Numerical metadata columns are:\\n{numerical_columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot the tables here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSU and SSU\n",
    "lsu = mgf_parquet_dfs['LSU']\n",
    "ssu = mgf_parquet_dfs['SSU']\n",
    "\n",
    "lsu_standard = pivot_taxonomic_data(lsu)\n",
    "ssu_standard = pivot_taxonomic_data(ssu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del mgf_parquet_dfs\n",
    "del lsu\n",
    "del ssu\n",
    "\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is used fot the tabular view only\n",
    "# df = mgf_parquet_dfs['SSU'].copy()\n",
    "# if DEBUG:\n",
    "#     logger.info(f'Number of unique ref_codes: {df.ref_code.nunique()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Development of the beta diversity part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: link these functions to the indicator\n",
    "# TODO: put them in the momics package\n",
    "def get_missing_taxa(df):\n",
    "    for taxon in [\"superkingdom\", \"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]:\n",
    "        logger.info(f'Not classified on {taxon}: {get_missing_taxa_single(df, taxon)}')\n",
    "    return\n",
    "\n",
    "def get_missing_taxa_single(df, taxon):\n",
    "    return len(df[df[taxon].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract from pivot tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to aggregate data by a specific taxonomic level\n",
    "def aggregate_by_taxonomic_level(df, level):\n",
    "    # Drop rows where the level is missing\n",
    "    df_level = df.dropna(subset=[level])\n",
    "    # Group by the specified level and sum abundances across samples (columns)\n",
    "    df_grouped = df_level.groupby(level).sum(numeric_only=True)\n",
    "    return df_grouped\n",
    "\n",
    "def separate_taxonomy(df):\n",
    "    # eukaryota_keywords = ['Discoba', 'Stramenopiles', 'Rhizaria', 'Alveolata', 'Amorphea', 'Archaeoplastida', 'Excavata']\n",
    "\n",
    "    # Separate rows based on \"Bacteria\", \"Archaea\", and \"Eukaryota\" entries\n",
    "    prokaryotes_all = df[df.index.str.contains(\"Bacteria|Archaea\", regex=True)]\n",
    "    eukaryota_all = df[df.index.str.contains(\"Eukaryota\", regex=True)]\n",
    "\n",
    "    # Further divide \"Prokaryotes all\" into \"Bacteria\" and \"Archaea\"\n",
    "    bacteria = prokaryotes_all[prokaryotes_all.index.str.contains(\"Bacteria\")]\n",
    "    archaea = prokaryotes_all[prokaryotes_all.index.str.contains(\"Archaea\")]\n",
    "\n",
    "    # Further divide \"Eukaryota all\" by specific keywords\n",
    "    # eukaryota_dict = {}\n",
    "    # for keyword in eukaryota_keywords:\n",
    "    #     subset = eukaryota_all[eukaryota_all.index.str.contains(keyword)]\n",
    "    #     eukaryota_dict[keyword] = subset\n",
    "    #     # Standardize each column to sum to 100 before saving the CSV\n",
    "    #     subset_normalized = subset.div(subset.sum(axis=0), axis=1) * 100\n",
    "\n",
    "    # Apply taxonomy splitting to the index\n",
    "    taxonomy_levels = bacteria.index.to_series().apply(split_taxonomy)\n",
    "    taxonomy_df = pd.DataFrame(taxonomy_levels.tolist(), columns=['phylum', 'class', 'order', 'family', 'genus', 'species'],\n",
    "                               index=bacteria.index)\n",
    "\n",
    "    # Combine taxonomy with the abundance data\n",
    "    bacteria_data = pd.concat([taxonomy_df, bacteria], axis=1)\n",
    "    \n",
    "\n",
    "    # Aggregate at each taxonomic level and save to CSV\n",
    "    taxonomic_levels = ['phylum', 'class', 'order', 'family', 'genus']\n",
    "    bacteria_levels_dict = {}\n",
    "    for level in taxonomic_levels:\n",
    "        aggregated_df = aggregate_by_taxonomic_level(bacteria_data, level)\n",
    "        # Standardize the values so each column sums to 100\n",
    "        aggregated_df_normalized = aggregated_df.div(aggregated_df.sum(axis=0), axis=1) * 100\n",
    "        bacteria_levels_dict[f\"Bacteria_{level}\"] = aggregated_df_normalized\n",
    "\n",
    "    all_data = {\n",
    "        \"Prokaryotes All\": prokaryotes_all,\n",
    "        \"Eukaryota All\": eukaryota_all,\n",
    "        \"Bacteria\": bacteria,\n",
    "        \"Archaea\": archaea\n",
    "    }\n",
    "    # all_data.update(eukaryota_dict)\n",
    "    all_data.update(bacteria_levels_dict)\n",
    "\n",
    "    return all_data\n",
    "\n",
    "def split_taxonomy(index_name):\n",
    "    # Remove anything before \"Bacteria\" or \"Archaea\"\n",
    "    if \"Bacteria\" in index_name:\n",
    "        taxonomy = index_name.split(\"Bacteria;\", 1)[1].split(\";\")\n",
    "    elif \"Archaea\" in index_name:\n",
    "        taxonomy = index_name.split(\"Archaea;\", 1)[1].split(\";\")\n",
    "    else:\n",
    "        taxonomy = []\n",
    "    # Return a list with taxonomic levels up to species\n",
    "    return taxonomy[1:7]  # ['phylum', 'class', 'order', 'family', 'genus', 'species']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropdowns for the pCOA\n",
    "- credits for inspiration to Andrzej Tkacz's NB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsu_standard.set_index('taxonomic_concat', inplace=True)\n",
    "ssu_standard.set_index('taxonomic_concat', inplace=True)\n",
    "\n",
    "split_taxo_tables_lsu = separate_taxonomy(lsu_standard)\n",
    "split_taxo_tables_ssu = separate_taxonomy(ssu_standard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<script type=\"esms-options\">{\"shimMode\": true}</script><style>*[data-root-id],\n",
       "*[data-root-id] > * {\n",
       "  box-sizing: border-box;\n",
       "  font-family: var(--jp-ui-font-family);\n",
       "  font-size: var(--jp-ui-font-size1);\n",
       "  color: var(--vscode-editor-foreground, var(--jp-ui-font-color1));\n",
       "}\n",
       "\n",
       "/* Override VSCode background color */\n",
       ".cell-output-ipywidget-background:has(\n",
       "    > .cell-output-ipywidget-background > .lm-Widget > *[data-root-id]\n",
       "  ),\n",
       ".cell-output-ipywidget-background:has(> .lm-Widget > *[data-root-id]) {\n",
       "  background-color: transparent !important;\n",
       "}\n",
       "</style>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n  const py_version = '3.6.2'.replace('rc', '-rc.').replace('.dev', '-dev.');\n  const reloading = false;\n  const Bokeh = root.Bokeh;\n\n  // Set a timeout for this load but only if we are not already initializing\n  if (typeof (root._bokeh_timeout) === \"undefined\" || (force || !root._bokeh_is_initializing)) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks;\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, js_modules, js_exports, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n    if (js_modules == null) js_modules = [];\n    if (js_exports == null) js_exports = {};\n\n    root._bokeh_onload_callbacks.push(callback);\n\n    if (root._bokeh_is_loading > 0) {\n      // Don't load bokeh if it is still initializing\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    } else if (js_urls.length === 0 && js_modules.length === 0 && Object.keys(js_exports).length === 0) {\n      // There is nothing to load\n      run_callbacks();\n      return null;\n    }\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n    window._bokeh_on_load = on_load\n\n    function on_error(e) {\n      const src_el = e.srcElement\n      console.error(\"failed to load \" + (src_el.href || src_el.src));\n    }\n\n    const skip = [];\n    if (window.requirejs) {\n      window.requirejs.config({'packages': {}, 'paths': {'tabulator': 'https://cdn.jsdelivr.net/npm/tabulator-tables@6.3.0/dist/js/tabulator.min', 'moment': 'https://cdn.jsdelivr.net/npm/luxon/build/global/luxon.min'}, 'shim': {}});\n      require([\"tabulator\"], function(Tabulator) {\n        window.Tabulator = Tabulator\n        on_load()\n      })\n      require([\"moment\"], function(moment) {\n        window.moment = moment\n        on_load()\n      })\n      root._bokeh_is_loading = css_urls.length + 2;\n    } else {\n      root._bokeh_is_loading = css_urls.length + js_urls.length + js_modules.length + Object.keys(js_exports).length;\n    }\n\n    const existing_stylesheets = []\n    const links = document.getElementsByTagName('link')\n    for (let i = 0; i < links.length; i++) {\n      const link = links[i]\n      if (link.href != null) {\n        existing_stylesheets.push(link.href)\n      }\n    }\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const escaped = encodeURI(url)\n      if (existing_stylesheets.indexOf(escaped) !== -1) {\n        on_load()\n        continue;\n      }\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }    if (((window.Tabulator !== undefined) && (!(window.Tabulator instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/tabulator-tables@6.3.0/dist/js/tabulator.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(encodeURI(urls[i]))\n      }\n    }    if (((window.moment !== undefined) && (!(window.moment instanceof HTMLElement))) || window.requirejs) {\n      var urls = ['https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/luxon/build/global/luxon.min.js'];\n      for (var i = 0; i < urls.length; i++) {\n        skip.push(encodeURI(urls[i]))\n      }\n    }    var existing_scripts = []\n    const scripts = document.getElementsByTagName('script')\n    for (let i = 0; i < scripts.length; i++) {\n      var script = scripts[i]\n      if (script.src != null) {\n        existing_scripts.push(script.src)\n      }\n    }\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (let i = 0; i < js_modules.length; i++) {\n      const url = js_modules[i];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) !== -1 || existing_scripts.indexOf(escaped) !== -1) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error;\n      element.async = false;\n      element.src = url;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n    for (const name in js_exports) {\n      const url = js_exports[name];\n      const escaped = encodeURI(url)\n      if (skip.indexOf(escaped) >= 0 || root[name] != null) {\n        if (!window.requirejs) {\n          on_load();\n        }\n        continue;\n      }\n      var element = document.createElement('script');\n      element.onerror = on_error;\n      element.async = false;\n      element.type = \"module\";\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      element.textContent = `\n      import ${name} from \"${url}\"\n      window.${name} = ${name}\n      window._bokeh_on_load()\n      `\n      document.head.appendChild(element);\n    }\n    if (!js_urls.length && !js_modules.length) {\n      on_load()\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.holoviz.org/panel/1.5.5/dist/bundled/reactiveesm/es-module-shims@^1.10.0/dist/es-module-shims.min.js\", \"https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/tabulator-tables@6.3.0/dist/js/tabulator.min.js\", \"https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/luxon/build/global/luxon.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.6.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.6.2.min.js\", \"https://cdn.holoviz.org/panel/1.5.5/dist/panel.min.js\"];\n  const js_modules = [];\n  const js_exports = {};\n  const css_urls = [\"https://cdn.holoviz.org/panel/1.5.5/dist/bundled/datatabulator/tabulator-tables@6.3.0/dist/css/tabulator_simple.min.css?v=1.5.5\"];\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {} // ensure no trailing comma for IE\n  ];\n\n  function run_inline_js() {\n    if ((root.Bokeh !== undefined) || (force === true)) {\n      for (let i = 0; i < inline_js.length; i++) {\n        try {\n          inline_js[i].call(root, root.Bokeh);\n        } catch(e) {\n          if (!reloading) {\n            throw e;\n          }\n        }\n      }\n      // Cache old bokeh versions\n      if (Bokeh != undefined && !reloading) {\n        var NewBokeh = root.Bokeh;\n        if (Bokeh.versions === undefined) {\n          Bokeh.versions = new Map();\n        }\n        if (NewBokeh.version !== Bokeh.version) {\n          Bokeh.versions.set(NewBokeh.version, NewBokeh)\n        }\n        root.Bokeh = Bokeh;\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    }\n    root._bokeh_is_initializing = false\n  }\n\n  function load_or_wait() {\n    // Implement a backoff loop that tries to ensure we do not load multiple\n    // versions of Bokeh and its dependencies at the same time.\n    // In recent versions we use the root._bokeh_is_initializing flag\n    // to determine whether there is an ongoing attempt to initialize\n    // bokeh, however for backward compatibility we also try to ensure\n    // that we do not start loading a newer (Panel>=1.0 and Bokeh>3) version\n    // before older versions are fully initialized.\n    if (root._bokeh_is_initializing && Date.now() > root._bokeh_timeout) {\n      // If the timeout and bokeh was not successfully loaded we reset\n      // everything and try loading again\n      root._bokeh_timeout = Date.now() + 5000;\n      root._bokeh_is_initializing = false;\n      root._bokeh_onload_callbacks = undefined;\n      root._bokeh_is_loading = 0\n      console.log(\"Bokeh: BokehJS was loaded multiple times but one version failed to initialize.\");\n      load_or_wait();\n    } else if (root._bokeh_is_initializing || (typeof root._bokeh_is_initializing === \"undefined\" && root._bokeh_onload_callbacks !== undefined)) {\n      setTimeout(load_or_wait, 100);\n    } else {\n      root._bokeh_is_initializing = true\n      root._bokeh_onload_callbacks = []\n      const bokeh_loaded = root.Bokeh != null && (root.Bokeh.version === py_version || (root.Bokeh.versions !== undefined && root.Bokeh.versions.has(py_version)));\n      if (!reloading && !bokeh_loaded) {\n        if (root.Bokeh) {\n          root.Bokeh = undefined;\n        }\n        console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n      }\n      load_libs(css_urls, js_urls, js_modules, js_exports, function() {\n        console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n        run_inline_js();\n      });\n    }\n  }\n  // Give older versions of the autoload script a head-start to ensure\n  // they initialize before we start loading newer version.\n  setTimeout(load_or_wait, 100)\n}(window));",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": "\nif ((window.PyViz === undefined) || (window.PyViz instanceof HTMLElement)) {\n  window.PyViz = {comms: {}, comm_status:{}, kernels:{}, receivers: {}, plot_index: []}\n}\n\n\n    function JupyterCommManager() {\n    }\n\n    JupyterCommManager.prototype.register_target = function(plot_id, comm_id, msg_handler) {\n      if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        comm_manager.register_target(comm_id, function(comm) {\n          comm.on_msg(msg_handler);\n        });\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        window.PyViz.kernels[plot_id].registerCommTarget(comm_id, function(comm) {\n          comm.onMsg = msg_handler;\n        });\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        google.colab.kernel.comms.registerTarget(comm_id, (comm) => {\n          var messages = comm.messages[Symbol.asyncIterator]();\n          function processIteratorResult(result) {\n            var message = result.value;\n            console.log(message)\n            var content = {data: message.data, comm_id};\n            var buffers = []\n            for (var buffer of message.buffers || []) {\n              buffers.push(new DataView(buffer))\n            }\n            var metadata = message.metadata || {};\n            var msg = {content, buffers, metadata}\n            msg_handler(msg);\n            return messages.next().then(processIteratorResult);\n          }\n          return messages.next().then(processIteratorResult);\n        })\n      }\n    }\n\n    JupyterCommManager.prototype.get_client_comm = function(plot_id, comm_id, msg_handler) {\n      if (comm_id in window.PyViz.comms) {\n        return window.PyViz.comms[comm_id];\n      } else if (window.comm_manager || ((window.Jupyter !== undefined) && (Jupyter.notebook.kernel != null))) {\n        var comm_manager = window.comm_manager || Jupyter.notebook.kernel.comm_manager;\n        var comm = comm_manager.new_comm(comm_id, {}, {}, {}, comm_id);\n        if (msg_handler) {\n          comm.on_msg(msg_handler);\n        }\n      } else if ((plot_id in window.PyViz.kernels) && (window.PyViz.kernels[plot_id])) {\n        var comm = window.PyViz.kernels[plot_id].connectToComm(comm_id);\n        comm.open();\n        if (msg_handler) {\n          comm.onMsg = msg_handler;\n        }\n      } else if (typeof google != 'undefined' && google.colab.kernel != null) {\n        var comm_promise = google.colab.kernel.comms.open(comm_id)\n        comm_promise.then((comm) => {\n          window.PyViz.comms[comm_id] = comm;\n          if (msg_handler) {\n            var messages = comm.messages[Symbol.asyncIterator]();\n            function processIteratorResult(result) {\n              var message = result.value;\n              var content = {data: message.data};\n              var metadata = message.metadata || {comm_id};\n              var msg = {content, metadata}\n              msg_handler(msg);\n              return messages.next().then(processIteratorResult);\n            }\n            return messages.next().then(processIteratorResult);\n          }\n        })\n        var sendClosure = (data, metadata, buffers, disposeOnDone) => {\n          return comm_promise.then((comm) => {\n            comm.send(data, metadata, buffers, disposeOnDone);\n          });\n        };\n        var comm = {\n          send: sendClosure\n        };\n      }\n      window.PyViz.comms[comm_id] = comm;\n      return comm;\n    }\n    window.PyViz.comm_manager = new JupyterCommManager();\n    \n\n\nvar JS_MIME_TYPE = 'application/javascript';\nvar HTML_MIME_TYPE = 'text/html';\nvar EXEC_MIME_TYPE = 'application/vnd.holoviews_exec.v0+json';\nvar CLASS_NAME = 'output';\n\n/**\n * Render data to the DOM node\n */\nfunction render(props, node) {\n  var div = document.createElement(\"div\");\n  var script = document.createElement(\"script\");\n  node.appendChild(div);\n  node.appendChild(script);\n}\n\n/**\n * Handle when a new output is added\n */\nfunction handle_add_output(event, handle) {\n  var output_area = handle.output_area;\n  var output = handle.output;\n  if ((output.data == undefined) || (!output.data.hasOwnProperty(EXEC_MIME_TYPE))) {\n    return\n  }\n  var id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n  var toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n  if (id !== undefined) {\n    var nchildren = toinsert.length;\n    var html_node = toinsert[nchildren-1].children[0];\n    html_node.innerHTML = output.data[HTML_MIME_TYPE];\n    var scripts = [];\n    var nodelist = html_node.querySelectorAll(\"script\");\n    for (var i in nodelist) {\n      if (nodelist.hasOwnProperty(i)) {\n        scripts.push(nodelist[i])\n      }\n    }\n\n    scripts.forEach( function (oldScript) {\n      var newScript = document.createElement(\"script\");\n      var attrs = [];\n      var nodemap = oldScript.attributes;\n      for (var j in nodemap) {\n        if (nodemap.hasOwnProperty(j)) {\n          attrs.push(nodemap[j])\n        }\n      }\n      attrs.forEach(function(attr) { newScript.setAttribute(attr.name, attr.value) });\n      newScript.appendChild(document.createTextNode(oldScript.innerHTML));\n      oldScript.parentNode.replaceChild(newScript, oldScript);\n    });\n    if (JS_MIME_TYPE in output.data) {\n      toinsert[nchildren-1].children[1].textContent = output.data[JS_MIME_TYPE];\n    }\n    output_area._hv_plot_id = id;\n    if ((window.Bokeh !== undefined) && (id in Bokeh.index)) {\n      window.PyViz.plot_index[id] = Bokeh.index[id];\n    } else {\n      window.PyViz.plot_index[id] = null;\n    }\n  } else if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n    var bk_div = document.createElement(\"div\");\n    bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n    var script_attrs = bk_div.children[0].attributes;\n    for (var i = 0; i < script_attrs.length; i++) {\n      toinsert[toinsert.length - 1].childNodes[1].setAttribute(script_attrs[i].name, script_attrs[i].value);\n    }\n    // store reference to server id on output_area\n    output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n  }\n}\n\n/**\n * Handle when an output is cleared or removed\n */\nfunction handle_clear_output(event, handle) {\n  var id = handle.cell.output_area._hv_plot_id;\n  var server_id = handle.cell.output_area._bokeh_server_id;\n  if (((id === undefined) || !(id in PyViz.plot_index)) && (server_id !== undefined)) { return; }\n  var comm = window.PyViz.comm_manager.get_client_comm(\"hv-extension-comm\", \"hv-extension-comm\", function () {});\n  if (server_id !== null) {\n    comm.send({event_type: 'server_delete', 'id': server_id});\n    return;\n  } else if (comm !== null) {\n    comm.send({event_type: 'delete', 'id': id});\n  }\n  delete PyViz.plot_index[id];\n  if ((window.Bokeh !== undefined) & (id in window.Bokeh.index)) {\n    var doc = window.Bokeh.index[id].model.document\n    doc.clear();\n    const i = window.Bokeh.documents.indexOf(doc);\n    if (i > -1) {\n      window.Bokeh.documents.splice(i, 1);\n    }\n  }\n}\n\n/**\n * Handle kernel restart event\n */\nfunction handle_kernel_cleanup(event, handle) {\n  delete PyViz.comms[\"hv-extension-comm\"];\n  window.PyViz.plot_index = {}\n}\n\n/**\n * Handle update_display_data messages\n */\nfunction handle_update_output(event, handle) {\n  handle_clear_output(event, {cell: {output_area: handle.output_area}})\n  handle_add_output(event, handle)\n}\n\nfunction register_renderer(events, OutputArea) {\n  function append_mime(data, metadata, element) {\n    // create a DOM node to render to\n    var toinsert = this.create_output_subarea(\n    metadata,\n    CLASS_NAME,\n    EXEC_MIME_TYPE\n    );\n    this.keyboard_manager.register_events(toinsert);\n    // Render to node\n    var props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n    render(props, toinsert[0]);\n    element.append(toinsert);\n    return toinsert\n  }\n\n  events.on('output_added.OutputArea', handle_add_output);\n  events.on('output_updated.OutputArea', handle_update_output);\n  events.on('clear_output.CodeCell', handle_clear_output);\n  events.on('delete.Cell', handle_clear_output);\n  events.on('kernel_ready.Kernel', handle_kernel_cleanup);\n\n  OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n    safe: true,\n    index: 0\n  });\n}\n\nif (window.Jupyter !== undefined) {\n  try {\n    var events = require('base/js/events');\n    var OutputArea = require('notebook/js/outputarea').OutputArea;\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  } catch(err) {\n  }\n}\n",
      "application/vnd.holoviews_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.holoviews_exec.v0+json": "",
      "text/html": [
       "<div id='43e0c7db-5a03-4e4b-8521-488f16cf6d47'>\n",
       "  <div id=\"dd0ff79e-2588-4334-be0a-21a05743478b\" data-root-id=\"43e0c7db-5a03-4e4b-8521-488f16cf6d47\" style=\"display: contents;\"></div>\n",
       "</div>\n",
       "<script type=\"application/javascript\">(function(root) {\n",
       "  var docs_json = {\"6e3003c0-8bc6-4830-9e77-7abe7f1b2fc2\":{\"version\":\"3.6.2\",\"title\":\"Bokeh Application\",\"roots\":[{\"type\":\"object\",\"name\":\"panel.models.browser.BrowserInfo\",\"id\":\"43e0c7db-5a03-4e4b-8521-488f16cf6d47\"},{\"type\":\"object\",\"name\":\"panel.models.comm_manager.CommManager\",\"id\":\"4adcb830-2a4a-431a-a13d-897d49eb3107\",\"attributes\":{\"plot_id\":\"43e0c7db-5a03-4e4b-8521-488f16cf6d47\",\"comm_id\":\"e7d73ece5f1e4e16a957e703a73085f8\",\"client_comm_id\":\"f50289a09fa240b9be2d836cbff0ed72\"}}],\"defs\":[{\"type\":\"model\",\"name\":\"ReactiveHTML1\"},{\"type\":\"model\",\"name\":\"FlexBox1\",\"properties\":[{\"name\":\"align_content\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"align_items\",\"kind\":\"Any\",\"default\":\"flex-start\"},{\"name\":\"flex_direction\",\"kind\":\"Any\",\"default\":\"row\"},{\"name\":\"flex_wrap\",\"kind\":\"Any\",\"default\":\"wrap\"},{\"name\":\"gap\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"justify_content\",\"kind\":\"Any\",\"default\":\"flex-start\"}]},{\"type\":\"model\",\"name\":\"FloatPanel1\",\"properties\":[{\"name\":\"config\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"contained\",\"kind\":\"Any\",\"default\":true},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"right-top\"},{\"name\":\"offsetx\",\"kind\":\"Any\",\"default\":null},{\"name\":\"offsety\",\"kind\":\"Any\",\"default\":null},{\"name\":\"theme\",\"kind\":\"Any\",\"default\":\"primary\"},{\"name\":\"status\",\"kind\":\"Any\",\"default\":\"normalized\"}]},{\"type\":\"model\",\"name\":\"GridStack1\",\"properties\":[{\"name\":\"mode\",\"kind\":\"Any\",\"default\":\"warn\"},{\"name\":\"ncols\",\"kind\":\"Any\",\"default\":null},{\"name\":\"nrows\",\"kind\":\"Any\",\"default\":null},{\"name\":\"allow_resize\",\"kind\":\"Any\",\"default\":true},{\"name\":\"allow_drag\",\"kind\":\"Any\",\"default\":true},{\"name\":\"state\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"drag1\",\"properties\":[{\"name\":\"slider_width\",\"kind\":\"Any\",\"default\":5},{\"name\":\"slider_color\",\"kind\":\"Any\",\"default\":\"black\"},{\"name\":\"value\",\"kind\":\"Any\",\"default\":50}]},{\"type\":\"model\",\"name\":\"click1\",\"properties\":[{\"name\":\"terminal_output\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"debug_name\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"clears\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"FastWrapper1\",\"properties\":[{\"name\":\"object\",\"kind\":\"Any\",\"default\":null},{\"name\":\"style\",\"kind\":\"Any\",\"default\":null}]},{\"type\":\"model\",\"name\":\"NotificationAreaBase1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"NotificationArea1\",\"properties\":[{\"name\":\"js_events\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}},{\"name\":\"notifications\",\"kind\":\"Any\",\"default\":[]},{\"name\":\"position\",\"kind\":\"Any\",\"default\":\"bottom-right\"},{\"name\":\"_clear\",\"kind\":\"Any\",\"default\":0},{\"name\":\"types\",\"kind\":\"Any\",\"default\":[{\"type\":\"map\",\"entries\":[[\"type\",\"warning\"],[\"background\",\"#ffc107\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-exclamation-triangle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]},{\"type\":\"map\",\"entries\":[[\"type\",\"info\"],[\"background\",\"#007bff\"],[\"icon\",{\"type\":\"map\",\"entries\":[[\"className\",\"fas fa-info-circle\"],[\"tagName\",\"i\"],[\"color\",\"white\"]]}]]}]}]},{\"type\":\"model\",\"name\":\"Notification\",\"properties\":[{\"name\":\"background\",\"kind\":\"Any\",\"default\":null},{\"name\":\"duration\",\"kind\":\"Any\",\"default\":3000},{\"name\":\"icon\",\"kind\":\"Any\",\"default\":null},{\"name\":\"message\",\"kind\":\"Any\",\"default\":\"\"},{\"name\":\"notification_type\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_rendered\",\"kind\":\"Any\",\"default\":false},{\"name\":\"_destroyed\",\"kind\":\"Any\",\"default\":false}]},{\"type\":\"model\",\"name\":\"TemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"BootstrapTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"TemplateEditor1\",\"properties\":[{\"name\":\"layout\",\"kind\":\"Any\",\"default\":[]}]},{\"type\":\"model\",\"name\":\"MaterialTemplateActions1\",\"properties\":[{\"name\":\"open_modal\",\"kind\":\"Any\",\"default\":0},{\"name\":\"close_modal\",\"kind\":\"Any\",\"default\":0}]},{\"type\":\"model\",\"name\":\"ReactiveESM1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"JSComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"ReactComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"AnyWidgetComponent1\",\"properties\":[{\"name\":\"esm_constants\",\"kind\":\"Any\",\"default\":{\"type\":\"map\"}}]},{\"type\":\"model\",\"name\":\"request_value1\",\"properties\":[{\"name\":\"fill\",\"kind\":\"Any\",\"default\":\"none\"},{\"name\":\"_synced\",\"kind\":\"Any\",\"default\":null},{\"name\":\"_request_sync\",\"kind\":\"Any\",\"default\":0}]}]}};\n",
       "  var render_items = [{\"docid\":\"6e3003c0-8bc6-4830-9e77-7abe7f1b2fc2\",\"roots\":{\"43e0c7db-5a03-4e4b-8521-488f16cf6d47\":\"dd0ff79e-2588-4334-be0a-21a05743478b\"},\"root_ids\":[\"43e0c7db-5a03-4e4b-8521-488f16cf6d47\"]}];\n",
       "  var docs = Object.values(docs_json)\n",
       "  if (!docs) {\n",
       "    return\n",
       "  }\n",
       "  const py_version = docs[0].version.replace('rc', '-rc.').replace('.dev', '-dev.')\n",
       "  async function embed_document(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    await Bokeh.embed.embed_items_notebook(docs_json, render_items);\n",
       "    for (const render_item of render_items) {\n",
       "      for (const root_id of render_item.root_ids) {\n",
       "\tconst id_el = document.getElementById(root_id)\n",
       "\tif (id_el.children.length && id_el.children[0].hasAttribute('data-root-id')) {\n",
       "\t  const root_el = id_el.children[0]\n",
       "\t  root_el.id = root_el.id + '-rendered'\n",
       "\t  for (const child of root_el.children) {\n",
       "            // Ensure JupyterLab does not capture keyboard shortcuts\n",
       "            // see: https://jupyterlab.readthedocs.io/en/4.1.x/extension/notebook.html#keyboard-interaction-model\n",
       "\t    child.setAttribute('data-lm-suppress-shortcuts', 'true')\n",
       "\t  }\n",
       "\t}\n",
       "      }\n",
       "    }\n",
       "  }\n",
       "  function get_bokeh(root) {\n",
       "    if (root.Bokeh === undefined) {\n",
       "      return null\n",
       "    } else if (root.Bokeh.version !== py_version) {\n",
       "      if (root.Bokeh.versions === undefined || !root.Bokeh.versions.has(py_version)) {\n",
       "\treturn null\n",
       "      }\n",
       "      return root.Bokeh.versions.get(py_version);\n",
       "    } else if (root.Bokeh.version === py_version) {\n",
       "      return root.Bokeh\n",
       "    }\n",
       "    return null\n",
       "  }\n",
       "  function is_loaded(root) {\n",
       "    var Bokeh = get_bokeh(root)\n",
       "    return (Bokeh != null && Bokeh.Panel !== undefined && ( root.Tabulator !== undefined) && ( root.Tabulator !== undefined))\n",
       "  }\n",
       "  if (is_loaded(root)) {\n",
       "    embed_document(root);\n",
       "  } else {\n",
       "    var attempts = 0;\n",
       "    var timer = setInterval(function(root) {\n",
       "      if (is_loaded(root)) {\n",
       "        clearInterval(timer);\n",
       "        embed_document(root);\n",
       "      } else if (document.readyState == \"complete\") {\n",
       "        attempts++;\n",
       "        if (attempts > 200) {\n",
       "          clearInterval(timer);\n",
       "\t  var Bokeh = get_bokeh(root)\n",
       "\t  if (Bokeh == null || Bokeh.Panel == null) {\n",
       "            console.warn(\"Panel: ERROR: Unable to run Panel code because Bokeh or Panel library is missing\");\n",
       "\t  } else {\n",
       "\t    console.warn(\"Panel: WARNING: Attempting to render but not all required libraries could be resolved.\")\n",
       "\t    embed_document(root)\n",
       "\t  }\n",
       "        }\n",
       "      }\n",
       "    }, 25, root)\n",
       "  }\n",
       "})(window);</script>"
      ]
     },
     "metadata": {
      "application/vnd.holoviews_exec.v0+json": {
       "id": "43e0c7db-5a03-4e4b-8521-488f16cf6d47"
      }
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO | Diversity analysis app | Granular levels are:\n",
      "['Prokaryotes All', 'Eukaryota All', 'Bacteria', 'Archaea', 'Bacteria_phylum', 'Bacteria_class', 'Bacteria_order', 'Bacteria_family', 'Bacteria_genus']\n"
     ]
    }
   ],
   "source": [
    "pn.extension(\"tabulator\")\n",
    "\n",
    "granular_tables = {\n",
    "    \"LSU\": split_taxo_tables_lsu,\n",
    "    \"SSU\": split_taxo_tables_ssu\n",
    "}\n",
    "\n",
    "select_granular_table = pn.widgets.Select(\n",
    "    name=\"Granular analysis\",\n",
    "    options= list(granular_tables.keys()),\n",
    "    description=\"Select a table for granular analysis\",\n",
    ")\n",
    "\n",
    "select_granular_level = pn.widgets.Select(\n",
    "    name=\"Subset taxonomic level\",\n",
    "    options=list(granular_tables[select_granular_table.value].keys()),\n",
    "    description=\"Select a table for analysis\",\n",
    ")\n",
    "\n",
    "logger.info(f\"Granular levels are:\\n{list(granular_tables[select_granular_table.value].keys())}\")\n",
    "\n",
    "\n",
    "factors_to_remove = ['ENA_accession_number_project', \"ENA_accession_number_umbrella\", \"arr_date_hq\",\n",
    "                     \"arr_date_seq\", \"contact_email\", \"contact_name\", \"contact_orcid\",\n",
    "                     \"investigation_type\", \"long_store\", \"organism_count_method\", \"organization_edmoid\",\n",
    "                     'other_person', 'other_person_orcid',\"organization_country\", \"project_name\",\n",
    "                     \"samp_store_date\", 'samp_mat_process', 'samp_mat_process_dev',\n",
    "                     'samp_store_loc', 'sampl_person', 'sampl_person_orcid', 'store_person',\n",
    "                     'store_person_orcid', 'time_fi', \"wa_id\",\n",
    "                     'env_broad_biome', 'env_local', \"extra_site_info\", 'failure_comment',\n",
    "                     'obs_id', 'size_frac','ship_date', 'ship_date_seq', 'sampling_event', 'organism_count',\n",
    "                     'samp_collect_device',\n",
    "                     'ammonium_method', 'chlorophyll_method', 'conduc_method', 'density_method', 'diss_oxygen_method',\n",
    "                     'nitrate_method', 'nitrite_method', 'ph_method', 'phaeopigments_method', 'phosphate_method', 'pigments_method', 'pressure_method',\n",
    "                     'sea_subsurf_salinity_method', 'sea_subsurf_temp_method', 'sea_surf_salinity_method', 'sea_surf_temp_method',\n",
    "                     'silicate_method', 'turbidity_method']\n",
    "factor_cols = [col for col in categorical_columns if col not in factors_to_remove]\n",
    "pcoa_factor_dropdowns = {\n",
    "    categorical_col: pn.widgets.MultiSelect(\n",
    "        name=categorical_col,\n",
    "        value=['All'],\n",
    "        options=['All'] + list(full_metadata[categorical_col].unique()),\n",
    "        size=6, width=180,)\n",
    "        for categorical_col in factor_cols\n",
    "}\n",
    "box_granular = pn.GridBox(\n",
    "    *pcoa_factor_dropdowns.values(),\n",
    "    ncols=5,\n",
    "    )\n",
    "\n",
    "color_factor_granular = pn.widgets.Select(\n",
    "    name=\"Color by\",\n",
    "    value=factor_cols[0],\n",
    "    options=factor_cols,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter metadata methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Filter the metadata table based on the selections in box_granular\n",
    "def filter_metadata_table(metadata_df, selected_factors):\n",
    "    # Create a copy of the metadata DataFrame\n",
    "    filtered_metadata = metadata_df.copy()\n",
    "    # Apply filters for each selected factor\n",
    "    for factor, selected_values in selected_factors.items():\n",
    "        if 'All' not in selected_values:\n",
    "            filtered_metadata = filtered_metadata[filtered_metadata[factor].isin(selected_values)]\n",
    "    return filtered_metadata\n",
    "\n",
    "def get_filtered_metadata():\n",
    "    # Retrieve the selected factors from the dropdowns\n",
    "    selected_factors = {col: pcoa_factor_dropdowns[col].value for col in factor_cols}\n",
    "    # Filter the metadata table\n",
    "    filtered_metadata = filter_metadata_table(full_metadata, selected_factors)\n",
    "    return filtered_metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter data methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## filter data according to the metadata\n",
    "def filter_data(df, filtered_metadata):\n",
    "    # Filter the DataFrame column names based on the 'ref_code' values in the filtered metadata\n",
    "\n",
    "    # filter columns names of df which are in the filtered metadata\n",
    "    cols_to_keep = list([col for col in df.columns.str.strip() if col in filtered_metadata['ref_code'].to_list()])\n",
    "\n",
    "    filtered_df = df[cols_to_keep]\n",
    "    return filtered_df\n",
    "\n",
    "def filter_all_box_selection(df):\n",
    "    # Retrieve the filtered metadata\n",
    "    filtered_metadata = get_filtered_metadata()\n",
    "    # Filter the data\n",
    "    filtered_data = filter_data(df, filtered_metadata)\n",
    "    return filtered_metadata, filtered_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Granular PCoA page for the app\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_metadata, filtered_data = filter_all_box_selection(granular_tables['LSU']['Bacteria_phylum'])\n",
    "\n",
    "# show indicator of the explained variance\n",
    "explained_var_indicator = pn.indicators.Number(\n",
    "    name='Explained variance by PC1 + PC2', value=0, format='{value:.1f}%',\n",
    "    font_size='20pt',\n",
    "    title_size='12pt',\n",
    "    colors=[(33, 'red'), (50, 'gold'), (66, 'green')]\n",
    ")\n",
    "\n",
    "beta_pc_plot_granular = pn.pane.Matplotlib(\n",
    "    height=600,\n",
    "    name=\"Beta PCoA\",\n",
    "    )\n",
    "\n",
    "def update_beta_pc_plot_granular(filtered_data, metadata, factor):\n",
    "    beta_pc_plot_granular.object, explained_var_indicator.value = pl.beta_plot_pc_granular(\n",
    "        filtered_data=filtered_data,\n",
    "        metadata=metadata,\n",
    "        factor=factor)\n",
    "\n",
    "pn.bind(update_beta_pc_plot_granular,\n",
    "    filtered_data=filtered_data,\n",
    "    metadata=filtered_metadata,\n",
    "    factor=color_factor_granular,\n",
    "    watch=True,\n",
    "    )\n",
    "\n",
    "\n",
    "#################\n",
    "# REMOVE from CPU purposes\n",
    "# atable = pn.widgets.Tabulator(\n",
    "#     filtered_data,\n",
    "#     name=\"Filtered data View\")\n",
    "\n",
    "# def update_atable(filtered_data):\n",
    "#     # Update the Tabulator table with the filtered data\n",
    "#     atable.value = filtered_data\n",
    "###################\n",
    "\n",
    "button_filter_table = pn.widgets.Button(\n",
    "    name=\"Filter table\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    ")\n",
    "\n",
    "def update_filtered_data(button):\n",
    "    logger.info(f\"Button clicked: {button.name}\")\n",
    "    # Retrieve the filtered metadata\n",
    "    filtered_metadata, filtered_data = filter_all_box_selection(\n",
    "        granular_tables[select_granular_table.value][select_granular_level.value])\n",
    "    logger.info(f\"matadata shape {filtered_metadata.shape}\")\n",
    "    logger.info(f\"data shape {filtered_data.shape}\")\n",
    "    # Update the beta plot\n",
    "    update_beta_pc_plot_granular(filtered_data, filtered_metadata, color_factor_granular.value)\n",
    "    update_subset_indicator(subset_selected, filtered_metadata)\n",
    "    update_taxa_count_indicator(taxa_selected, filtered_data)\n",
    "\n",
    "    # REMOVED for CPU purposes\n",
    "    # Update the table with the filtered data\n",
    "    # update_atable(filtered_data)\n",
    "\n",
    "\n",
    "button_filter_table.on_click(update_filtered_data)\n",
    "\n",
    "\n",
    "pcoa_instructions = pn.pane.Markdown(\n",
    "    \"\"\"\n",
    "    ### Instructions\n",
    "    1. Side panel filters LSU/SSU tables by taxonomy levels.\n",
    "    2. Color_by is used to color the beta diversity plot.\n",
    "    3. Main panel filter further the table by the metadata values.\n",
    "        - `Ctrl`-click to select multiple values in the dropdowns.\n",
    "    4. Filtering and update of the plot happens only after clicking the `Filter table` button to save CPU.\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "pcoa_tab_granular = pn.Column(\n",
    "    pcoa_instructions,\n",
    "    box_granular,\n",
    "    button_filter_table,\n",
    "    explained_var_indicator,\n",
    "    beta_pc_plot_granular,\n",
    "    scroll=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permanova page for the app\n",
    "- Credits to Andrzej Tkacz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Watcher(inst=Button(button_type='primary', name='PERMANOVA', width=200), cls=<class 'panel.widgets.button.Button'>, fn=<function <lambda> at 0x0000022E256BEDE0>, mode='args', onlychanged=False, parameter_names=('clicks',), what='value', queued=False, precedence=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import skbio\n",
    "from skbio.stats.distance import permanova\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "# PERMANOVA Dropdowns\n",
    "permanova_factor = pn.widgets.Select(\n",
    "    name=\"Main Permanova factor\",\n",
    "    options=['All'] + factor_cols,\n",
    "    description='Limit by group(s) in factor:',\n",
    ")\n",
    "\n",
    "permanova_group = pn.widgets.MultiSelect(\n",
    "    name=\"Groups of unique values of the factor\",\n",
    "    options=[],\n",
    "    description='Groups:',\n",
    ")\n",
    "\n",
    "permanova_additional_factors = pn.widgets.MultiSelect(\n",
    "    name=\"Factors to test vs ALL the rest\",\n",
    "    options=factor_cols,\n",
    "    description='PERMANOVA Factors:',\n",
    ")\n",
    "\n",
    "# Update groups based on selected factor\n",
    "def update_groups(permanova_factor):\n",
    "    logger.info(f\"Permanova factor value: {permanova_factor}\")\n",
    "    if permanova_factor in factor_cols:\n",
    "        unique_groups = sorted(full_metadata[permanova_factor].dropna().unique())\n",
    "        permanova_group.options = unique_groups\n",
    "    elif permanova_factor == 'All':\n",
    "        permanova_group.options = sorted(full_metadata['ref_code'].dropna().unique())\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown factor: {permanova_factor}\")\n",
    "    \n",
    "pn.bind(update_groups,\n",
    "    permanova_factor,\n",
    "    watch=True,\n",
    ")\n",
    "\n",
    "permanova_result_indicator = pn.widgets.Tabulator(pd.DataFrame(), name='Permanova Result')\n",
    "\n",
    "\n",
    "def run_permanova(permanova_factor, permanova_group, permanova_additional_factors):\n",
    "    data = granular_tables[select_granular_table.value][select_granular_level.value]\n",
    "    # Filter metadata based on selected groups\n",
    "    if permanova_factor.value == 'All':\n",
    "        filtered_metadata = full_metadata.copy()\n",
    "    else:\n",
    "        filtered_metadata = full_metadata[full_metadata[permanova_factor.value].isin(permanova_group.value)]\n",
    "    \n",
    "    if 'ref_code' not in filtered_metadata.columns:\n",
    "        raise KeyError(\"The 'ref_code' column is missing from filtered metadata.\")\n",
    "    \n",
    "    # Match data and metadata samples\n",
    "    matched_data = data[filtered_metadata['ref_code']].T\n",
    "    abundance_matrix = matched_data\n",
    "\n",
    "    permanova_results = {}\n",
    "    factors_to_test = permanova_additional_factors.value\n",
    "    for remaining_factor in factors_to_test:\n",
    "        factor_metadata = filtered_metadata.dropna(subset=[remaining_factor])\n",
    "        combined_abundance = abundance_matrix.loc[factor_metadata['ref_code']]\n",
    "        \n",
    "        # Calculate Bray-Curtis distance matrix\n",
    "        dissimilarity_matrix = pairwise_distances(combined_abundance, metric='braycurtis')\n",
    "        distance_matrix_obj = skbio.DistanceMatrix(dissimilarity_matrix, ids=combined_abundance.index)\n",
    "        \n",
    "        factor_metadata = factor_metadata.set_index('ref_code')\n",
    "        factor_metadata = factor_metadata.loc[factor_metadata.index.intersection(distance_matrix_obj.ids)]\n",
    "        \n",
    "        if remaining_factor not in factor_metadata.columns:\n",
    "            continue\n",
    "\n",
    "        group_vector = factor_metadata[remaining_factor]\n",
    "        if group_vector.nunique() < len(group_vector):\n",
    "            if set(distance_matrix_obj.ids) == set(group_vector.index):\n",
    "                permanova_result = permanova(distance_matrix_obj, grouping=group_vector, permutations=999)\n",
    "                permanova_results[remaining_factor] = permanova_result\n",
    "                if DEBUG:\n",
    "                    logger.info(f\"Factor: {remaining_factor}\")\n",
    "                    logger.info(f\"  F-statistic: {permanova_result['test statistic']:.4f}\")\n",
    "                    logger.info(f\"  p-value: {permanova_result['p-value']:.4f}\\n\")\n",
    "        else:\n",
    "            logger.info(f\"Skipping factor '{remaining_factor}' due to unique values in grouping vector.\")\n",
    "\n",
    "    permanova_result_indicator.value = pd.DataFrame.from_dict(permanova_results)\n",
    "    return permanova_results\n",
    "\n",
    "permanova_button = pn.widgets.Button(\n",
    "    name=\"PERMANOVA\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    ")\n",
    "\n",
    "permanova_button.on_click(\n",
    "    lambda event: run_permanova(\n",
    "        permanova_factor,\n",
    "        permanova_group,\n",
    "        permanova_additional_factors,\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "permanova_instructions = pn.pane.Markdown(\n",
    "    \"\"\"\n",
    "    ### Instructions\n",
    "    1. Select a factor to limit the analysis.\n",
    "    2. Select groups in the factor.\n",
    "    3. Select additional factors for against which PERMANOVA will be run (`Ctrl`-click to select multiple).\n",
    "    4. Click the `PERMANOVA` button to run the analysis (`Ctrl`-click to select multiple).\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "permanova_tab = pn.Column(\n",
    "    permanova_instructions,\n",
    "    pn.Row(\n",
    "        permanova_factor,\n",
    "        permanova_group,\n",
    "        permanova_additional_factors,\n",
    "    ),\n",
    "    permanova_button,\n",
    "    permanova_result_indicator,\n",
    "    scroll=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_samplings = full_metadata['ref_code'].nunique()\n",
    "subset = filtered_metadata['ref_code'].nunique()\n",
    "taxa_count = len(filtered_data)\n",
    "\n",
    "def update_subset_indicator(indicator, df):\n",
    "    indicator.value = df['ref_code'].nunique()\n",
    "\n",
    "subset_selected = pn.indicators.Number(\n",
    "    name=\"Subset of samples you filtered\",\n",
    "    value=subset,\n",
    "    format=\"{value}\" + f\"/{total_samplings}\",\n",
    "    width=150,\n",
    "    font_size=\"34px\",\n",
    "    title_size=\"14px\",\n",
    ")\n",
    "\n",
    "def update_taxa_count_indicator(indicator, df):\n",
    "    indicator.value = len(df)\n",
    "\n",
    "taxa_selected = pn.indicators.Number(\n",
    "    name=\"Taxa in the selection.\",\n",
    "    value=taxa_count,\n",
    "    format=\"{value}\",\n",
    "    width=150,\n",
    "    font_size=\"34px\",\n",
    "    title_size=\"14px\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## App setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pn.extension(\"tabulator\")\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    pn.extension(comms='colab')\n",
    "ACCENT = \"teal\"\n",
    "\n",
    "styles = {\n",
    "    \"box-shadow\": \"rgba(50, 50, 93, 0.25) 0px 6px 12px -2px, rgba(0, 0, 0, 0.3) 0px 3px 7px -3px\",\n",
    "    \"border-radius\": \"4px\",\n",
    "    \"padding\": \"10px\",\n",
    "}\n",
    "\n",
    "# TODO: there is a bug in the panel library that does not allow to open png files, renoming does not help \n",
    "image = pn.pane.JPG(os.path.join(assets_folder, \"figs/metaGOflow_logo_italics.jpg\"),\n",
    "                    width=200,\n",
    "                    height=100,\n",
    "                    )\n",
    "\n",
    "tabs = pn.Tabs(\n",
    "    ('PCoA', pcoa_tab_granular),\n",
    "    ('Permanova', permanova_tab),\n",
    "    # atable,\n",
    "    styles=styles,\n",
    "    margin=10\n",
    ")\n",
    "_, indicator_usage = create_indicators_diversity()\n",
    "\n",
    "def update_used_gb(event):\n",
    "    if not event:\n",
    "        return\n",
    "\n",
    "    used_gb, total_gb = memory_load()\n",
    "    indicator_usage.value = used_gb\n",
    "\n",
    "\n",
    "# logger.info(f\"just before the app definition {granular_tables[select_granular_table][select_granular_level].shape}\")\n",
    "filtered_metadata, filtered_data = filter_all_box_selection(\n",
    "        granular_tables[select_granular_table.value][select_granular_level.value],\n",
    "        )\n",
    "\n",
    "def app():\n",
    "    cb = pn.state.add_periodic_callback(\n",
    "        partial(update_used_gb, indicator_usage),\n",
    "        period=1000,\n",
    "        timeout=None,\n",
    "        )\n",
    "\n",
    "    toggle = pn.widgets.Toggle(\n",
    "        name='Toggle callback',\n",
    "        value=True,\n",
    "        button_type='success',)\n",
    "    toggle.link(cb, bidirectional=True, value='running')\n",
    "\n",
    "    template = pn.template.FastListTemplate(\n",
    "        title=\"Diversity Analysis\",\n",
    "        sidebar=[image,\n",
    "                \"# Beta granular\", select_granular_table, select_granular_level,\n",
    "                color_factor_granular,\n",
    "                pn.layout.Divider(),\n",
    "                subset_selected,\n",
    "                taxa_selected,\n",
    "                pn.layout.Divider(),\n",
    "                indicator_usage,\n",
    "                toggle,\n",
    "                ],\n",
    "        main=[pn.Column(\n",
    "                tabs,\n",
    "            )],\n",
    "        main_layout=None,\n",
    "        accent=ACCENT,\n",
    "    )\n",
    "    return template\n",
    "\n",
    "template = app()\n",
    "\n",
    "# stupid trick to trigger updata()\n",
    "color_factor_granular.value = color_factor_granular.options[1]\n",
    "color_factor_granular.value = color_factor_granular.options[0]\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):  \n",
    "    s = serve_app(template, env=env, name=\"diversity_analysis\")\n",
    "else:\n",
    "    template.servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment this if running if running ngrok tunnel which you want to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use for the ngrok tunnel in GColab\n",
    "# close_server(s, env=env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "panel-cell-order": [
   "34c21507",
   "5911b6c2",
   "800d5521",
   "b8e2ef8c",
   "3994457e",
   "39906e39",
   "6de5ad4b",
   "bc1d1b61",
   "7485ce5b",
   "8f444fef"
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
