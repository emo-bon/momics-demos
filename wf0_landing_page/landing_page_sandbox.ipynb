{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display sequencing emo-bon efforts across European sites\n",
    "- At first showing the validated and relessed data from [emo-bon-data-validataion](https://github.com/emo-bon/emo-bon-data-validation/tree/main/validated-data)\n",
    "- Second, ask Cymon what metadata can be shown about data which are not ready/released yet.\n",
    "- I use `leafmap` for GIS integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system dependent setup\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "import logging\n",
    "logger = logging.getLogger(name=\"Diversity analysis app\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # clone the momics-demos repository to use the utils module from there\n",
    "    # TODO: eventually utils from momics will be used for that\n",
    "    try:\n",
    "        os.system('git clone https://github.com/palec87/momics-demos.git')\n",
    "        logger.info(f\"Repository cloned\")\n",
    "    except OSError as e:\n",
    "        logger.info(f\"An error occurred while cloning the repository: {e}\")\n",
    "\n",
    "    sys.path.insert(0,'/content/momics-demos')\n",
    "\n",
    "else:\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # local utils, to be removed in the future\n",
    "\n",
    "    # downside of this is that all the deps need to be installed in the current (momics-demos) environment\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../../marine-omics')))  # local momics package, to be removed too\n",
    "\n",
    "from utils import init_setup, get_notebook_environment\n",
    "init_setup()\n",
    "\n",
    "# Initialize the environment variable\n",
    "notebook_environment = 'unknown'\n",
    "# Determine the notebook environment\n",
    "env = get_notebook_environment()\n",
    "logger.info(f\"Environment: {env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "# import leafmap.leafmap as leafmap\n",
    "import leafmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data from the validated-data repo ran through pydantic by Cymon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_obs = \"https://raw.githubusercontent.com/emo-bon/emo-bon-data-validation/refs/heads/main/validated-data/Observatory_combined_logsheets_validated.csv\"\n",
    "url_metadata = \"https://raw.githubusercontent.com/emo-bon/emo-bon-data-validation/refs/heads/main/validated-data/Batch1and2_combined_logsheets_2024-11-12.csv\"\n",
    "\n",
    "\n",
    "df_obs = pd.read_csv(url_obs ,index_col=0)\n",
    "df_metadata = pd.read_csv(url_metadata ,index_col=0)\n",
    "\n",
    "df_obs.columns, df_metadata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = leafmap.Map(center=(50, 10), zoom=4)\n",
    "m.add_points_from_xy(\n",
    "    df_obs, x=\"longitude\", y=\"latitude\",\n",
    "    popup=['organization', \"contact_name\", \"contact_email\", \"ENA_accession_number_umbrella\", 'tot_depth_water_col'],\n",
    "    layer_name=\"EMO-BON Observatories\")\n",
    "# m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequencing data tracking from Cymon's spreadsheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seq_track_data(kind=\"FILTERS\"):\n",
    "    url = f\"https://docs.google.com/spreadsheets/d/1j9tRRsRCcyViDMTB1X7lx8POY1P5bV7UijxKKSebZAM/gviz/tq?tqx=out:csv&sheet={kind}\"\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    df_out = process_seq_track_data(df, kind)\n",
    "    return df_out\n",
    "\n",
    "\n",
    "def process_seq_track_data(df, kind):\n",
    "    df_out = df[df.columns[:17]]  # drop empty cols at the end\n",
    "    df_out.rename(columns={\"Unnamed: 16\": \"comments\"}, inplace=True)\n",
    "    df_out.drop(columns=[\"Comp. Resources\", \"Run Duration\"], inplace=True)\n",
    "    return df_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sediment = get_seq_track_data(\"SEDIMENTS\")\n",
    "df_filters = get_seq_track_data(\"FILTERS\")\n",
    "\n",
    "# for col in df_sediment.columns:\n",
    "#     print(f\"column: {col}\", df_sediment[col].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in df_filters.columns:\n",
    "#     print(f\"column: {col}\", df_filters[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a template for the TRACKING table from the `sequencing-crate` repo\n",
    "- sequence [repo](https://github.com/emo-bon/sequencing-crate/tree/main/shipment)\n",
    "- full will combine emo-bon-validation, sequencing-crate and metagoflow tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filters.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merging tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min version\n",
    "# columns from shipment: sourcce_mat_id, scientific_name, ref_code\n",
    "# this should run through the validataion process\n",
    "\n",
    "# columns from MGflow tracker: ref_code (to merge on), batch_number, seq_run_ro_crate_fname, forward_read_fname, backward_read_fname, run_status,\n",
    "# version, date_started, who, system_run, output_loc, output_size, notes\n",
    "\n",
    "ALL_SHIPMENTS = [\"001\", \"002\", \"003-0\", \"003-1\", \"003-2\"]\n",
    "\n",
    "def query_batch_shipment_data(batch_string):\n",
    "    url = f\"https://raw.githubusercontent.com/emo-bon/sequencing-crate/refs/heads/main/shipment/batch-{batch_string}/run-information-batch-{batch_string}.csv\"\n",
    "    df = pd.read_csv(url)\n",
    "    df['batch'] = batch_string\n",
    "\n",
    "\n",
    "    # I need to extract the sample type on this level\n",
    "    if \"source_material_id\" in df.columns:\n",
    "\n",
    "        # cycle through the source_material_id and extract the sample type\n",
    "        for i in range(len(df)):\n",
    "            if \"_Wa_\" in df.loc[i, \"source_material_id\"]:\n",
    "                df.loc[i, \"sample_type\"] = \"filters\"\n",
    "            else:\n",
    "                df.loc[i, \"sample_type\"] = \"sediment\"\n",
    "\n",
    "            if \"blank\" in df.loc[i, \"source_material_id\"].lower():\n",
    "                df.loc[i, \"sample_type\"] = df.loc[i, \"sample_type\"] + \"_blank\"\n",
    "\n",
    "    else:\n",
    "        # cycle through the old_source_mat_id and extract the sample type\n",
    "        for i in range(len(df)):\n",
    "            if \"_Wa_\" in df.loc[i, \"old_source_mat_id\"]:\n",
    "                df.loc[i, \"sample_type\"] = \"filters\"\n",
    "            else:\n",
    "                df.loc[i, \"sample_type\"] = \"sediment\"\n",
    "\n",
    "            if \"blank\" in df.loc[i, \"old_source_mat_id\"].lower():\n",
    "                df.loc[i, \"sample_type\"] = df.loc[i, \"sample_type\"] + \"_blank\"\n",
    "\n",
    "    return df\n",
    "\n",
    "def query_all_shipment_data():\n",
    "    df = pd.concat([query_batch_shipment_data(batch) for batch in ALL_SHIPMENTS], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "def query_track_data():\n",
    "    df_sed = get_seq_track_data(\"SEDIMENTS\")\n",
    "    df_filt = get_seq_track_data(\"FILTERS\")\n",
    "\n",
    "    # concatenate the two dataframes\n",
    "    df = pd.concat([df_sed, df_filt], ignore_index=True)\n",
    "\n",
    "    # rename certain columns\n",
    "    df.rename(columns={\"Seq Run RO-Crate Filename\": \"seq_run_ro_crate_fname\"}, inplace=True)\n",
    "    df.rename(columns={\"Forward Read Filename\": \"forward_read_fname\"}, inplace=True)\n",
    "    df.rename(columns={\"BackwardRead Filename\": \"backward_read_fname\"}, inplace=True)\n",
    "    df.rename(columns={\"Output Location\": \"output_loc\"}, inplace=True)\n",
    "\n",
    "    # rename columns to replace the space with underscore and make them lowercase\n",
    "    df.columns = df.columns.str.replace(\" \", \"_\").str.lower()\n",
    "    return df\n",
    "\n",
    "def infer_sample_type(df):\n",
    "    df[\"sample_type\"] = df[\"old_source_mat_id\"].apply(lambda x: \"filters\" if \"_Wa_\" in x else \"sediment\")\n",
    "    # append _blamk if in the lowercase of source_mat_id_orig\n",
    "    # df[\"sample_type\"] = df[\"old_source_mat_id\"].str.lower().apply(lambda x: x + \"_blank\" if \"blank\" in x else x)\n",
    "    df[\"sample_type\"] = df[\"sample_type\"].apply(lambda x: x + \"_blank\" if \"blank\" in df[\"source_mat_id_orig\"].str.lower() else x)\n",
    "    return df\n",
    "\n",
    "def min_merge(df_shipment, df_tracking):\n",
    "    df_shipment[\"ref_code\"] = df_shipment[\"ref_code\"].str.replace(\" \", \"\")\n",
    "    df_tracking[\"ref_code\"] = df_tracking[\"ref_code\"].str.replace(\" \", \"\")\n",
    "\n",
    "    df_shipment = df_shipment[[\"ref_code\", \"batch\", \"sample_type\"]]\n",
    "\n",
    "    df_tracking = df_tracking[[\"ref_code\", \"seq_run_ro_crate_fname\", \"forward_read_fname\",\n",
    "                               \"backward_read_fname\", \"run_status\", \"version\", \"date_started\",\n",
    "                               \"who\", \"system_run\", \"output_loc\", \"output_size\"]]\n",
    "\n",
    "    df = df_shipment.merge(df_tracking, on=\"ref_code\", how=\"left\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_shipments = query_all_shipment_data()\n",
    "df_tracking = query_track_data()\n",
    "# df_shipments.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full = min_merge(df_shipments, df_tracking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_full.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save as csv\n",
    "df_full.to_csv(\"min_merged.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momicsdem",
   "language": "python",
   "name": "momicsdem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
