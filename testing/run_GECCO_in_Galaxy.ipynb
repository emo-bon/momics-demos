{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interacting with Galaxy through the API to run the tool GECCO to identify putative novel Biosynthetic Gene Clusters (BGCs)\n",
    "\n",
    "**Information about GECCO:** https://github.com/zellerlab/GECCO <br>\n",
    "\n",
    "**Information about Galaxy** <br>\n",
    "Training: https://training.galaxyproject.org/ <br>\n",
    "Galaxy for Earth System and Environment: https://earth-system.usegalaxy.eu/ (DP: this one works)<br>\n",
    "European Galaxy server: https://usegalaxy.eu/ (DP: I did not manage to run GECCO there as of 25-01-21)<br>\n",
    "\n",
    "**Questions:**\n",
    "How do I solve storage of job and file `IDs` which I need to query later?\n",
    "  - Should be compatible for running locally and on the BC VRE\n",
    "  - For now use `.json`\n",
    "  - This file is created upon submission, but needs to be updated after the job is done. How if the user logs-out.\n",
    "  - The analysis NB should be the one querying the results `IDs`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Installing and importing required modules <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "logger = logging.getLogger(name=\"GECCO galaxy runner\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    # clone the momics-demos repository to use the utils module from there\n",
    "    # TODO: eventually utils from momics will be used for that\n",
    "    try:\n",
    "        os.system('git clone https://github.com/palec87/momics-demos.git')\n",
    "        logger.info(f\"Repository cloned\")\n",
    "    except OSError as e:\n",
    "        logger.info(f\"An error occurred while cloning the repository: {e}\")\n",
    "\n",
    "    sys.path.insert(0,'/content/momics-demos')\n",
    "\n",
    "else:\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))  # local utils, to be removed in the future\n",
    "\n",
    "    # downside of this is that all the deps need to be installed in the current (momics-demos) environment\n",
    "    sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '../../../marine-omics')))  # local momics package, to be removed too\n",
    "\n",
    "from utils import init_setup, get_notebook_environment\n",
    "init_setup()\n",
    "\n",
    "# Initialize the environment variable\n",
    "notebook_environment = 'unknown'\n",
    "# Determine the notebook environment\n",
    "env = get_notebook_environment()\n",
    "logger.info(f\"Environment: {env}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "from datetime import datetime\n",
    "from platform import python_version\n",
    "import logging\n",
    "\n",
    "# Import\n",
    "import bioblend.galaxy as g  # BioBlend is a Python library, wrapping the functionality of Galaxy and CloudMan APIs\n",
    "# import boto3\n",
    "import pandas as pd\n",
    "from bioblend.galaxy import GalaxyInstance\n",
    "from bioblend.galaxy.datasets import DatasetClient\n",
    "\n",
    "from momics.galaxy.blue_cloud import BCGalaxy\n",
    "# instead of the jupyter magic, you can also use\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Galaxy setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to create a galaxy API key?\n",
    "\n",
    "Code [here](https://github.com/galaxyproject/bioblend/blob/main/docs/examples/create_user_get_api_key.py). *If you already have login at Galaxy*, go to User(top right) -> Preferences -> Manage API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read your secrets from the .env file\n",
    "# To see your API key login -> click 'user' (top right) -> click 'preferences' -> click 'Manage API Key' (menu to the left) -> click the icon to 'copy key'\n",
    "GALAXY_URL = os.getenv(\"GALAXY_EARTH_URL\")  # alternatively os.environ.get('GALAXY_URL'), \"https://earth-system.usegalaxy.eu/\"\n",
    "GALAXY_KEY = os.getenv(\"GALAXY_EARTH_KEY\")  # alternatively os.environ.get('GALAXY_KEY')\n",
    "\n",
    "history_name = \"GECCO Run\"\n",
    "# setup for gecco and galaxy\n",
    "upload_data_flag = False\n",
    "gecco_tool_id = \"toolshed.g2.bx.psu.edu/repos/althonos/gecco/gecco/0.9.6\"  # The id of the tool GECCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to Galaxy instance\n",
    "gi = GalaxyInstance(url=GALAXY_URL, key=GALAXY_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = gi.histories.get_histories()\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a new history for the GECCO run named `GECCO Run`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "history = gi.histories.create_history(name=history_name)\n",
    "history_id = history[\"id\"]\n",
    "print(history_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Upload input files to the Galaxy history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the file to upload to Jupyter (here using a sample fasta file in the folder 'data')\n",
    "# file_path = \"data/EMOBON00092_final_V2.contigs.fa\"  # Ensure the file is in your working directory\n",
    "file_path = \"../input_gecco/EMOBON00092_final_V2.contigs.fa\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Upload file\n",
    "upload_data = gi.tools.upload_file(file_path, history_id)\n",
    "uploaded_dataset_id = upload_data[\"outputs\"][0][\"id\"]\n",
    "print(\n",
    "    f\"File uploaded to Galaxy with dataset ID: {uploaded_dataset_id}\"\n",
    ")  # dataset ID might be usefull bellow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing code\n",
    "dc = DatasetClient(gi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi.datasets.get_datasets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dc.get_datasets(history_id=history_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run GECCO in Galaxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tool_info = gi.tools.show_tool(gecco_tool_id)\n",
    "print(tool_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## method to find all your available datasets on galaxy\n",
    "# this method is called upon pressing a button in the webapp\n",
    "def filter_datasets_by_key(datasets, key, value):\n",
    "    lst_dict = [k for k in datasets if key in k and k[key] == value]\n",
    "    names = [(k[\"name\"], k['id']) for k in lst_dict]\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not upload_data_flag:\n",
    "    dname, did = filter_datasets_by_key(gi.datasets.get_datasets(), \"extension\", 'fasta')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define inputs for the GECCO tool with additional parameters\n",
    "\n",
    "if upload_data_flag:\n",
    "    inputs = {\n",
    "        \"input\": {\n",
    "            \"id\": uploaded_dataset_id,  # The dataset ID from the upload step\n",
    "            \"src\": \"hda\",  # History Dataset Association\n",
    "        },\n",
    "        \"mask\": True,  # Enable masking of regions with unknown nucleotides\n",
    "        \"cds\": 3,  # Minimum number of genes required for a cluster\n",
    "        \"threshold\": 0.05,  # Probability threshold for cluster detection\n",
    "        \"postproc\": \"gecco\",  # Post-processing method for gene cluster validation\n",
    "        \"antismash_sideload\": False,  # ,  # Generate an antiSMASH v6 sideload JSON file\n",
    "        #'email': 'email@email.pt'  # Email notification\n",
    "    }\n",
    "else:\n",
    "    inputs = {\n",
    "        \"input\": {\n",
    "            \"id\": did,  # The dataset ID from the upload step\n",
    "            \"src\": \"hda\",  # History Dataset Association\n",
    "        },\n",
    "        \"mask\": True,  # Enable masking of regions with unknown nucleotides\n",
    "        \"cds\": 3,  # Minimum number of genes required for a cluster\n",
    "        \"threshold\": 0.05,  # Probability threshold for cluster detection\n",
    "        \"postproc\": \"gecco\",  # Post-processing method for gene cluster validation\n",
    "        \"antismash_sideload\": False,  # ,  # Generate an antiSMASH v6 sideload JSON file\n",
    "        #'email': 'email@email.pt'  # Email notification\n",
    "    }\n",
    "\n",
    "# Run the GECCO tool\n",
    "tool_run = gi.tools.run_tool(\n",
    "    history_id=history_id, tool_id=gecco_tool_id, tool_inputs=inputs\n",
    ")\n",
    "\n",
    "# Get job ID to monitor\n",
    "job_id = tool_run[\"jobs\"][0][\"id\"]\n",
    "print(f\"GECCO tool job submitted with job ID: {job_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gi.jobs.cancel_job(job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the `.json` file locally for the job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_galaxy_job_json(tool_id: str, job_id: str, history_id: str):\n",
    "    # Store the job information in a JSON file\n",
    "    job_info = gi.jobs.show_job(job_id)\n",
    "    job_info[\"tool_id\"] = tool_id\n",
    "    job_info[\"history_id\"] = history_id\n",
    "    job_info[\"job_id\"] = job_id\n",
    "    \n",
    "    ## Get the current datetime and format it\n",
    "    # datetime_stamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    job_info_file = f\"job_info_{job_id}.json\"\n",
    "    with open(job_info_file, \"w\") as f:\n",
    "        json.dump(job_info, f)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor the job status (get job id from running the previous cell)\n",
    "# gi.jobs.show_job(job_id), # 11ac94870d0bb33a4a74056d2ffeb889\n",
    "gi.jobs.get_state(job_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test, can I store the job info in a json file for a deleted job\n",
    "# yes it works\n",
    "store_galaxy_job_json(gecco_tool_id, job_id, history_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: this file needs to be also updated once the job is done and user accesses it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### List the Outputs from the Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Get history id from running the previous cell\n",
    "# List datasets in the history after the tool run\n",
    "datasets = gi.histories.show_history(history_id, contents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the `.tsv` table outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Identify the output dataset ids\n",
    "# To understand the output: https://git.lumc.nl/mflarralde/gecco\n",
    "target_names = {\n",
    "    \"GECCO summary of detected genes on data 1 (TSV)\": \"dataset_id_2\",\n",
    "    \"GECCO summary of detected features on data 1 (TSV)\": \"dataset_id_3\",\n",
    "    \"GECCO summary of detected BGCs on data 1 (TSV)\": \"dataset_id_4\",\n",
    "}\n",
    "\n",
    "# Initialize the dataset ID variables\n",
    "dataset_id_2 = None\n",
    "dataset_id_3 = None\n",
    "dataset_id_4 = None\n",
    "\n",
    "# Loop through the datasets and assign the IDs to the correct variable\n",
    "for dataset in datasets:\n",
    "    if dataset[\"name\"] in target_names:\n",
    "        if target_names[dataset[\"name\"]] == \"dataset_id_2\":\n",
    "            dataset_id_2 = dataset[\"id\"]\n",
    "        elif target_names[dataset[\"name\"]] == \"dataset_id_3\":\n",
    "            dataset_id_3 = dataset[\"id\"]\n",
    "        elif target_names[dataset[\"name\"]] == \"dataset_id_4\":\n",
    "            dataset_id_4 = dataset[\"id\"]\n",
    "\n",
    "# Display the results\n",
    "print(f\"Dataset ID 2: {dataset_id_2}\")\n",
    "print(f\"Dataset ID 3: {dataset_id_3}\")\n",
    "print(f\"Dataset ID 4: {dataset_id_4}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download here\n",
    "\n",
    "# Download the dataset (as TSV) to the 'data'folder\n",
    "tsv_data2 = gi.datasets.download_dataset(\n",
    "    dataset_id_2,\n",
    "    file_path=\"../data/summary_detected_genes.tsv\",\n",
    "    use_default_filename=False,\n",
    ")\n",
    "tsv_data3 = gi.datasets.download_dataset(\n",
    "    dataset_id_3,\n",
    "    file_path=\"../data/summary_detected_features.tsv\",\n",
    "    use_default_filename=False,\n",
    ")\n",
    "tsv_data4 = gi.datasets.download_dataset(\n",
    "    dataset_id_4, file_path=\"../data/summary_detected_BGC.tsv\", use_default_filename=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the TSV File into a panda DataFrame\n",
    "\n",
    "# df_detected_BGC = pd.read_csv('detected_BGC.tsv', sep='\\t')\n",
    "df_summary_detected_genes = pd.read_csv(\"../data/summary_detected_genes.tsv\", sep=\"\\t\")\n",
    "df_summary_detected_features = pd.read_csv(\"../data/summary_detected_features.tsv\", sep=\"\\t\")\n",
    "df_summary_detected_BGC = pd.read_csv(\"../data/summary_detected_BGC.tsv\", sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the first few rows of each DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_detected_genes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_detected_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_summary_detected_BGC.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momicsdem",
   "language": "python",
   "name": "momicsdem"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
