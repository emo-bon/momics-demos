{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# App for analyzing GECCO runs from the Galaxy\n",
    "\n",
    "1. Upload local data or query results of the GECCO from the Galaxy.\n",
    "2. Identifying Biosynthetic Gene Clusters (BGCs).\n",
    "3. Visualize BGCs.\n",
    "4. Compare two samples in respect to each other.\n",
    "\n",
    "Note: Sending GECCO jobs to Galaxy is part of another separate application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Platform dependent part\n",
    "- Resolve platform setup\n",
    "- the difference to local imports should be resolved by setting the VRE packages well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "import psutil\n",
    "from IPython import get_ipython\n",
    "\n",
    "logger = logging.getLogger(name=\"GECCO analyzer\")\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('Setting Google colab, you will need a ngrok account to make the dashboard display over the tunnel. \\\n",
    "    https://ngrok.com/')\n",
    "    # clone the momics-demos repository to use it to load data\n",
    "    try:\n",
    "        os.system('git clone https://github.com/palec87/momics-demos.git')\n",
    "        logger.info(f\"Repository cloned\")\n",
    "    except OSError as e:\n",
    "        logger.info(f\"An error occurred while cloning the repository: {e}\")\n",
    "\n",
    "    sys.path.insert(0,'/content/momics-demos')\n",
    "\n",
    "    # this step takes time beacause of many dependencies\n",
    "    os.system('pip install momics@git+https://github.com/emo-bon/marine-omics-methods.git@main')\n",
    "\n",
    "elif psutil.users() == []:\n",
    "    logger.info(\"Binder\")\n",
    "\n",
    "    logger.info('Binder will not allow you to upload the \".env\" file')\n",
    "    os.environ[\"GALAXY_EARTH_URL\"] = \"https://earth-system.usegalaxy.eu/\"\n",
    "    ###########################################################################################\n",
    "    ### INPUT TOKEN HERE, If not using Galaxy, put any string below, but cannot stay empty ####\n",
    "    ###########################################################################################\n",
    "    os.environ[\"GALAXY_EARTH_KEY\"] = \"\"\n",
    "    assert os.environ[\"GALAXY_EARTH_KEY\"] != \"\", \"token cannot be an empty string, SET your API key.\"\n",
    "\n",
    "else:\n",
    "    logger.info(\"Local server\")\n",
    "\n",
    "from momics.utils import init_setup, get_notebook_environment, memory_load, reconfig_logger\n",
    "\n",
    "# Set up logging\n",
    "reconfig_logger()\n",
    "\n",
    "init_setup()\n",
    "\n",
    "# Determine the notebook environment\n",
    "env = get_notebook_environment()\n",
    "logger.info(f\"Environment: {env}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be repeated here for the Pannel dashboard to work, WEIRD\n",
    "# TODO: report as possible bug\n",
    "import sys\n",
    "import os\n",
    "import io\n",
    "\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "\n",
    "# Import\n",
    "import bioblend.galaxy as g  # BioBlend is a Python library, wrapping the functionality of Galaxy and CloudMan APIs\n",
    "from bioblend.galaxy import GalaxyInstance\n",
    "\n",
    "# All low level functions are imported from the momics package\n",
    "import momics.diversity as div\n",
    "import momics.plotting as pl\n",
    "from momics.panel_utils import serve_app, close_server\n",
    "from momics.loader import bytes_to_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering\n",
    "from collections import defaultdict\n",
    "from time import time\n",
    "\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import metrics\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "extra_stop_words = [\n",
    "    'domain', 'pfam', 'protein', 'family', 'superfamily', 'clan', 'interpro',\n",
    "    'et', 'al', 'cite', '[[', ']]', '(', ')', 'figure', 'fig', 'table', 'tab', 'see',\n",
    "    'also', 'example', 'examples', 'exampled', 'exampled', 'exemplary', 'exemplaryd',\n",
    "    'https', 'www', 'doi', 'review', 'swissprot', 'uniprot', 'org', 'ncbi', 'pubmed', 'pubmedcentral',\n",
    "    'ncbi', 'genbank', 'refseq', 'genome', 'genomic', 'gene', 'genes', 'protein', 'proteins',\n",
    "    'sequence', 'sequences', 'seq', 'seqs', 'nucleotide', 'nucleotides', 'amino', 'acids',\n",
    "    'acid', 'acids', 'aa', 'aa', 'nt', 'nts', 'ntseq', 'ntseqs', 'ntseqd', 'ntseqd',\n",
    "]\n",
    "\n",
    "stop_words = list(text.ENGLISH_STOP_WORDS.union(extra_stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True  # enable stdout logging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    root_folder = os.path.abspath(os.path.join('/content/momics-demos'))\n",
    "else:\n",
    "    root_folder = os.path.abspath(os.path.join('../'))\n",
    "\n",
    "assets_folder = os.path.join(root_folder, 'assets')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DF display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension(\"tabulator\", \"mathjax\", \"filedropper\")\n",
    "pn.extension(notifications=True)\n",
    "DATASETS = {}\n",
    "\n",
    "FILTERED_domains = None\n",
    "PFAM_dict = {}\n",
    "ACCENT = \"teal\"\n",
    "\n",
    "styles = {\n",
    "    \"box-shadow\": \"rgba(50, 50, 93, 0.25) 0px 6px 12px -2px, rgba(0, 0, 0, 0.3) 0px 3px 7px -3px\",\n",
    "    \"border-radius\": \"4px\",\n",
    "    \"padding\": \"10px\",\n",
    "}\n",
    "\n",
    "image = pn.pane.JPG(os.path.join(assets_folder, \"figs/logo_gecco.jpeg\"),\n",
    "                    width=100, height=100)\n",
    "\n",
    "md_upload = pn.pane.Markdown(\n",
    "\"\"\"\n",
    "**Upload your files:**\n",
    "You need to select exactly 3 files where filenames contain:\n",
    "- `BGCs` or `clusters`\n",
    "- `features`\n",
    "- `genes`\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "md_get_galaxy = pn.pane.Markdown(\n",
    "\"\"\"\n",
    "**Get files from Galaxy:**\n",
    "You need to provide your `Galaxy API` credentials and `job_id`.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "upload_local = pn.widgets.FileInput(\n",
    "    multiple=True,\n",
    ")\n",
    "\n",
    "literal_galaxy_url = pn.widgets.TextInput(\n",
    "    name='Galaxy server URL',\n",
    "    placeholder='Enter a https server address here...',\n",
    ")\n",
    "\n",
    "literal_galaxy_key = pn.widgets.PasswordInput(\n",
    "    name='Password',\n",
    "    placeholder='Enter your password here...',\n",
    ")\n",
    "\n",
    "button_display_loaded = pn.widgets.Button(\n",
    "    name=\"Display loaded files\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BGC types display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bgc_types = pn.pane.HoloViews(\n",
    "    height=600,\n",
    "    width=1000,\n",
    "    name=\"BGC types\",\n",
    "    )\n",
    "\n",
    "viz_tab = pn.Column(\n",
    "    bgc_types,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pfam API calls tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from time import sleep\n",
    "import json\n",
    "\n",
    "# plot the domain abundance\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import holoviews as hv\n",
    "import hvplot.pandas  # noqa\n",
    "\n",
    "PLOT_FACE_COLOR = \"#e6e6e6\"\n",
    "\n",
    "\n",
    "def construct_pfam_url(pfam_id):\n",
    "    \"\"\"\n",
    "    Construct the URL for the PFAM database.\n",
    "    \"\"\"\n",
    "    return f\"https://www.ebi.ac.uk/interpro/api/entry/pfam/{pfam_id}\"\n",
    "\n",
    "\n",
    "def filter_domain(df, abundance_cutoff=500):\n",
    "    \"\"\"\n",
    "    Filter the domain abundance data.\n",
    "    \"\"\"\n",
    "    # Filter out domains with abundance less than abundance_cutoff\n",
    "    s = df.domain.value_counts()\n",
    "    filtered_domains = s[s > abundance_cutoff]\n",
    "    return filtered_domains\n",
    "\n",
    "\n",
    "def extract_from_pfam_query(api_decode):\n",
    "    \"\"\"\n",
    "    Extract data from the returned API call.\n",
    "    \"\"\"\n",
    "    slim_keys = ['accession', 'name', 'description', 'integrated']\n",
    "    return {k: api_decode['metadata'][k] for k in slim_keys}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def api_loop_pfam(domains: pd.Series, n_calls: int = 10):\n",
    "    pfam_dict = {}\n",
    "    discarded = {}  # not possible to get complete data from the API\n",
    "    # check if n_calls is less than the number of domains\n",
    "    if n_calls > len(domains):\n",
    "        n_calls = len(domains)\n",
    "        logger.info(f\"n_calls is greater than the number of domains, setting to {n_calls}\")\n",
    "\n",
    "    for pfam_id in tqdm(domains.index[:n_calls], desc=\"API calls progress\", leave=True, colour='#666666'):\n",
    "        logger.info(f\"Fetching data for PFAM ID: {pfam_id}\")\n",
    "        url = construct_pfam_url(pfam_id)\n",
    "        req = request.Request(url)\n",
    "\n",
    "        try:\n",
    "            res = request.urlopen(req)\n",
    "        except request.HTTPError as e:\n",
    "            logger.error(f\"HTTP error: {e.code} - {e.reason}\")\n",
    "            if e.code == 404:\n",
    "                logger.warning('Not found, skipping')\n",
    "                discarded[pfam_id] = None\n",
    "            elif e.code == 410:\n",
    "                logger.warning('Discarded, Gone from the database, ie obsolete')\n",
    "            else:\n",
    "                logger.warning('Unknown error, skipping')\n",
    "                discarded[pfam_id] = None\n",
    "            continue\n",
    "\n",
    "        if res.status == 408:\n",
    "            logger.info('sleeping so skip request')\n",
    "\n",
    "        payload = json.loads(res.read().decode())\n",
    "        metadata = extract_from_pfam_query(payload)\n",
    "        logger.info(f\"metadata: {metadata}\")\n",
    "\n",
    "        if metadata['description'] is None:  # some pfams do not have description\n",
    "            logger.info('No description, trying to fetch one from the IPR')\n",
    "            ipr_id = payload['metadata']['integrated']\n",
    "            logger.info(f\"ipr_id: {ipr_id}\")\n",
    "            if ipr_id is None:\n",
    "                logger.warning('No IPR ID, skipping')\n",
    "                discarded[pfam_id] = metadata\n",
    "                continue\n",
    "            url = f\"https://www.ebi.ac.uk/interpro/api/entry/interpro/{ipr_id}\"\n",
    "            logger.info(f\"Fetching data for IPR ID: {ipr_id}\")\n",
    "\n",
    "            # API call\n",
    "            req = request.Request(url)\n",
    "            res = request.urlopen(req)\n",
    "            if res.status == 408:\n",
    "                logger.info('sleeping so skip request')\n",
    "\n",
    "            payload = json.loads(res.read().decode())\n",
    "            metadata['description'] = payload['metadata']['description']\n",
    "    \n",
    "        pfam_dict[pfam_id] = metadata\n",
    "        sleep(0.5)\n",
    "    return pfam_dict, discarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_pfam = pn.pane.Markdown(\n",
    "\"\"\"\n",
    "**Features table** contains pfam ids for the identified proteins.\n",
    "- Histogram shows counts of each pfam id over all the contigs.\n",
    "- To cluster the pfam domains by function, we query the pfam database for their description.\n",
    "- Select how many calls (ordered by counts in the histogram) you want to make.\n",
    "- Each call takes approximately 0.7 second.\n",
    "- Returned values are filtered and stored in the dictionary (also in your working directory).\n",
    "-   The dictionary `json` is saved in the working directory with a flag from abundance cutoff value, to facilitate loading.\n",
    "- In the next tab, you can **tokenize**, **embed** and **cluster** the description of the pfam domains.\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "abundance_cutoff = pn.widgets.IntInput(\n",
    "    name='Abundance cutoff',\n",
    "    value=500, step=50, start=50, end=10000,\n",
    "    align=\"center\",\n",
    ")\n",
    "\n",
    "n_calls = pn.widgets.IntInput(\n",
    "    name='Number of API calls',\n",
    "    value=50, step=10, start=20, end=500,\n",
    ")\n",
    "\n",
    "histogram = pn.pane.HoloViews(\n",
    "    height=500,\n",
    "    name=\"Histogram\",\n",
    ")\n",
    "\n",
    "# buttons\n",
    "button_histogram = pn.widgets.Button(\n",
    "    name=\"Plot histogram\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    "    align=\"end\",\n",
    ")\n",
    "\n",
    "button_run_api = pn.widgets.Button(\n",
    "    name=\"Run API\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    "    align=\"end\",\n",
    ")\n",
    "\n",
    "button_load_pfam_dict = pn.widgets.Button(\n",
    "    name=\"Load PFAM dict\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    "    align=\"end\",\n",
    ")\n",
    "\n",
    "tqdm = pn.widgets.Tqdm(\n",
    "    width=200,\n",
    "    align=(\"end\", 'end'),\n",
    ")\n",
    "\n",
    "pfam_tab = pn.Column(\n",
    "    markdown_pfam,\n",
    "    pn.Row(\n",
    "        abundance_cutoff,\n",
    "        button_histogram,\n",
    "    ),\n",
    "    pn.Row(\n",
    "        n_calls,\n",
    "        button_load_pfam_dict,\n",
    "        button_run_api,\n",
    "        tqdm,\n",
    "    ),\n",
    "    histogram,\n",
    "    scroll=True,\n",
    "    sizing_mode=\"stretch_both\",\n",
    ")\n",
    "\n",
    "def load_pfam_dict():\n",
    "    \"\"\"\n",
    "    Load the PFAM dictionary from a file\n",
    "    \"\"\"\n",
    "    global PFAM_dict\n",
    "    try:\n",
    "        with open(os.path.join(f'pfam_dict_cutoff_{abundance_cutoff.value}.json'), 'r') as f:\n",
    "            PFAM_dict = json.load(f)\n",
    "    except FileNotFoundError:\n",
    "        logger.warning(f\"File not found: {os.path.join(f'pfam_dict_cutoff_{abundance_cutoff.value}.json')}\")\n",
    "        pn.state.notifications.warning(\n",
    "            f\"File not found: {os.path.join(f'pfam_dict_cutoff_{abundance_cutoff.value}.json')}\",\n",
    "            duration=2000)\n",
    "        return\n",
    "    logger.info(f\"PFAM_dict loaded from {os.path.join(f'pfam_dict_cutoff_{abundance_cutoff.value}.json')}\")\n",
    "\n",
    "\n",
    "def run_api(n_calls):\n",
    "    \"\"\"\n",
    "    Run the API calls to get the pfam descriptions\n",
    "    \"\"\"\n",
    "    if FILTERED_domains is None or not isinstance(FILTERED_domains, pd.Series):\n",
    "        logger.info(\"No datasets loaded or FILTERED_domains is not a pandas Series\")\n",
    "        pn.state.notifications.warning('Plot histogram, which filters domains.', duration=2000)\n",
    "        return\n",
    "    PFAM_dict, discarded = api_loop_pfam(FILTERED_domains, n_calls)\n",
    "\n",
    "    if discarded is None or not isinstance(discarded, pd.Series):\n",
    "       logger.warning(f\"Discarded PFAM IDs: {len(discarded)}\")\n",
    "       pn.state.notifications.info(\n",
    "           f\"Not found and discarded PFAM IDs: {len(discarded)}\",\n",
    "            duration=2000)\n",
    "       \n",
    "    # save the PFAM_dict to a file\n",
    "    with open(os.path.join(f'pfam_dict_cutoff_{abundance_cutoff.value}.json'), 'w') as f:\n",
    "        json.dump(PFAM_dict, f)\n",
    "    logger.info(f\"PFAM_dict saved to {os.path.join(f'pfam_dict_cutoff_{abundance_cutoff.value}.json')}\")\n",
    "\n",
    "\n",
    "def filter_histogram(abundance_cutoff):\n",
    "    \"\"\"\n",
    "    Filter the features table from DATASETS and plot the histogram\n",
    "    \"\"\"\n",
    "    if DATASETS == {}:\n",
    "        logger.info(\"No datasets loaded\")\n",
    "        pn.state.notifications.warning('You have to load datasets first.', duration=2000)\n",
    "        return\n",
    "    df = DATASETS[next((key for key in DATASETS if 'features' in key))]\n",
    "    # Filter out domains with abundance less than abundance_cutoff\n",
    "    global FILTERED_domains\n",
    "    FILTERED_domains = filter_domain(df, abundance_cutoff)\n",
    "    logger.info(f\"Filtered domains: {FILTERED_domains}\")\n",
    "\n",
    "    # plot the histogram\n",
    "    histogram.object = pl.plot_domain_abundance(FILTERED_domains, abundance_cutoff)\n",
    "\n",
    "\n",
    "## Buttons ##\n",
    "button_histogram.on_click(\n",
    "    lambda event: filter_histogram(abundance_cutoff.value)\n",
    ")\n",
    "\n",
    "button_run_api.on_click(\n",
    "    lambda event: run_api(n_calls.value)\n",
    ")\n",
    "\n",
    "button_load_pfam_dict.on_click(\n",
    "    lambda event: load_pfam_dict()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_domains(pfam_dict, n_clusters=5, stop_words='english'):\n",
    "    \"\"\"\n",
    "    Cluster the domains using KMeans clustering.\n",
    "    \"\"\"\n",
    "    # Extract the domain descriptions\n",
    "    descriptions = [v['description'][0]['text'] for v in pfam_dict.values()]\n",
    "    logger.info(f\"Number of descriptions: {len(descriptions)}\")\n",
    "    # strip <p> tags\n",
    "    descriptions = [desc.replace('<p>', '').replace('</p>', '') for desc in descriptions]\n",
    "    logger.info(f'descriptions: {descriptions}')\n",
    "    # Create a TF-IDF vectorizer\n",
    "    vectorizer = TfidfVectorizer(\n",
    "        stop_words=stop_words,\n",
    "        max_df=0.15,\n",
    "    )\n",
    "    X = vectorizer.fit_transform(descriptions)\n",
    "\n",
    "    # Perform KMeans clustering\n",
    "    kmeans = KMeans(init='k-means++', n_clusters=n_clusters, n_init=4, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "\n",
    "    # Get the cluster labels\n",
    "    labels = kmeans.labels_\n",
    "\n",
    "    # Create a dictionary to store the clusters\n",
    "    clusters = {}\n",
    "    for i, label in enumerate(labels):\n",
    "        if label not in clusters:\n",
    "            clusters[label] = []\n",
    "        clusters[label].append(list(pfam_dict.keys())[i])\n",
    "\n",
    "    return clusters, labels, X, kmeans, vectorizer\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, labels, name=None, n_runs=5):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    logger.info(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        # logger.info(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    return evaluation, evaluation_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "markdown_cluster = pn.pane.Markdown(\n",
    "\"\"\"\n",
    "From the dictionary, the domain descriptions are vectorized and clustered.\n",
    "- Select the number of clusters you want to create.\n",
    "- The clustering is done using the `k-means` algorithm.\n",
    "- The most important words from the clusters are extracted and displayed.\n",
    "\"\"\")\n",
    "\n",
    "n_clusters = pn.widgets.IntInput(\n",
    "    name='Number of clusters',\n",
    "    value=5, step=1, start=2, end=50,\n",
    ")\n",
    "\n",
    "n_important_words = pn.widgets.IntInput(\n",
    "    name='Number of important words',\n",
    "    value=10, step=1, start=5, end=30,\n",
    ")\n",
    "\n",
    "perplexity = pn.widgets.IntInput(\n",
    "    name='Perplexity',\n",
    "    value=10, step=1, start=2, end=50,\n",
    ")\n",
    "\n",
    "n_components_LSA = pn.widgets.IntInput(\n",
    "    name='Number of components for LSA',\n",
    "    value=10, step=1, start=5, end=100,\n",
    ")\n",
    "\n",
    "button_cluster = pn.widgets.Button(\n",
    "    name=\"Cluster\",\n",
    "    button_type=\"primary\",\n",
    "    width=200,\n",
    ")\n",
    "\n",
    "tsne = pn.pane.HoloViews(\n",
    "    height=500,\n",
    "    name=\"t-SNE\",\n",
    ")\n",
    "\n",
    "vip_words = pn.widgets.Tabulator()\n",
    "reports = pn.widgets.Tabulator(\n",
    "    name=\"Clustering reports\",\n",
    "    sizing_mode=\"stretch_both\",\n",
    ")\n",
    "\n",
    "cluster_tab = pn.Column(\n",
    "    markdown_cluster,\n",
    "    pn.Row(\n",
    "        n_clusters,\n",
    "        perplexity,\n",
    "        n_important_words,\n",
    "    ),\n",
    "    pn.Row(\n",
    "        n_components_LSA,\n",
    "    ),\n",
    "    button_cluster,\n",
    "    pn.Row(\n",
    "        tsne,\n",
    "        vip_words,\n",
    "    ),\n",
    "    reports,\n",
    "    scroll=True,\n",
    "    sizing_mode=\"stretch_both\",\n",
    ")\n",
    "\n",
    "def cluster():\n",
    "    similarity, labels, X_tfidf, kmeans, vectorizer = cluster_domains(\n",
    "        PFAM_dict,\n",
    "        n_clusters=n_clusters.value,\n",
    "        stop_words=stop_words,\n",
    "    )\n",
    "    \n",
    "    _, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "    logger.info(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "    # calculate the t-SNE\n",
    "    logger.info(f\"N samples: {X_tfidf.shape[0]}\")\n",
    "    if X_tfidf.shape[0] <= perplexity.value:\n",
    "        logger.warning(f\"Perplexity is greater than the number of samples, setting to {X_tfidf.shape[0] - 1}\")\n",
    "        perplexity.value = X_tfidf.shape[0] - 1\n",
    "\n",
    "    X_embedded = TSNE(\n",
    "        n_components=2,\n",
    "        learning_rate='auto',\n",
    "        init='random',\n",
    "        perplexity=perplexity.value,\n",
    "    ).fit_transform(X_tfidf.toarray())\n",
    "\n",
    "    logger.info(X_embedded.shape)\n",
    "    tsne.object = pl.plot_tsne(X_embedded, kmeans)\n",
    "\n",
    "    # LSA\n",
    "    logger.info(f\"data shape: {X_tfidf.shape}\")\n",
    "    if X_tfidf.shape[1] <= n_components_LSA.value:\n",
    "        logger.warning(f\"Number of components is greater than the number of features, setting to {X_tfidf.shape[1] - 1}\")\n",
    "        n_components_LSA.value = X_tfidf.shape[1] - 1\n",
    "    lsa = make_pipeline(\n",
    "        TruncatedSVD(n_components=n_components_LSA.value),\n",
    "        Normalizer(copy=False),\n",
    "    )\n",
    "    t0 = time()\n",
    "    X_lsa = lsa.fit_transform(X_tfidf)\n",
    "    explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "    logger.info(f\"LSA done in {time() - t0:.3f} s\")\n",
    "    logger.info(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")\n",
    "\n",
    "    # K-means again\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=n_clusters.value,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "    )\n",
    "\n",
    "    evaluation, evaluation_std = fit_and_evaluate(kmeans, X_lsa, labels, name=\"KMeans\\nwith LSA on tf-idf vectors\")\n",
    "    # Combine values and standard deviations into a single DataFrame\n",
    "    stats = {\n",
    "        \"Metric\": evaluation.keys(),\n",
    "        \"Value\": evaluation.values(),\n",
    "        \"Std\": evaluation_std.values(),\n",
    "    }\n",
    "\n",
    "    df = pd.DataFrame(stats)\n",
    "    # drop row with \"estimator\" in the name\n",
    "    df = df[~df[\"Metric\"].str.contains(\"estimator\")]\n",
    "    # round the values to 3 decimal places\n",
    "    df[\"Value\"] = pd.to_numeric(df[\"Value\"], errors=\"coerce\").round(4)\n",
    "    df[\"Std\"] = pd.to_numeric(df[\"Std\"], errors=\"coerce\").round(4)\n",
    "    df = df.set_index(\"Metric\")\n",
    "    # change index name\n",
    "    df.index.name = \"KMeans\\nwith LSA on tf-idf vectors\"\n",
    "    # Display the DataFrame\n",
    "    reports.value = df\n",
    "\n",
    "    original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\n",
    "    order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    # Collect cluster terms into a dictionary\n",
    "    cluster_terms = {}\n",
    "    for i in range(kmeans.n_clusters):\n",
    "        cluster_terms[f\"Cluster {i}\"] = [terms[ind] for ind in order_centroids[i, :n_important_words.value]]\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    df_clusters = pd.DataFrame.from_dict(cluster_terms, orient=\"index\")#.transpose()\n",
    "    vip_words.value = df_clusters\n",
    "\n",
    "\n",
    "button_cluster.on_click(\n",
    "    lambda event: cluster()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tabs = pn.Tabs(\n",
    "    ('BGCs',\"\"),\n",
    "    ('features', \"\"),\n",
    "    ('genes', \"\"),\n",
    "    (\"BGC types\", viz_tab),\n",
    "    (\"pfam API calls\", pfam_tab),\n",
    "    (\"Cluster pfam\", cluster_tab),\n",
    "    dynamic=True,\n",
    "    styles=styles,\n",
    "    sizing_mode=\"stretch_both\",\n",
    "    margin=10,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### update methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_tables_after_upload(datasets):\n",
    "    logger.info(\"Displaying tables after upload...\")\n",
    "    tabs.__setitem__(0,\n",
    "                     pn.widgets.Tabulator(\n",
    "                        datasets[next((key for key in datasets if 'BGCs' in key or 'clusters' in key))],\n",
    "                        name='BGCs',\n",
    "                        page_size=50,\n",
    "                    ),\n",
    "                    )\n",
    "    tabs.__setitem__(1,\n",
    "                     pn.widgets.Tabulator(\n",
    "                        datasets[next((key for key in datasets if 'features' in key))],\n",
    "                        name='features',\n",
    "                        page_size=50,\n",
    "                    ),\n",
    "                    )\n",
    "    tabs.__setitem__(2,\n",
    "                     pn.widgets.Tabulator(\n",
    "                        datasets[next((key for key in datasets if 'genes' in key))],\n",
    "                        name='genes',\n",
    "                        page_size=50,\n",
    "                    ),\n",
    "                    )\n",
    "\n",
    "\n",
    "def process_uploaded_tables(file_names, file_data):\n",
    "    \"\"\"\n",
    "    Process the uploaded tables and display them in a tabular format.\n",
    "    \"\"\"\n",
    "    logger.info(\"Processing uploaded tables...\")\n",
    "    DATASETS.clear()\n",
    "    if file_data is None:\n",
    "        pn.state.notifications.warning(\n",
    "            'Files not loaded yet, try again soon.',\n",
    "            duration=2000,\n",
    "            )\n",
    "        return\n",
    "    logger.info(f\"files: {file_names}\")\n",
    "    for i, name in enumerate(file_names):\n",
    "        logger.info(f\"Processing {name}...\")\n",
    "        DATASETS[name] = bytes_to_df(file_data[i])\n",
    "    # Display the first table\n",
    "    display_tables_after_upload(DATASETS)\n",
    "    update_bgs_types_plot()\n",
    "\n",
    "\n",
    "def update_bgs_types_plot():\n",
    "    \"\"\"\n",
    "    Update the BGC types plot.\n",
    "    \"\"\"\n",
    "    logger.info(\"Updating BGC types plot...\")\n",
    "    bgc_types.object = pl.hvplot_bgcs_violin(\n",
    "        DATASETS[next((key for key in DATASETS if 'BGCs' in key or 'clusters' in key))],\n",
    "        normalize=False,\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bindings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "button_display_loaded.on_click(\n",
    "    lambda event: process_uploaded_tables(upload_local.filename, upload_local.value)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## APP setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.extension(\"tabulator\", \"mathjax\")\n",
    "\n",
    "def app():\n",
    "    template = pn.template.FastListTemplate(\n",
    "        title=\"Biosynthetic Gene Cluster Analysis\",\n",
    "        sidebar=[image,\n",
    "                md_upload, \n",
    "                pn.Row(upload_local),\n",
    "                pn.layout.Divider(margin=(-20, 0, 0, 0)),\n",
    "                # this is prepared for galaxy, but not implemented yet\n",
    "                # md_get_galaxy, literal_galaxy_url, literal_galaxy_key,\n",
    "                button_display_loaded,\n",
    "                ],\n",
    "        main=[pn.Column(\n",
    "            # markdown_intro,\n",
    "            # pn.layout.Divider(margin=(-20, 0, 0, 0)),\n",
    "            tabs,\n",
    "            scroll=True,\n",
    "        )],\n",
    "        main_layout=None,\n",
    "        accent=ACCENT,\n",
    "    )\n",
    "    return template\n",
    "\n",
    "template = app()\n",
    "\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):  \n",
    "    s = serve_app(template, env=env, name=\"GECCO_analyser\")\n",
    "else:\n",
    "    template.servable()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uncomment this if running if running ngrok tunnel which you want to quit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only use for the ngrok tunnel in GColab\n",
    "# close_server(s, env=env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momics-demos",
   "language": "python",
   "name": "momics-demos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
