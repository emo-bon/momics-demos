{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f05842fe-abe3-4f43-a341-5e44aa879126",
   "metadata": {},
   "source": [
    "<h1> Beta diversity <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f24b32-af5a-4c88-a39e-54cdb10e183b",
   "metadata": {},
   "source": [
    "<h3> Installing and importing required modules <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "10a1b39e-6844-4ce6-9d3b-e687bbacc843",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-bio in /opt/conda/lib/python3.10/site-packages (0.6.2)\n",
      "Requirement already satisfied: requests>=2.20.0 in /opt/conda/lib/python3.10/site-packages (from scikit-bio) (2.31.0)\n",
      "Requirement already satisfied: decorator>=3.4.2 in /opt/conda/lib/python3.10/site-packages (from scikit-bio) (4.4.2)\n",
      "Requirement already satisfied: natsort>=4.0.3 in /opt/conda/lib/python3.10/site-packages (from scikit-bio) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in /opt/conda/lib/python3.10/site-packages (from scikit-bio) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /opt/conda/lib/python3.10/site-packages (from scikit-bio) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.9.0 in /opt/conda/lib/python3.10/site-packages (from scikit-bio) (1.10.1)\n",
      "Requirement already satisfied: h5py>=3.6.0 in /opt/conda/lib/python3.10/site-packages (from scikit-bio) (3.8.0)\n",
      "Requirement already satisfied: biom-format>=2.1.16 in /opt/conda/lib/python3.10/site-packages (from scikit-bio) (2.1.16)\n",
      "Requirement already satisfied: statsmodels>=0.14.0 in /opt/conda/lib/python3.10/site-packages (from scikit-bio) (0.14.0)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from biom-format>=2.1.16->scikit-bio) (7.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->scikit-bio) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->scikit-bio) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5.0->scikit-bio) (2023.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->scikit-bio) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->scikit-bio) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->scikit-bio) (2.0.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.20.0->scikit-bio) (2023.7.22)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.14.0->scikit-bio) (0.5.3)\n",
      "Requirement already satisfied: packaging>=21.3 in /opt/conda/lib/python3.10/site-packages (from statsmodels>=0.14.0->scikit-bio) (23.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels>=0.14.0->scikit-bio) (1.16.0)\n",
      "Requirement already satisfied: duckdb in /opt/conda/lib/python3.10/site-packages (1.1.2)\n",
      "Requirement already satisfied: pingouin in /opt/conda/lib/python3.10/site-packages (0.5.5)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.10/site-packages (from pingouin) (3.7.1)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pingouin) (1.24.3)\n",
      "Requirement already satisfied: pandas>=1.5 in /opt/conda/lib/python3.10/site-packages (from pingouin) (2.0.2)\n",
      "Requirement already satisfied: pandas-flavor in /opt/conda/lib/python3.10/site-packages (from pingouin) (0.6.0)\n",
      "Requirement already satisfied: scikit-learn>=1.2 in /opt/conda/lib/python3.10/site-packages (from pingouin) (1.2.2)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from pingouin) (1.10.1)\n",
      "Requirement already satisfied: seaborn in /opt/conda/lib/python3.10/site-packages (from pingouin) (0.11.0)\n",
      "Requirement already satisfied: statsmodels in /opt/conda/lib/python3.10/site-packages (from pingouin) (0.14.0)\n",
      "Requirement already satisfied: tabulate in /opt/conda/lib/python3.10/site-packages (from pingouin) (0.9.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5->pingouin) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5->pingouin) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas>=1.5->pingouin) (2023.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.2->pingouin) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=1.2->pingouin) (3.1.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pingouin) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pingouin) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pingouin) (4.39.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pingouin) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pingouin) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pingouin) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/conda/lib/python3.10/site-packages (from matplotlib->pingouin) (3.0.9)\n",
      "Requirement already satisfied: xarray in /opt/conda/lib/python3.10/site-packages (from pandas-flavor->pingouin) (2023.9.0)\n",
      "Requirement already satisfied: patsy>=0.5.2 in /opt/conda/lib/python3.10/site-packages (from statsmodels->pingouin) (0.5.3)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from patsy>=0.5.2->statsmodels->pingouin) (1.16.0)\n",
      "Requirement already satisfied: minio in /opt/conda/lib/python3.10/site-packages (7.2.10)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from minio) (2023.7.22)\n",
      "Requirement already satisfied: urllib3 in /opt/conda/lib/python3.10/site-packages (from minio) (2.0.2)\n",
      "Requirement already satisfied: argon2-cffi in /opt/conda/lib/python3.10/site-packages (from minio) (21.3.0)\n",
      "Requirement already satisfied: pycryptodome in /opt/conda/lib/python3.10/site-packages (from minio) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from minio) (4.6.2)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/conda/lib/python3.10/site-packages (from argon2-cffi->minio) (21.2.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/conda/lib/python3.10/site-packages (from argon2-cffi-bindings->argon2-cffi->minio) (1.15.1)\n",
      "Requirement already satisfied: pycparser in /opt/conda/lib/python3.10/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->minio) (2.21)\n",
      "Requirement already satisfied: pandasql in /opt/conda/lib/python3.10/site-packages (0.7.3)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from pandasql) (1.24.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from pandasql) (2.0.2)\n",
      "Requirement already satisfied: sqlalchemy in /opt/conda/lib/python3.10/site-packages (from pandasql) (2.0.15)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->pandasql) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->pandasql) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->pandasql) (2023.3)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy->pandasql) (4.6.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /opt/conda/lib/python3.10/site-packages (from sqlalchemy->pandasql) (2.0.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->pandasql) (1.16.0)\n",
      "Requirement already satisfied: timeit_decorator in /opt/conda/lib/python3.10/site-packages (1.1.1)\n",
      "Requirement already satisfied: tabulate>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from timeit_decorator) (0.9.0)\n"
     ]
    }
   ],
   "source": [
    "#Install\n",
    "!pip install scikit-bio\n",
    "!pip install duckdb\n",
    "!pip install pingouin\n",
    "!pip install minio\n",
    "!pip install pandasql\n",
    "!pip install timeit_decorator\n",
    "\n",
    "#Import\n",
    "import io\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import warnings\n",
    "\n",
    "import duckdb\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "from duckdb import BinderException, CatalogException\n",
    "from IPython.display import display\n",
    "from minio import Minio, S3Error\n",
    "from pandasql import sqldf\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.special import entr  # Ensure you import entropy\n",
    "from scipy.stats import entropy, levene, shapiro\n",
    "from skbio.stats.distance import DistanceMatrix, permanova\n",
    "from sklearn.decomposition import PCA\n",
    "from timeit_decorator import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63cd1bf9-cc47-4958-afd9-0913aeb65494",
   "metadata": {},
   "source": [
    "<h3> Downloading data from Minio <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d982bc0c-4414-48d2-b120-c9f8983c5402",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "MaxRetryError",
     "evalue": "HTTPConnectionPool(host='10.4.1.4', port=9000): Max retries exceeded with url: /emo-bon-tables?location= (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f27cfb5b700>, 'Connection to 10.4.1.4 timed out. (connect timeout=300)'))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTimeoutError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:200\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m socket\u001b[38;5;241m.\u001b[39mgaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/util/connection.py:85\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 85\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m err\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/util/connection.py:73\u001b[0m, in \u001b[0;36mcreate_connection\u001b[0;34m(address, timeout, source_address, socket_options)\u001b[0m\n\u001b[1;32m     72\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[0;32m---> 73\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[0;31mTimeoutError\u001b[0m: [Errno 110] Connection timed out",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    789\u001b[0m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    795\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    796\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    797\u001b[0m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    798\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    799\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    800\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    802\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    803\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    805\u001b[0m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:496\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    495\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 496\u001b[0m     \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43menforce_content_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menforce_content_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;66;03m# We are swallowing BrokenPipeError (errno.EPIPE) since the server is\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;66;03m# legitimately able to close the connection after sending a valid response.\u001b[39;00m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;66;03m# With this behaviour, the received response is still readable.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:388\u001b[0m, in \u001b[0;36mHTTPConnection.request\u001b[0;34m(self, method, url, body, headers, chunked, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mputheader(header, value)\n\u001b[0;32m--> 388\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mendheaders\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;66;03m# If we're given a body we start sending that in chunks.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1278\u001b[0m, in \u001b[0;36mHTTPConnection.endheaders\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CannotSendHeader()\n\u001b[0;32m-> 1278\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_output\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmessage_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencode_chunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencode_chunked\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:1038\u001b[0m, in \u001b[0;36mHTTPConnection._send_output\u001b[0;34m(self, message_body, encode_chunked)\u001b[0m\n\u001b[1;32m   1037\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_buffer[:]\n\u001b[0;32m-> 1038\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1040\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m message_body \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1041\u001b[0m \n\u001b[1;32m   1042\u001b[0m     \u001b[38;5;66;03m# create a consistent interface to message_body\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/http/client.py:976\u001b[0m, in \u001b[0;36mHTTPConnection.send\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mauto_open:\n\u001b[0;32m--> 976\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:236\u001b[0m, in \u001b[0;36mHTTPConnection.connect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconnect\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 236\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tunnel_host:\n\u001b[1;32m    238\u001b[0m         \u001b[38;5;66;03m# If we're tunneling it means we're connected to our proxy.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connection.py:209\u001b[0m, in \u001b[0;36mHTTPConnection._new_conn\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m SocketTimeout \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ConnectTimeoutError(\n\u001b[1;32m    210\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    211\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhost\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m timed out. (connect timeout=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimeout\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    212\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mConnectTimeoutError\u001b[0m: (<urllib3.connection.HTTPConnection object at 0x7f27cfb5b700>, 'Connection to 10.4.1.4 timed out. (connect timeout=300)')",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mMaxRetryError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m objects \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mlist_objects(bucket_name, recursive\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m objs \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, obj \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(objects):\n\u001b[1;32m     14\u001b[0m     objs\u001b[38;5;241m.\u001b[39mappend(obj\u001b[38;5;241m.\u001b[39mobject_name)\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrint index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mobj\u001b[38;5;241m.\u001b[39mobject_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/minio/api.py:3117\u001b[0m, in \u001b[0;36mMinio._list_objects\u001b[0;34m(self, bucket_name, continuation_token, delimiter, encoding_type, fetch_owner, include_user_meta, max_keys, prefix, start_after, version_id_marker, use_api_v1, include_version)\u001b[0m\n\u001b[1;32m   3114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m version_id_marker:\n\u001b[1;32m   3115\u001b[0m     query[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mversion-id-marker\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m version_id_marker\n\u001b[0;32m-> 3117\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3118\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3119\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3120\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDictType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3121\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3123\u001b[0m objects, is_truncated, start_after, version_id_marker \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   3124\u001b[0m     parse_list_objects(response)\n\u001b[1;32m   3125\u001b[0m )\n\u001b[1;32m   3127\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m include_version:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/minio/api.py:437\u001b[0m, in \u001b[0;36mMinio._execute\u001b[0;34m(self, method, bucket_name, object_name, body, headers, query_params, preload_content, no_body_trace)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute\u001b[39m(\n\u001b[1;32m    426\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    427\u001b[0m         method: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    434\u001b[0m         no_body_trace: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    435\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseHTTPResponse:\n\u001b[1;32m    436\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Execute HTTP request.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 437\u001b[0m     region \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_region\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_url_open(\n\u001b[1;32m    441\u001b[0m             method,\n\u001b[1;32m    442\u001b[0m             region,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    449\u001b[0m             no_body_trace\u001b[38;5;241m=\u001b[39mno_body_trace,\n\u001b[1;32m    450\u001b[0m         )\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/minio/api.py:494\u001b[0m, in \u001b[0;36mMinio._get_region\u001b[0;34m(self, bucket_name)\u001b[0m\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m region\n\u001b[1;32m    493\u001b[0m \u001b[38;5;66;03m# Execute GetBucketLocation REST API to get region of the bucket.\u001b[39;00m\n\u001b[0;32m--> 494\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_url_open\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mGET\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mus-east-1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbucket_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbucket_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43mquery_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlocation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    501\u001b[0m element \u001b[38;5;241m=\u001b[39m ET\u001b[38;5;241m.\u001b[39mfromstring(response\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mdecode())\n\u001b[1;32m    502\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m element\u001b[38;5;241m.\u001b[39mtext:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/minio/api.py:302\u001b[0m, in \u001b[0;36mMinio._url_open\u001b[0;34m(self, method, region, bucket_name, object_name, body, headers, query_params, preload_content, no_body_trace)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    300\u001b[0m         http_headers\u001b[38;5;241m.\u001b[39madd(key, value)\n\u001b[0;32m--> 302\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_http\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m    \u001b[49m\u001b[43murlunsplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhttp_headers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    308\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_stream:\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_trace_stream\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHTTP/1.1 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/poolmanager.py:433\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    431\u001b[0m     response \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39murlopen(method, url, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\n\u001b[1;32m    432\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 433\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mu\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    435\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:874\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    893\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:874\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    893\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "    \u001b[0;31m[... skipping similar frames: HTTPConnectionPool.urlopen at line 874 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:874\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn:\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Try again\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     log\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetrying (\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m) after connection broken by \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, retries, err, url\n\u001b[1;32m    873\u001b[0m     )\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    880\u001b[0m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    881\u001b[0m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    882\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpool_timeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpool_timeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    884\u001b[0m \u001b[43m        \u001b[49m\u001b[43mrelease_conn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrelease_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    885\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    886\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody_pos\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbody_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    887\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    888\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    889\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    890\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    892\u001b[0m \u001b[38;5;66;03m# Handle redirect?\u001b[39;00m\n\u001b[1;32m    893\u001b[0m redirect_location \u001b[38;5;241m=\u001b[39m redirect \u001b[38;5;129;01mand\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget_redirect_location()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/connectionpool.py:844\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    841\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_e, (\u001b[38;5;167;01mOSError\u001b[39;00m, HTTPException)):\n\u001b[1;32m    842\u001b[0m     new_e \u001b[38;5;241m=\u001b[39m ProtocolError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConnection aborted.\u001b[39m\u001b[38;5;124m\"\u001b[39m, new_e)\n\u001b[0;32m--> 844\u001b[0m retries \u001b[38;5;241m=\u001b[39m \u001b[43mretries\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mincrement\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_e\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_pool\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_stacktrace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexc_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    847\u001b[0m retries\u001b[38;5;241m.\u001b[39msleep()\n\u001b[1;32m    849\u001b[0m \u001b[38;5;66;03m# Keep track of the error for the retry warning.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/urllib3/util/retry.py:515\u001b[0m, in \u001b[0;36mRetry.increment\u001b[0;34m(self, method, url, response, error, _pool, _stacktrace)\u001b[0m\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m new_retry\u001b[38;5;241m.\u001b[39mis_exhausted():\n\u001b[1;32m    514\u001b[0m     reason \u001b[38;5;241m=\u001b[39m error \u001b[38;5;129;01mor\u001b[39;00m ResponseError(cause)\n\u001b[0;32m--> 515\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MaxRetryError(_pool, url, reason) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreason\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    517\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIncremented Retry for (url=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m): \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, url, new_retry)\n\u001b[1;32m    519\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m new_retry\n",
      "\u001b[0;31mMaxRetryError\u001b[0m: HTTPConnectionPool(host='10.4.1.4', port=9000): Max retries exceeded with url: /emo-bon-tables?location= (Caused by ConnectTimeoutError(<urllib3.connection.HTTPConnection object at 0x7f27cfb5b700>, 'Connection to 10.4.1.4 timed out. (connect timeout=300)'))"
     ]
    }
   ],
   "source": [
    "client = Minio(\n",
    "    \"10.4.1.4:9000\",\n",
    "    secure=False,\n",
    "    access_key=\"PapyfVxlHhHD63nJnB0W\",\n",
    "    secret_key=\"7Z2q9uD44CS2HEBGhbvJi0nhLcvjgffZwG9wqJ1j\",\n",
    ")\n",
    "\n",
    "# The destination bucket and filename on the MinIO server\n",
    "bucket_name = \"emo-bon-tables\"\n",
    "\n",
    "objects = client.list_objects(bucket_name, recursive=True)\n",
    "objs = []\n",
    "for i, obj in enumerate(objects):\n",
    "    objs.append(obj.object_name)\n",
    "    print(f\"Print index {i} has object {obj.object_name}\")\n",
    "\n",
    "table_name = objs[1]\n",
    "try:\n",
    "    response = client.get_object(bucket_name, table_name)\n",
    "    buffer = io.BytesIO(response.read())\n",
    "except S3Error:\n",
    "    raise\n",
    "finally:\n",
    "    df = pd.read_parquet(buffer, engine=\"pyarrow\")\n",
    "    response.close()\n",
    "    response.release_conn()\n",
    "df.info()\n",
    "print(f\"Downloaded {table_name} into df dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138b94d6-fd71-48b3-adbc-31526b4735f1",
   "metadata": {},
   "source": [
    "<h3> Here the user should be able to select a table (change the code below) <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860c6238-08d4-43fa-8259-57ece95dfdf0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    duckdb.sql(\"DROP TABLE SSU\")\n",
    "except CatalogException:\n",
    "    pass\n",
    "duckdb.sql(\"CREATE TABLE SSU AS SELECT * FROM df\")\n",
    "duckdb.sql(\"SELECT COUNT(*) FROM SSU\")\n",
    "\n",
    "# Query to get column names from the 'SSU' table\n",
    "column_headers = duckdb.sql(\n",
    "    \"SELECT column_name FROM information_schema.columns WHERE table_name = 'SSU';\"\n",
    ").fetchdf()\n",
    "\n",
    "# Print the column headers\n",
    "print(column_headers[\"column_name\"].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5858145-a158-49c6-ab0c-a21ccf548c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_object(bucket_name, file_format, file_name, verbose=True):\n",
    "    if verbose:\n",
    "        print(f\"{bucket_name=} - {file_format=} - {file_name=}\")\n",
    "    try:\n",
    "        response = client.get_object(bucket_name, file_name)\n",
    "        buffer = io.BytesIO(response.read())\n",
    "    except S3Error:\n",
    "        raise\n",
    "    finally:\n",
    "        if file_format == \"parquet\":\n",
    "            df = pd.read_parquet(buffer, engine=\"pyarrow\")\n",
    "        elif file_format == \"csv\":\n",
    "            df = pd.read_csv(buffer)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown {file_format=}\")\n",
    "        response.close()\n",
    "        response.release_conn()\n",
    "        if verbose:\n",
    "            print(f\"Downloaded {file_name} into dataframe\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84004ef0-03f3-4cbf-8d12-9779351eab5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The destination bucket and filename on the MinIO server\n",
    "#bucket_name = \"emo-bon-tables\"\n",
    "#for obj in client.list_objects(bucket_name):\n",
    "#    print(obj.object_name)\n",
    "# Currently only v1 tables are available"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dcfc447-74cf-401c-bf9c-3c2fd1181a4e",
   "metadata": {},
   "source": [
    "<h3> Working on full metadata <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47a9888-b238-47a6-b127-371d60b13e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MGF parquet tables to data frames\n",
    "bucket_name = \"emo-bon-tables\"\n",
    "objects = client.list_objects(bucket_name, recursive=True)\n",
    "mgf_parquet_dfs = {}\n",
    "for obj in objects:\n",
    "    name = obj.object_name.split(\".\")[-2]\n",
    "    df = get_object(bucket_name, \"parquet\", obj.object_name, verbose=False)\n",
    "    mgf_parquet_dfs[name] = df\n",
    "\n",
    "# Sample metadata\n",
    "# Get the latest Batch combined logsheets file\n",
    "# Remember we are downloading from MinIO\n",
    "batch_file = \"Batch1and2_combined_logsheets_2024-09-11.csv\"\n",
    "sample_metadata = (\"emo-bon-metadata-tables\", \"csv\", batch_file)\n",
    "sample_metadata = get_object(*sample_metadata, verbose=False)\n",
    "\n",
    "# Observatory metadata - from the GoogleSheets\n",
    "observatory_metadata = (\n",
    "    \"emo-bon-metadata-tables\",\n",
    "    \"csv\",\n",
    "    \"Observatory_combined_logsheets_validated.csv\",\n",
    ")\n",
    "observatory_metadata = get_object(*observatory_metadata, verbose=False)\n",
    "\n",
    "# Into duckdb\n",
    "try:\n",
    "    duckdb.sql(\"DROP TABLE SAMPLE_METADATA\")\n",
    "    duckdb.sql(\"DROP TABLE OBS_METADATA\")\n",
    "    for table_name in mgf_parquet_dfs:\n",
    "        cmd = f\"DROP TABLE {table_name}\"\n",
    "        duckdb.sql(cmd)\n",
    "except CatalogException:\n",
    "    pass\n",
    "duckdb.sql(\"CREATE TABLE SAMPLE_METADATA AS SELECT * FROM sample_metadata\")\n",
    "duckdb.sql(\"SELECT COUNT(*) FROM SAMPLE_METADATA\")\n",
    "duckdb.sql(\"CREATE TABLE OBS_METADATA AS SELECT * FROM observatory_metadata\")\n",
    "duckdb.sql(\"SELECT COUNT(*) FROM OBS_METADATA\")\n",
    "for table_name in mgf_parquet_dfs:\n",
    "    df = mgf_parquet_dfs[table_name]\n",
    "    cmd = f\"CREATE TABLE {table_name} AS SELECT * FROM df\"\n",
    "    duckdb.sql(cmd)\n",
    "\n",
    "duckdb.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efd038d-3cb6-49c0-a531-71b00a009163",
   "metadata": {},
   "outputs": [],
   "source": [
    "duckdb.sql(\"SHOW TABLES\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee61bd15-362b-46ab-ba6d-4930a3262832",
   "metadata": {},
   "source": [
    "<h3> Define the query <h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09215d3-aa9a-47f1-8234-82b84caf0cc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "PIVOT (\n",
    "SELECT\n",
    "ref_code,\n",
    "class,\n",
    "sum(abundance) as total\n",
    "FROM SSU \n",
    "WHERE class <> '' AND class IS NOT NULL\n",
    "GROUP BY class, ref_code\n",
    "ORDER BY class, sum(abundance) DESC\n",
    ")\n",
    "ON ref_code\n",
    "USING sum(total)\n",
    "ORDER BY class\n",
    "\"\"\"\n",
    "\n",
    "# Save the result of the query into a DuckDB table called \"SSU_class_all\"\n",
    "duckdb.sql(\n",
    "    f\"\"\"\n",
    "CREATE TABLE SSU_class_all AS\n",
    "{query}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Optionally, show the saved table to verify it\n",
    "duckdb.sql(\"SELECT * FROM SSU_class_all\").show(max_width=30, max_rows=55)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b262811e-ab91-43f0-8279-09f3016010ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### joining all the tables together - metadata and the SSU class level\n",
    "### NaN converted to 0, but the data needs standardisation and square rooting\n",
    "\n",
    "# Define the query for full_metadata\n",
    "query = \"\"\"\n",
    "SELECT OBS_METADATA.*,\n",
    "       SAMPLE_METADATA.*,\n",
    "FROM SAMPLE_METADATA\n",
    "INNER JOIN OBS_METADATA \n",
    "ON SAMPLE_METADATA.obs_id = OBS_METADATA.obs_id\n",
    "AND SAMPLE_METADATA.env_package = OBS_METADATA.env_package\n",
    "ORDER BY SAMPLE_METADATA.ref_code ASC\n",
    "\"\"\"\n",
    "\n",
    "# Get the full_metadata DataFrame\n",
    "full_metadata = duckdb.sql(query).df()\n",
    "\n",
    "# Define the query to get SSU_class_all data\n",
    "ssu_class_all_query = \"SELECT * FROM SSU_class_all\"\n",
    "\n",
    "# Get the SSU_class_all DataFrame\n",
    "ssu_class_all = duckdb.sql(ssu_class_all_query).df()\n",
    "\n",
    "# Step 1: Transpose the 'SSU_class_all' DataFrame\n",
    "ssu_class_all_transposed = ssu_class_all.transpose().reset_index()\n",
    "ssu_class_all_transposed.columns = [\"ref_code\"] + list(\n",
    "    ssu_class_all_transposed.iloc[0, 1:]\n",
    ")  # Set ref_code as the first column and the remaining as data\n",
    "ssu_class_all_transposed = ssu_class_all_transposed.drop(0).reset_index(\n",
    "    drop=True\n",
    ")  # Drop the first row used for column names\n",
    "\n",
    "# Step 2: Convert NaN values in the transposed 'SSU_class_all' to 0\n",
    "ssu_class_all_transposed = ssu_class_all_transposed.fillna(0)\n",
    "\n",
    "# Step 3: Add prefix to each column header from SSU_class_all\n",
    "ssu_class_all_transposed.columns = [\"ref_code\"] + [\n",
    "    \"class_\" + col for col in ssu_class_all_transposed.columns[1:]\n",
    "]\n",
    "\n",
    "# Step 4: Merge the transposed 'SSU_class_all' with 'full_metadata' on 'ref_code'\n",
    "merged_data = pd.merge(\n",
    "    full_metadata, ssu_class_all_transposed, on=\"ref_code\", how=\"inner\"\n",
    ")\n",
    "\n",
    "# Step 5: Reorder columns to place SSU_class_all columns at the beginning\n",
    "ssu_class_all_columns = [\n",
    "    col for col in ssu_class_all_transposed.columns if col != \"ref_code\"\n",
    "]\n",
    "other_columns = [\n",
    "    col\n",
    "    for col in merged_data.columns\n",
    "    if col not in ssu_class_all_columns and col != \"ref_code\"\n",
    "]\n",
    "\n",
    "# Define new column order: SSU_class_all columns first, then other columns, followed by 'ref_code'\n",
    "new_column_order = ssu_class_all_columns + other_columns + [\"ref_code\"]\n",
    "merged_data = merged_data[new_column_order]\n",
    "\n",
    "# Display the reordered merged data (optional)\n",
    "print(merged_data)\n",
    "\n",
    "# Export the result to CSV\n",
    "output_csv = \"merged_data.csv\"  # Specify your file path here\n",
    "\n",
    "duckdb.sql(\n",
    "    f\"\"\"\n",
    "COPY (\n",
    "    {query}\n",
    ") TO '{output_csv}' WITH (FORMAT CSV, HEADER TRUE)\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a6ca06-5b7a-4ad8-9799-fc658961c4ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the query to get SSU_class_all\n",
    "ssu_class_all_query = \"SELECT * FROM SSU_class_all\"\n",
    "\n",
    "# Get the SSU_class_all DataFrame\n",
    "ssu_class_all = duckdb.sql(ssu_class_all_query).df()\n",
    "\n",
    "# Option 1: Print the full DataFrame\n",
    "print(ssu_class_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f228ccdc-7f74-42c3-9cfb-4a05bcbd9aa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the function to calculate Shannon index\n",
    "\n",
    "\n",
    "def calculate_shannon_index(df):\n",
    "    def shannon_index(row):\n",
    "        total_abundance = row.sum()\n",
    "        if total_abundance == 0:\n",
    "            return np.nan\n",
    "        relative_abundance = row / total_abundance\n",
    "        return entropy(\n",
    "            relative_abundance, base=np.e\n",
    "        )  # Shannon entropy (natural log base)\n",
    "\n",
    "    return df.apply(shannon_index, axis=1)\n",
    "\n",
    "\n",
    "# Extract numeric columns and metadata\n",
    "numeric_columns = [col for col in merged_data.columns if col.startswith(\"class_\")]\n",
    "metadata_columns = [\n",
    "    col\n",
    "    for col in merged_data.columns\n",
    "    if not col.startswith(\"class_\") and col != \"ref_code\"\n",
    "]\n",
    "\n",
    "# Convert columns to numeric and replace NaN with 0\n",
    "merged_data[numeric_columns] = (\n",
    "    merged_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    ")\n",
    "\n",
    "# Calculate Shannon index for each sample\n",
    "shannon_values = calculate_shannon_index(merged_data[numeric_columns])\n",
    "\n",
    "# Create a DataFrame for Shannon index values\n",
    "shannon_df = pd.DataFrame(\n",
    "    {\"ref_code\": merged_data[\"ref_code\"], \"Shannon\": shannon_values}\n",
    ")\n",
    "\n",
    "# Plot the Shannon index for each sample\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.bar(shannon_df[\"ref_code\"], shannon_df[\"Shannon\"], color=\"skyblue\")\n",
    "plt.xlabel(\"Sample\")\n",
    "plt.ylabel(\"Shannon Index\")\n",
    "plt.title(\"Shannon Index for Each Sample\")\n",
    "plt.xticks(rotation=90)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "096ea382-5d0e-4368-a766-5356ef42590f",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = [col for col in merged_data.columns if col.startswith(\"class_\")]\n",
    "metadata_columns = [\n",
    "    col\n",
    "    for col in merged_data.columns\n",
    "    if not col.startswith(\"class_\") and col != \"ref_code\"\n",
    "]\n",
    "\n",
    "# Convert columns to numeric and replace NaN with 0\n",
    "merged_data[numeric_columns] = (\n",
    "    merged_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    ")\n",
    "\n",
    "# Calculate Shannon index for each sample\n",
    "shannon_values = calculate_shannon_index(merged_data[numeric_columns])\n",
    "\n",
    "\n",
    "# Example if metadata is a list of dictionaries\n",
    "metadata = pd.DataFrame(metadata)\n",
    "\n",
    "\n",
    "# Function to calculate Shannon index\n",
    "def calculate_shannon_index(df):\n",
    "    def shannon_index(row):\n",
    "        total_abundance = row.sum()\n",
    "        if total_abundance == 0:\n",
    "            return np.nan\n",
    "        relative_abundance = row / total_abundance\n",
    "        return entr(relative_abundance).sum()  # Shannon entropy\n",
    "\n",
    "    return df.apply(shannon_index, axis=1)\n",
    "\n",
    "\n",
    "def calculate_alpha_diversity(df, metadata):\n",
    "    # Filter numeric columns for Shannon calculation\n",
    "    numeric_columns = [col for col in df.columns if col.startswith(\"class_\")]\n",
    "    transposed_data = df[numeric_columns].copy()  # Select only numeric columns\n",
    "    shannon_values = calculate_shannon_index(transposed_data)\n",
    "    alpha_diversity_df = pd.DataFrame(\n",
    "        {\"ref_code\": df[\"ref_code\"], \"Shannon\": shannon_values}\n",
    "    )\n",
    "    alpha_diversity_df = pd.merge(\n",
    "        alpha_diversity_df, metadata, on=\"ref_code\", how=\"left\"\n",
    "    )\n",
    "    return alpha_diversity_df\n",
    "\n",
    "\n",
    "def plot_shannon_index(alpha_diversity_df, selected_factor):\n",
    "    alpha_diversity_sorted = alpha_diversity_df.sort_values(by=selected_factor)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=\"ref_code\",\n",
    "        y=\"Shannon\",\n",
    "        hue=selected_factor,\n",
    "        data=alpha_diversity_sorted,\n",
    "        dodge=False,\n",
    "        palette=\"coolwarm\",\n",
    "        errorbar=None,\n",
    "    )\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Shannon Index\")\n",
    "    plt.title(f\"Shannon Index Grouped by {selected_factor}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_average_shannon_per_condition(alpha_diversity_df, selected_factor):\n",
    "    palette = sns.color_palette(\n",
    "        \"coolwarm\", len(alpha_diversity_df[selected_factor].unique())\n",
    "    )\n",
    "    grouped_data = alpha_diversity_df.groupby(selected_factor)[\"Shannon\"]\n",
    "    means = grouped_data.mean()\n",
    "    errors = grouped_data.sem()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    means.plot(kind=\"bar\", yerr=errors, color=palette, capsize=5)\n",
    "    plt.xlabel(selected_factor)\n",
    "    plt.ylabel(\"Average Shannon Index\")\n",
    "    plt.title(f\"Average Shannon Index by {selected_factor}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def check_normality(data):\n",
    "    \"\"\"Check normality using Shapiro-Wilk test\"\"\"\n",
    "    stat, p_value = shapiro(data)\n",
    "    return p_value > 0.05  # True if data is normally distributed\n",
    "\n",
    "\n",
    "def check_homogeneity_of_variances(groups):\n",
    "    \"\"\"Check homogeneity of variances using Levene's test\"\"\"\n",
    "    stat, p_value = levene(*groups)\n",
    "    return p_value > 0.05  # True if variances are equal\n",
    "\n",
    "\n",
    "def run_anova_and_posthoc(alpha_diversity_df, selected_factor):\n",
    "    # ANOVA for the selected factor\n",
    "    anova_results = pg.anova(\n",
    "        data=alpha_diversity_df, dv=\"Shannon\", between=selected_factor\n",
    "    )\n",
    "\n",
    "    # Create a string to capture the output\n",
    "    anova_output = \"\\nANOVA Results:\\n\"\n",
    "    anova_output += anova_results.to_string(index=False)\n",
    "\n",
    "    # Check the distribution of data\n",
    "    grouped_data = alpha_diversity_df.groupby(selected_factor)[\"Shannon\"]\n",
    "    groups = [group for name, group in grouped_data]\n",
    "\n",
    "    normal = check_normality(alpha_diversity_df[\"Shannon\"])\n",
    "    homogeneity = check_homogeneity_of_variances(groups)\n",
    "\n",
    "    normality_result = f\"Normality check: {'Pass' if normal else 'Fail'}\"\n",
    "    homogeneity_result = (\n",
    "        f\"Homogeneity of variances check: {'Pass' if homogeneity else 'Fail'}\"\n",
    "    )\n",
    "\n",
    "    anova_output += f\"\\n{normality_result}\\n{homogeneity_result}\"\n",
    "\n",
    "    if anova_results[\"p-unc\"].values[0] < 0.05:\n",
    "        posthoc_results_str = \"\"\n",
    "        if normal and homogeneity:\n",
    "            # Tukey's HSD for normal data and equal variances\n",
    "            posthoc_results = pg.pairwise_ttests(\n",
    "                data=alpha_diversity_df,\n",
    "                dv=\"Shannon\",\n",
    "                between=selected_factor,\n",
    "                padjust=\"holm\",\n",
    "            )\n",
    "            posthoc_results_str = \"\\nPost-hoc test used: Tukey's HSD\\n\"\n",
    "            posthoc_results_str += posthoc_results.to_string(index=False)\n",
    "        else:\n",
    "            # Games-Howell for non-normal data or unequal variances\n",
    "            posthoc_results = pg.pairwise_gameshowell(\n",
    "                data=alpha_diversity_df, dv=\"Shannon\", between=selected_factor\n",
    "            )\n",
    "            posthoc_results_str = \"\\nPost-hoc test used: Games-Howell\\n\"\n",
    "            posthoc_results_str += posthoc_results.to_string(index=False)\n",
    "\n",
    "        anova_output += f\"\\n\\n{posthoc_results_str}\"\n",
    "    else:\n",
    "        anova_output += f\"\\nNo significant difference found in {selected_factor}. Post-hoc test is not necessary.\"\n",
    "\n",
    "    return anova_output\n",
    "\n",
    "\n",
    "def update_plot(change):\n",
    "    with output_plot:\n",
    "        output_plot.clear_output(wait=True)\n",
    "        selected_factor = color_factor_dropdown.value\n",
    "        alpha_diversity_df = calculate_alpha_diversity(\n",
    "            data, metadata\n",
    "        )  # Ensure 'data' and 'metadata' are defined\n",
    "        plot_shannon_index(alpha_diversity_df, selected_factor)\n",
    "        plot_average_shannon_per_condition(alpha_diversity_df, selected_factor)\n",
    "\n",
    "        # Run ANOVA and display results\n",
    "        anova_output = run_anova_and_posthoc(alpha_diversity_df, selected_factor)\n",
    "        print(anova_output)  # Print ANOVA results to the output area\n",
    "\n",
    "\n",
    "# Ensure metadata is a DataFrame and contains valid columns\n",
    "if isinstance(metadata, pd.DataFrame):\n",
    "    options = [\n",
    "        col\n",
    "        for col in metadata.columns\n",
    "        if not col.startswith(\"class_\") and col != \"ref_code\"\n",
    "    ]\n",
    "else:\n",
    "    print(\"metadata is not a DataFrame. Please check the definition.\")\n",
    "    options = []\n",
    "\n",
    "# Widget Definition for Color Factor\n",
    "color_factor_dropdown = widgets.Dropdown(\n",
    "    options=options,\n",
    "    description=\"Color Factor:\",\n",
    ")\n",
    "# Output area for plots\n",
    "output_plot = widgets.Output()\n",
    "\n",
    "# Link the dropdown to the update function\n",
    "color_factor_dropdown.observe(update_plot, names=\"value\")\n",
    "\n",
    "# Display widgets\n",
    "display(color_factor_dropdown, output_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1104e8b-8374-452f-a7f8-6a9a84cfa725",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metadata.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b61ba6-29a7-42e6-9538-e9e9cb93d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming merged_data is a DataFrame already defined\n",
    "numeric_columns = [col for col in merged_data.columns if col.startswith(\"class_\")]\n",
    "metadata_columns = [\n",
    "    col\n",
    "    for col in merged_data.columns\n",
    "    if not col.startswith(\"class_\") and col != \"ref_code\"\n",
    "]\n",
    "\n",
    "# Convert columns to numeric and replace NaN with 0\n",
    "merged_data[numeric_columns] = (\n",
    "    merged_data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    ")\n",
    "\n",
    "\n",
    "# Calculate Shannon index for each sample\n",
    "def calculate_shannon_index(df):\n",
    "    def shannon_index(row):\n",
    "        total_abundance = row.sum()\n",
    "        if total_abundance == 0:\n",
    "            return np.nan\n",
    "        relative_abundance = row / total_abundance\n",
    "        return entr(relative_abundance).sum()  # Shannon entropy\n",
    "\n",
    "    return df.apply(shannon_index, axis=1)\n",
    "\n",
    "\n",
    "shannon_values = calculate_shannon_index(merged_data[numeric_columns])\n",
    "\n",
    "# Ensure metadata is a DataFrame\n",
    "metadata = pd.DataFrame(metadata) if isinstance(metadata, (list, dict)) else metadata\n",
    "\n",
    "\n",
    "def calculate_alpha_diversity(df, metadata):\n",
    "    numeric_columns = [col for col in df.columns if col.startswith(\"class_\")]\n",
    "    transposed_data = df[numeric_columns].copy()\n",
    "    shannon_values = calculate_shannon_index(transposed_data)\n",
    "    alpha_diversity_df = pd.DataFrame(\n",
    "        {\"ref_code\": df[\"ref_code\"], \"Shannon\": shannon_values}\n",
    "    )\n",
    "    alpha_diversity_df = pd.merge(\n",
    "        alpha_diversity_df, metadata, on=\"ref_code\", how=\"left\"\n",
    "    )\n",
    "    return alpha_diversity_df\n",
    "\n",
    "\n",
    "def plot_shannon_index(alpha_diversity_df, selected_factor):\n",
    "    alpha_diversity_sorted = alpha_diversity_df.sort_values(by=selected_factor)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(\n",
    "        x=\"ref_code\",\n",
    "        y=\"Shannon\",\n",
    "        hue=selected_factor,\n",
    "        data=alpha_diversity_sorted,\n",
    "        dodge=False,\n",
    "        palette=\"coolwarm\",\n",
    "        errorbar=None,\n",
    "    )\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Shannon Index\")\n",
    "    plt.title(f\"Shannon Index Grouped by {selected_factor}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_average_shannon_per_condition(alpha_diversity_df, selected_factor):\n",
    "    palette = sns.color_palette(\n",
    "        \"coolwarm\", len(alpha_diversity_df[selected_factor].unique())\n",
    "    )\n",
    "    grouped_data = alpha_diversity_df.groupby(selected_factor)[\"Shannon\"]\n",
    "    means = grouped_data.mean()\n",
    "    errors = grouped_data.sem()\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    means.plot(kind=\"bar\", yerr=errors, color=palette, capsize=5)\n",
    "    plt.xlabel(selected_factor)\n",
    "    plt.ylabel(\"Average Shannon Index\")\n",
    "    plt.title(f\"Average Shannon Index by {selected_factor}\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def check_normality(data):\n",
    "    \"\"\"Check normality using Shapiro-Wilk test\"\"\"\n",
    "    stat, p_value = shapiro(data)\n",
    "    return p_value > 0.05  # True if data is normally distributed\n",
    "\n",
    "\n",
    "def check_homogeneity_of_variances(groups):\n",
    "    \"\"\"Check homogeneity of variances using Levene's test\"\"\"\n",
    "    stat, p_value = levene(*groups)\n",
    "    return p_value > 0.05  # True if variances are equal\n",
    "\n",
    "\n",
    "def run_anova_and_posthoc(alpha_diversity_df, selected_factor):\n",
    "    # ANOVA for the selected factor\n",
    "    anova_results = pg.anova(\n",
    "        data=alpha_diversity_df, dv=\"Shannon\", between=selected_factor\n",
    "    )\n",
    "\n",
    "    # Create a string to capture the output\n",
    "    anova_output = \"\\nANOVA Results:\\n\"\n",
    "    anova_output += anova_results.to_string(index=False)\n",
    "\n",
    "    # Check the distribution of data\n",
    "    grouped_data = alpha_diversity_df.groupby(selected_factor)[\"Shannon\"]\n",
    "    groups = [group for name, group in grouped_data]\n",
    "\n",
    "    normal = check_normality(alpha_diversity_df[\"Shannon\"])\n",
    "    homogeneity = check_homogeneity_of_variances(groups)\n",
    "\n",
    "    normality_result = f\"Normality check: {'Pass' if normal else 'Fail'}\"\n",
    "    homogeneity_result = (\n",
    "        f\"Homogeneity of variances check: {'Pass' if homogeneity else 'Fail'}\"\n",
    "    )\n",
    "\n",
    "    anova_output += f\"\\n{normality_result}\\n{homogeneity_result}\"\n",
    "\n",
    "    if anova_results[\"p-unc\"].values[0] < 0.05:\n",
    "        posthoc_results_str = \"\"\n",
    "        if normal and homogeneity:\n",
    "            # Tukey's HSD for normal data and equal variances\n",
    "            posthoc_results = pg.pairwise_ttests(\n",
    "                data=alpha_diversity_df,\n",
    "                dv=\"Shannon\",\n",
    "                between=selected_factor,\n",
    "                padjust=\"holm\",\n",
    "            )\n",
    "            posthoc_results_str = \"\\nPost-hoc test used: Tukey's HSD\\n\"\n",
    "            posthoc_results_str += posthoc_results.to_string(index=False)\n",
    "        else:\n",
    "            # Games-Howell for non-normal data or unequal variances\n",
    "            posthoc_results = pg.pairwise_gameshowell(\n",
    "                data=alpha_diversity_df, dv=\"Shannon\", between=selected_factor\n",
    "            )\n",
    "            posthoc_results_str = \"\\nPost-hoc test used: Games-Howell\\n\"\n",
    "            posthoc_results_str += posthoc_results.to_string(index=False)\n",
    "\n",
    "        anova_output += f\"\\n\\n{posthoc_results_str}\"\n",
    "    else:\n",
    "        anova_output += f\"\\nNo significant difference found in {selected_factor}. Post-hoc test is not necessary.\"\n",
    "\n",
    "    return anova_output\n",
    "\n",
    "\n",
    "def update_plot(change):\n",
    "    with output_plot:\n",
    "        output_plot.clear_output(wait=True)\n",
    "        selected_factor = color_factor_dropdown.value\n",
    "        alpha_diversity_df = calculate_alpha_diversity(\n",
    "            merged_data, metadata\n",
    "        )  # Ensure 'merged_data' and 'metadata' are defined\n",
    "        plot_shannon_index(alpha_diversity_df, selected_factor)\n",
    "        plot_average_shannon_per_condition(alpha_diversity_df, selected_factor)\n",
    "\n",
    "        # Run ANOVA and display results\n",
    "        anova_output = run_anova_and_posthoc(alpha_diversity_df, selected_factor)\n",
    "        print(anova_output)  # Print ANOVA results to the output area\n",
    "\n",
    "\n",
    "# Ensure metadata is a DataFrame\n",
    "if isinstance(metadata, pd.DataFrame):\n",
    "    color_factor_dropdown = widgets.Dropdown(\n",
    "        options=[\n",
    "            col\n",
    "            for col in metadata.columns\n",
    "            if not col.startswith(\"class_\") and col != \"ref_code\"\n",
    "        ],\n",
    "        description=\"Color Factor:\",\n",
    "    )\n",
    "else:\n",
    "    print(\"Error: metadata is not a DataFrame.\")\n",
    "    # Handle the error or initialize metadata properly\n",
    "\n",
    "# Output area for plots\n",
    "output_plot = widgets.Output()\n",
    "\n",
    "# Link the dropdown to the update function\n",
    "color_factor_dropdown.observe(update_plot, names=\"value\")\n",
    "\n",
    "# Display widgets\n",
    "display(color_factor_dropdown, output_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c6a025-3052-4826-9c1e-e5d6b25bf2d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### delete later\n",
    "\n",
    "# Step 1: Load the table from DuckDB into a DataFrame\n",
    "data = duckdb.sql(\"SELECT * FROM SSU_for_visualisation\").fetchdf()\n",
    "\n",
    "# Step 2: Set 'taxonomic_concat' as the index and transpose the DataFrame\n",
    "transposed_data = data.set_index(\"taxonomic_concat\").T\n",
    "\n",
    "# Step 3: Load the material information from the CSV file\n",
    "metadata = pd.read_csv(\"/home/jupyter-andrzej/metadata/metadata_jup_test4.csv\")\n",
    "\n",
    "# Step 4: Merge metadata with transposed data based on 'ref_code'\n",
    "merged_data = pd.merge(\n",
    "    transposed_data,\n",
    "    metadata[\n",
    "        [\"ref_code\"]\n",
    "        + [\n",
    "            col\n",
    "            for col in metadata.columns\n",
    "            if col not in [\"ref_code\", \"our_code\", \"#NAME\"]\n",
    "        ]\n",
    "    ],\n",
    "    left_index=True,\n",
    "    right_on=\"ref_code\",\n",
    ")\n",
    "\n",
    "# Exclude metadata columns from Shannon index calculation\n",
    "metadata_columns = [\n",
    "    \"filter_size\",\n",
    "    \"date\",\n",
    "    \"replcate\",\n",
    "    \"station_filter\",\n",
    "    \"filter_rep\",\n",
    "    \"location\",\n",
    "    \"country\",\n",
    "    \"column12\",\n",
    "    \"column13\",\n",
    "]\n",
    "numeric_columns = [\n",
    "    col for col in transposed_data.columns if col not in metadata_columns\n",
    "]\n",
    "\n",
    "\n",
    "#############################\n",
    "# Step 1: Load the table from DuckDB into a DataFrame\n",
    "data = merged_data  # Use 'merged_data'\n",
    "\n",
    "# Ensure that we only use numeric data for distance calculation\n",
    "numeric_columns = [\n",
    "    col for col in data.columns if col.startswith(\"class_\")\n",
    "]  # Columns with the prefix 'class_'\n",
    "metadata = [\n",
    "    col for col in data.columns if not col.startswith(\"class_\") and col != \"ref_code\"\n",
    "]\n",
    "\n",
    "# Convert the columns to numeric (if they are not already) and replace NaN with 0\n",
    "data[numeric_columns] = (\n",
    "    data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    ")\n",
    "# data[numeric_columns] = data[numeric_columns].astype(float).fillna(0)\n",
    "\n",
    "\n",
    "# Now all 'NaN' values in columns with names starting with 'class_' have been replaced with 0.\n",
    "\n",
    "\n",
    "############################\n",
    "\n",
    "\n",
    "# Function to calculate Shannon index\n",
    "# def calculate_shannon_index(df):\n",
    "#    def shannon_index(row):\n",
    "#        float_values = row\n",
    "#        if float_values.sum() == 0:\n",
    "#            return np.nan\n",
    "#        relative_abundance = float_values / float_values.sum()\n",
    "#        return entropy(relative_abundance, base=np.e)\n",
    "\n",
    "#    return df.apply(shannon_index, axis=1)\n",
    "\n",
    "# Filter columns matching the pattern 'number;sk_*' (where 'number;' can be any text)\n",
    "# pattern = re.compile(r'^.*;sk_.*')\n",
    "# filtered_data = merged_data.filter(regex=pattern)\n",
    "\n",
    "# Calculate Shannon index and add it as a new column\n",
    "# merged_data['Shannon'] = calculate_shannon_index(filtered_data)\n",
    "\n",
    "# Function to plot alpha diversity metrics\n",
    "\n",
    "\n",
    "def plot_alpha_diversity_grouped(alpha_diversity_df, selected_factor):\n",
    "    alpha_diversity_sorted = alpha_diversity_df.sort_values(by=selected_factor)\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Suppress FutureWarnings related to the ci parameter\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "        sns.barplot(\n",
    "            x=\"ref_code\",\n",
    "            y=\"Shannon\",\n",
    "            hue=selected_factor,\n",
    "            data=alpha_diversity_sorted,\n",
    "            dodge=False,\n",
    "            palette=\"coolwarm\",\n",
    "            errorbar=None,\n",
    "        )\n",
    "\n",
    "    plt.xlabel(\"Sample\")\n",
    "    plt.ylabel(\"Shannon Index\")\n",
    "    plt.title(f\"Shannon Index Grouped by {selected_factor}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Calculate alpha diversity metrics (Shannon)\n",
    "\n",
    "\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "def calculate_shannon_index(df):\n",
    "    def shannon_index(row):\n",
    "        total_abundance = row.sum()\n",
    "        if total_abundance == 0:\n",
    "            return np.nan\n",
    "        relative_abundance = row / total_abundance\n",
    "        return entropy(\n",
    "            relative_abundance, base=np.e\n",
    "        )  # Shannon entropy (natural log base)\n",
    "\n",
    "    return df.apply(shannon_index, axis=1)\n",
    "\n",
    "\n",
    "def calculate_alpha_diversity(transposed_data, metadata):\n",
    "    # Calculate Shannon index for each sample\n",
    "    shannon_values = calculate_shannon_index(transposed_data)\n",
    "\n",
    "    # Create a DataFrame for Shannon index values\n",
    "    alpha_diversity_df = pd.DataFrame(\n",
    "        {\"ref_code\": transposed_data.index, \"Shannon\": shannon_values}\n",
    "    )\n",
    "\n",
    "    # Merge the Shannon index with metadata\n",
    "    alpha_diversity_df = pd.merge(\n",
    "        alpha_diversity_df, metadata, on=\"ref_code\", how=\"left\"\n",
    "    )\n",
    "\n",
    "    return alpha_diversity_df\n",
    "\n",
    "\n",
    "# Function to calculate ANOVA for all factors and run post-hoc test for the selected factor\n",
    "def run_anova_for_factors(alpha_diversity_df, selected_factor):\n",
    "    # List of all factors from the metadata (excluding ref_code and other irrelevant columns)\n",
    "    all_factors = [\n",
    "        col\n",
    "        for col in metadata.columns\n",
    "        if not re.search(r\"\\d\", col) and col not in [\"ref_code\", \"our_code\", \"#NAME\"]\n",
    "    ]\n",
    "\n",
    "    print(\"\\nANOVA Results for all Factors:\")\n",
    "    anova_results_list = []\n",
    "    for factor in all_factors:\n",
    "        anova_results = pg.anova(data=alpha_diversity_df, dv=\"Shannon\", between=factor)\n",
    "        f_value = anova_results[\"F\"].values[0]\n",
    "        p_value = anova_results[\"p-unc\"].values[0]\n",
    "        anova_results_list.append((factor, f_value, p_value))\n",
    "        print(f\"{factor} - F-value: {f_value:.4f}, p-value: {p_value:.4f}\")\n",
    "\n",
    "    # Based on the selected factor, run the post-hoc test if p-value < 0.05\n",
    "    selected_factor_anova = [\n",
    "        result for result in anova_results_list if result[0] == selected_factor\n",
    "    ][0]\n",
    "\n",
    "    if selected_factor_anova[2] < 0.05:\n",
    "        print(f\"\\nRunning post-hoc tests for {selected_factor}...\")\n",
    "        posthoc_results = pg.pairwise_ttests(\n",
    "            data=alpha_diversity_df, dv=\"Shannon\", between=selected_factor\n",
    "        )\n",
    "        print(\"\\nPosthoc Test Results:\")\n",
    "        print(posthoc_results)\n",
    "    else:\n",
    "        print(\n",
    "            f\"No significant difference found in {selected_factor}. Post-hoc test is not necessary.\"\n",
    "        )\n",
    "\n",
    "\n",
    "def check_normality(data):\n",
    "    \"\"\"Check normality using Shapiro-Wilk test\"\"\"\n",
    "    stat, p_value = shapiro(data)\n",
    "    return p_value > 0.05  # True if data is normally distributed\n",
    "\n",
    "\n",
    "def check_homogeneity_of_variances(groups):\n",
    "    \"\"\"Check homogeneity of variances using Levene's test\"\"\"\n",
    "    stat, p_value = levene(*groups)\n",
    "    return p_value > 0.05  # True if variances are equal\n",
    "\n",
    "\n",
    "def run_anova_and_posthoc(alpha_diversity_df, selected_factor):\n",
    "    # ANOVA for the selected factor\n",
    "    anova_results = pg.anova(\n",
    "        data=alpha_diversity_df, dv=\"Shannon\", between=selected_factor\n",
    "    )\n",
    "    print(\"\\nANOVA Results:\")\n",
    "    print(anova_results)\n",
    "\n",
    "    # Check the distribution of data\n",
    "    grouped_data = alpha_diversity_df.groupby(selected_factor)[\"Shannon\"]\n",
    "    groups = [group for name, group in grouped_data]\n",
    "\n",
    "    normal = check_normality(alpha_diversity_df[\"Shannon\"])\n",
    "    homogeneity = check_homogeneity_of_variances(groups)\n",
    "\n",
    "    print(f\"Normality check: {'Pass' if normal else 'Fail'}\")\n",
    "    print(f\"Homogeneity of variances check: {'Pass' if homogeneity else 'Fail'}\")\n",
    "\n",
    "    if anova_results[\"p-unc\"].values[0] < 0.05:\n",
    "        print(f\"\\nRunning post-hoc tests for {selected_factor}...\")\n",
    "\n",
    "        if normal and homogeneity:\n",
    "            # Tukey's HSD for normal data and equal variances\n",
    "            posthoc_results = pg.pairwise_ttests(\n",
    "                data=alpha_diversity_df,\n",
    "                dv=\"Shannon\",\n",
    "                between=selected_factor,\n",
    "                padjust=\"holm\",\n",
    "            )\n",
    "            print(\"Post-hoc test used: Tukey's HSD\")\n",
    "        else:\n",
    "            # Games-Howell for non-normal data or unequal variances\n",
    "            posthoc_results = pg.pairwise_gameshowell(\n",
    "                data=alpha_diversity_df, dv=\"Shannon\", between=selected_factor\n",
    "            )\n",
    "            print(\"Post-hoc test used: Games-Howell\")\n",
    "\n",
    "        print(\"\\nPosthoc Test Results:\")\n",
    "        print(posthoc_results)\n",
    "    else:\n",
    "        print(\n",
    "            f\"No significant difference found in {selected_factor}. Post-hoc test is not necessary.\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Use the function in the update_alpha_diversity\n",
    "def update_alpha_diversity(change):\n",
    "    with output_alpha_diversity:\n",
    "        output_alpha_diversity.clear_output(wait=True)\n",
    "\n",
    "        selected_factor = alpha_factor_dropdown.value\n",
    "\n",
    "        # Calculate alpha diversity with metadata\n",
    "        alpha_diversity_df = calculate_alpha_diversity(transposed_data, metadata)\n",
    "\n",
    "        # Plot the alpha diversity grouped by the selected factor\n",
    "        plot_alpha_diversity_grouped(alpha_diversity_df, selected_factor)\n",
    "\n",
    "        # Plot the average Shannon index per category\n",
    "        plot_average_shannon(alpha_diversity_df, selected_factor)\n",
    "\n",
    "        # Run ANOVA and post-hoc tests with appropriate test selection\n",
    "        run_anova_and_posthoc(alpha_diversity_df, selected_factor)\n",
    "\n",
    "\n",
    "# Widget Definition for Grouping Factor (only)\n",
    "alpha_factor_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        col\n",
    "        for col in metadata.columns\n",
    "        if not re.search(r\"\\d\", col) and col not in [\"ref_code\", \"our_code\", \"#NAME\"]\n",
    "    ],\n",
    "    description=\"Grouping Factor:\",\n",
    ")\n",
    "\n",
    "\n",
    "def plot_average_shannon(alpha_diversity_df, selected_factor):\n",
    "    # Determine the color palette from the grouped plot\n",
    "    palette = sns.color_palette(\n",
    "        \"coolwarm\", len(alpha_diversity_df[selected_factor].unique())\n",
    "    )\n",
    "\n",
    "    # Group data by the selected factor and calculate the mean and standard error of Shannon index\n",
    "    grouped_data = alpha_diversity_df.groupby(selected_factor)[\"Shannon\"]\n",
    "    means = grouped_data.mean()\n",
    "    errors = grouped_data.sem()  # Standard error of the mean\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "\n",
    "    # Plot the average Shannon index with error bars and custom colors\n",
    "    bars = means.plot(kind=\"bar\", yerr=errors, color=palette, capsize=5)\n",
    "    plt.xlabel(selected_factor)\n",
    "    plt.ylabel(\"Average Shannon Index\")\n",
    "    plt.title(f\"Average Shannon Index by {selected_factor}\")\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Output area for Alpha Diversity\n",
    "output_alpha_diversity = widgets.Output()\n",
    "\n",
    "# Link alpha diversity dropdowns to the update function\n",
    "alpha_factor_dropdown.observe(update_alpha_diversity, names=\"value\")\n",
    "\n",
    "# Display widgets\n",
    "display(alpha_factor_dropdown, output_alpha_diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415e908e-90dc-4396-beee-499df8bae513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a86f58-d48c-48e1-bcf5-ca30531cd53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming `merged` is the updated DataFrame\n",
    "data = merged_data  # Use 'merged' instead of 'SSU_class_all'\n",
    "\n",
    "# Set 'ref_code' as the index and transpose the DataFrame\n",
    "transposed_data = data.set_index(\"ref_code\").T\n",
    "\n",
    "# Load metadata from `merged`\n",
    "metadata = full_metadata\n",
    "\n",
    "# Exclude metadata columns from Shannon index calculation\n",
    "metadata_columns = [\n",
    "    \"filter_size\",\n",
    "    \"date\",\n",
    "    \"replcate\",\n",
    "    \"station_filter\",\n",
    "    \"filter_rep\",\n",
    "    \"location\",\n",
    "    \"country\",\n",
    "    \"column12\",\n",
    "    \"column13\",\n",
    "]\n",
    "numeric_columns = [col for col in transposed_data.columns if col.startswith(\"class_\")]\n",
    "\n",
    "\n",
    "# Function to compute the distance matrix based on the selected distance metric\n",
    "def compute_distance_matrix(metric):\n",
    "    if metric == \"braycurtis\":\n",
    "        return squareform(pdist(transposed_data[numeric_columns], metric=\"braycurtis\"))\n",
    "    elif metric == \"euclidean\":\n",
    "        return squareform(pdist(transposed_data[numeric_columns], metric=\"euclidean\"))\n",
    "    elif metric == \"jaccard\":\n",
    "        return squareform(pdist(transposed_data[numeric_columns], metric=\"jaccard\"))\n",
    "    elif metric == \"cityblock\":\n",
    "        return squareform(pdist(transposed_data[numeric_columns], metric=\"cityblock\"))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distance metric selected\")\n",
    "\n",
    "\n",
    "# Step 6: Perform PERMANOVA and PCoA based on the selected distance metric\n",
    "def run_permanova_and_pcoa(distance_matrix):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        permanova_summary = pd.DataFrame(columns=[\"Factor\", \"Pseudo-F\", \"p-value\"])\n",
    "        factors_to_analyze = [\n",
    "            col\n",
    "            for col in metadata.columns\n",
    "            if not re.search(r\"\\d\", col)\n",
    "            and col not in [\"ref_code\", \"our_code\", \"#NAME\"]\n",
    "        ]\n",
    "        for factor in factors_to_analyze:\n",
    "            unique_values = merged_data[factor].nunique()\n",
    "            if unique_values > 1:\n",
    "                metadata_reindexed = merged_data.set_index(\"ref_code\").reindex(\n",
    "                    transposed_data.index\n",
    "                )\n",
    "                try:\n",
    "                    dist_matrix_obj = DistanceMatrix(\n",
    "                        distance_matrix, ids=transposed_data.index\n",
    "                    )\n",
    "                    permanova_results = permanova(\n",
    "                        dist_matrix_obj,\n",
    "                        metadata_reindexed[factor].values,\n",
    "                        permutations=999,\n",
    "                    )\n",
    "                    pseudo_F = (\n",
    "                        permanova_results[\"test statistic\"]\n",
    "                        if \"test statistic\" in permanova_results\n",
    "                        else None\n",
    "                    )\n",
    "                    p_value = (\n",
    "                        permanova_results[\"p-value\"]\n",
    "                        if \"p-value\" in permanova_results\n",
    "                        else None\n",
    "                    )\n",
    "                    new_row = pd.DataFrame(\n",
    "                        {\n",
    "                            \"Factor\": [factor],\n",
    "                            \"Pseudo-F\": [pseudo_F],\n",
    "                            \"p-value\": [p_value],\n",
    "                        }\n",
    "                    )\n",
    "                    permanova_summary = pd.concat(\n",
    "                        [permanova_summary, new_row], ignore_index=True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        permanova_summary = permanova_summary.sort_values(\n",
    "            by=\"Pseudo-F\", ascending=False\n",
    "        )\n",
    "        print(\"\\nPERMANOVA Summary:\")\n",
    "        print(permanova_summary.to_string(index=False))\n",
    "\n",
    "        # PCoA (PCA as approximation)\n",
    "        pcoa = PCA(n_components=2)\n",
    "        pcoa_coords = pcoa.fit_transform(distance_matrix)\n",
    "        pcoa_df = pd.DataFrame(\n",
    "            pcoa_coords, columns=[\"PCoA1\", \"PCoA2\"], index=transposed_data.index\n",
    "        )\n",
    "\n",
    "        return permanova_summary, pcoa_df\n",
    "\n",
    "\n",
    "# Plot the PCoA with optional coloring by Shannon index\n",
    "def plot_pcoa_black(pcoa_df, color_by=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if color_by is not None:\n",
    "        scatter = plt.scatter(\n",
    "            pcoa_df[\"PCoA1\"], pcoa_df[\"PCoA2\"], c=color_by, cmap=\"RdYlGn\", edgecolor=\"k\"\n",
    "        )\n",
    "        plt.colorbar(scatter, label=\"Shannon Index\")\n",
    "    else:\n",
    "        plt.scatter(pcoa_df[\"PCoA1\"], pcoa_df[\"PCoA2\"], color=\"black\")\n",
    "\n",
    "    plt.xlabel(\"PCoA1\")\n",
    "    plt.ylabel(\"PCoA2\")\n",
    "    plt.title(\"PCoA Plot\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the PCoA with the selected factor\n",
    "def plot_pcoa_with_factor(pcoa_df, selected_factor):\n",
    "    if selected_factor not in merged_data.columns:\n",
    "        print(f\"Factor '{selected_factor}' not found in the metadata.\")\n",
    "        return\n",
    "\n",
    "    pcoa_df[selected_factor] = (\n",
    "        merged_data.set_index(\"ref_code\")[selected_factor].reindex(pcoa_df.index).values\n",
    "    )\n",
    "    colors_list = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"orange\", \"purple\", \"pink\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    unique_values = pcoa_df[selected_factor].unique()\n",
    "    for i, value in enumerate(unique_values):\n",
    "        color = colors_list[i % len(colors_list)]\n",
    "        subset = pcoa_df[pcoa_df[selected_factor] == value]\n",
    "        plt.scatter(subset[\"PCoA1\"], subset[\"PCoA2\"], label=value, color=color)\n",
    "\n",
    "    plt.xlabel(\"PCoA1\")\n",
    "    plt.ylabel(\"PCoA2\")\n",
    "    plt.title(f\"PCoA Plot Grouped by {selected_factor}\")\n",
    "    plt.legend(title=selected_factor)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# SIMPER calculation excluding Shannon and metadata columns\n",
    "def calculate_simper(group1, group2):\n",
    "    group1_filtered = group1.select_dtypes(include=[np.number]).drop(\n",
    "        columns=[\"Shannon\"], errors=\"ignore\"\n",
    "    )\n",
    "    group2_filtered = group2.select_dtypes(include=[np.number]).drop(\n",
    "        columns=[\"Shannon\"], errors=\"ignore\"\n",
    "    )\n",
    "\n",
    "    mean_group1 = group1_filtered.mean()\n",
    "    mean_group2 = group2_filtered.mean()\n",
    "\n",
    "    differences = abs(mean_group1 - mean_group2)\n",
    "    total_difference = differences.sum()\n",
    "    contributions = (differences / total_difference) * 100\n",
    "\n",
    "    return contributions\n",
    "\n",
    "\n",
    "# SIMPER plot\n",
    "def plot_simper(simper_result, selected_group1, selected_group2):\n",
    "    simper_result_sorted = simper_result.sort_values(ascending=False).head(20)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    simper_result_sorted.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Contribution to Dissimilarity (%)\")\n",
    "    plt.title(\n",
    "        f\"SIMPER: Top 20 Features Dissimilarity between {selected_group1} and {selected_group2}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Interactive part begins ---\n",
    "\n",
    "# Define widgets\n",
    "distance_metric_dropdown = widgets.Dropdown(\n",
    "    options=[\"braycurtis\", \"euclidean\", \"jaccard\", \"cityblock\"],\n",
    "    description=\"Distance Metric:\",\n",
    ")\n",
    "\n",
    "factor_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        col\n",
    "        for col in metadata.columns\n",
    "        if not re.search(r\"\\d\", col) and col not in [\"ref_code\", \"our_code\", \"#NAME\"]\n",
    "    ],\n",
    "    description=\"PCoA Factor:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "simper_dropdown_1 = widgets.Dropdown(\n",
    "    options=[], description=\"SIMPER: Condition 1:\", disabled=True\n",
    ")\n",
    "\n",
    "simper_dropdown_2 = widgets.Dropdown(\n",
    "    options=[], description=\"SIMPER: Condition 2:\", disabled=True\n",
    ")\n",
    "\n",
    "# Output areas\n",
    "output_permanova = widgets.Output()\n",
    "output_pcoa = widgets.Output()\n",
    "output_simper = widgets.Output()\n",
    "output_shannon = widgets.Output()\n",
    "\n",
    "\n",
    "# Function to run analysis after distance metric is selected\n",
    "def on_distance_change(change):\n",
    "    with output_permanova:\n",
    "        output_permanova.clear_output(wait=True)\n",
    "        with output_pcoa:\n",
    "            output_pcoa.clear_output(wait=True)\n",
    "            with output_shannon:\n",
    "                output_shannon.clear_output(wait=True)\n",
    "\n",
    "                selected_metric = distance_metric_dropdown.value\n",
    "                distance_matrix = compute_distance_matrix(selected_metric)\n",
    "\n",
    "                # PERMANOVA and initial PCoA\n",
    "                global permanova_summary, pcoa_df\n",
    "                permanova_summary, pcoa_df = run_permanova_and_pcoa(distance_matrix)\n",
    "\n",
    "                # Plot PCoA colored by Shannon index\n",
    "                plot_pcoa_black(pcoa_df, color_by=transposed_data[\"Shannon\"])\n",
    "\n",
    "                # Update factor and SIMPER dropdowns\n",
    "                available_factors = [\n",
    "                    col\n",
    "                    for col in merged_data.columns\n",
    "                    if not re.search(r\"\\d\", col)\n",
    "                    and col not in [\"ref_code\", \"our_code\", \"#NAME\"]\n",
    "                ]\n",
    "                factor_dropdown.options = available_factors\n",
    "                factor_dropdown.disabled = False\n",
    "\n",
    "                # Initialize SIMPER dropdowns\n",
    "                if factor_dropdown.value:\n",
    "                    current_factor = factor_dropdown.value\n",
    "                    simper_options = sorted(merged_data[current_factor].unique())\n",
    "                    simper_dropdown_1.options = simper_options\n",
    "                    simper_dropdown_2.options = simper_options\n",
    "                    simper_dropdown_1.disabled = False\n",
    "                    simper_dropdown_2.disabled = False\n",
    "\n",
    "\n",
    "distance_metric_dropdown.observe(on_distance_change, names=\"value\")\n",
    "\n",
    "\n",
    "# Function to update PCoA plot after factor is selected\n",
    "def on_factor_change(change):\n",
    "    with output_pcoa:\n",
    "        output_pcoa.clear_output(wait=True)\n",
    "\n",
    "        # Plot the PCoA with colors based on the selected factor\n",
    "        selected_factor = factor_dropdown.value\n",
    "        plot_pcoa_with_factor(pcoa_df, selected_factor)\n",
    "\n",
    "        # Enable and update the SIMPER condition dropdowns based on the selected factor\n",
    "        simper_dropdown_1.options = merged_data[selected_factor].unique()\n",
    "        simper_dropdown_2.options = merged_data[selected_factor].unique()\n",
    "        simper_dropdown_1.disabled = False\n",
    "        simper_dropdown_2.disabled = False\n",
    "\n",
    "\n",
    "# Function to run SIMPER after both conditions are selected\n",
    "def on_simper_change(change):\n",
    "    if simper_dropdown_1.value != simper_dropdown_2.value:\n",
    "        with output_simper:\n",
    "            output_simper.clear_output(wait=True)\n",
    "\n",
    "            # Filter data for the two selected conditions\n",
    "            selected_factor = factor_dropdown.value\n",
    "            group1 = merged_data[\n",
    "                merged_data[selected_factor] == simper_dropdown_1.value\n",
    "            ]\n",
    "            group2 = merged_data[\n",
    "                merged_data[selected_factor] == simper_dropdown_2.value\n",
    "            ]\n",
    "\n",
    "            # Calculate SIMPER and plot\n",
    "            simper_result = calculate_simper(group1, group2)\n",
    "            plot_simper(simper_result, simper_dropdown_1.value, simper_dropdown_2.value)\n",
    "\n",
    "\n",
    "# Link widget events to functions\n",
    "distance_metric_dropdown.observe(on_distance_change, names=\"value\")\n",
    "factor_dropdown.observe(on_factor_change, names=\"value\")\n",
    "simper_dropdown_1.observe(on_simper_change, names=\"value\")\n",
    "simper_dropdown_2.observe(on_simper_change, names=\"value\")\n",
    "\n",
    "# Display widgets and output areas\n",
    "display(\n",
    "    distance_metric_dropdown,\n",
    "    output_permanova,\n",
    "    output_pcoa,\n",
    "    factor_dropdown,\n",
    "    simper_dropdown_1,\n",
    "    simper_dropdown_2,\n",
    "    output_simper,\n",
    "    output_shannon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b5106b0-68f8-4069-b064-970015102238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the table from DuckDB into a DataFrame\n",
    "data = merged_data  # Use 'merged_data'\n",
    "\n",
    "\n",
    "# Ensure that we only use numeric data for distance calculation\n",
    "numeric_columns = [\n",
    "    col for col in data.columns if col.startswith(\"class_\")\n",
    "]  # Columns with the prefix 'class_'\n",
    "\n",
    "# Convert the columns to numeric (if they are not already) and replace NaN with 0\n",
    "data[numeric_columns] = (\n",
    "    data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    ")\n",
    "\n",
    "\n",
    "# Now compute the distance matrix\n",
    "def compute_distance_matrix(metric):\n",
    "    numeric_data = data[numeric_columns]  # Use updated data with NaNs replaced by 0\n",
    "\n",
    "    if metric == \"braycurtis\":\n",
    "        return squareform(pdist(numeric_data, metric=\"braycurtis\"))\n",
    "    elif metric == \"euclidean\":\n",
    "        return squareform(pdist(numeric_data, metric=\"euclidean\"))\n",
    "    elif metric == \"jaccard\":\n",
    "        return squareform(pdist(numeric_data, metric=\"jaccard\"))\n",
    "    elif metric == \"cityblock\":\n",
    "        return squareform(pdist(numeric_data, metric=\"cityblock\"))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distance metric selected\")\n",
    "\n",
    "\n",
    "# Step 3: Perform PERMANOVA and PCoA based on the selected distance metric\n",
    "def run_permanova_and_pcoa(distance_matrix):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        permanova_summary = pd.DataFrame(columns=[\"Factor\", \"Pseudo-F\", \"p-value\"])\n",
    "        factors_to_analyze = [\n",
    "            col\n",
    "            for col in data.columns\n",
    "            if col not in numeric_columns and col != \"ref_code\"\n",
    "        ]\n",
    "        for factor in factors_to_analyze:\n",
    "            unique_values = data[factor].nunique()\n",
    "            if unique_values > 1:\n",
    "                try:\n",
    "                    dist_matrix_obj = DistanceMatrix(distance_matrix, ids=data.index)\n",
    "                    permanova_results = permanova(\n",
    "                        dist_matrix_obj, data[factor].values, permutations=999\n",
    "                    )\n",
    "                    pseudo_F = (\n",
    "                        permanova_results[\"test statistic\"]\n",
    "                        if \"test statistic\" in permanova_results\n",
    "                        else None\n",
    "                    )\n",
    "                    p_value = (\n",
    "                        permanova_results[\"p-value\"]\n",
    "                        if \"p-value\" in permanova_results\n",
    "                        else None\n",
    "                    )\n",
    "                    new_row = pd.DataFrame(\n",
    "                        {\n",
    "                            \"Factor\": [factor],\n",
    "                            \"Pseudo-F\": [pseudo_F],\n",
    "                            \"p-value\": [p_value],\n",
    "                        }\n",
    "                    )\n",
    "                    permanova_summary = pd.concat(\n",
    "                        [permanova_summary, new_row], ignore_index=True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        permanova_summary = permanova_summary.sort_values(\n",
    "            by=\"Pseudo-F\", ascending=False\n",
    "        )\n",
    "        print(\"\\nPERMANOVA Summary:\")\n",
    "        print(permanova_summary.to_string(index=False))\n",
    "\n",
    "        # PCoA (PCA as approximation)\n",
    "        pcoa = PCA(n_components=2)\n",
    "        pcoa_coords = pcoa.fit_transform(distance_matrix)\n",
    "        pcoa_df = pd.DataFrame(\n",
    "            pcoa_coords, columns=[\"PCoA1\", \"PCoA2\"], index=data.index\n",
    "        )\n",
    "\n",
    "        return permanova_summary, pcoa_df\n",
    "\n",
    "\n",
    "# Plot the PCoA with optional coloring by Shannon index\n",
    "def plot_pcoa_black(pcoa_df, color_by=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if color_by is not None:\n",
    "        scatter = plt.scatter(\n",
    "            pcoa_df[\"PCoA1\"], pcoa_df[\"PCoA2\"], c=color_by, cmap=\"RdYlGn\", edgecolor=\"k\"\n",
    "        )\n",
    "        plt.colorbar(scatter, label=\"Shannon Index\")\n",
    "    else:\n",
    "        plt.scatter(pcoa_df[\"PCoA1\"], pcoa_df[\"PCoA2\"], color=\"black\")\n",
    "\n",
    "    plt.xlabel(\"PCoA1\")\n",
    "    plt.ylabel(\"PCoA2\")\n",
    "    plt.title(\"PCoA Plot\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the PCoA with the selected factor\n",
    "def plot_pcoa_with_factor(pcoa_df, selected_factor):\n",
    "    if selected_factor not in data.columns:\n",
    "        print(f\"Factor '{selected_factor}' not found in the metadata.\")\n",
    "        return\n",
    "\n",
    "    pcoa_df[selected_factor] = data[selected_factor].values\n",
    "    colors_list = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"orange\", \"purple\", \"pink\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    unique_values = pcoa_df[selected_factor].unique()\n",
    "    for i, value in enumerate(unique_values):\n",
    "        color = colors_list[i % len(colors_list)]\n",
    "        subset = pcoa_df[pcoa_df[selected_factor] == value]\n",
    "        plt.scatter(subset[\"PCoA1\"], subset[\"PCoA2\"], label=value, color=color)\n",
    "\n",
    "    plt.xlabel(\"PCoA1\")\n",
    "    plt.ylabel(\"PCoA2\")\n",
    "    plt.title(f\"PCoA Plot Grouped by {selected_factor}\")\n",
    "    plt.legend(title=selected_factor)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# SIMPER calculation\n",
    "def calculate_simper(group1, group2):\n",
    "    group1_filtered = group1[numeric_columns]\n",
    "    group2_filtered = group2[numeric_columns]\n",
    "\n",
    "    mean_group1 = group1_filtered.mean()\n",
    "    mean_group2 = group2_filtered.mean()\n",
    "\n",
    "    differences = abs(mean_group1 - mean_group2)\n",
    "    total_difference = differences.sum()\n",
    "    contributions = (differences / total_difference) * 100\n",
    "\n",
    "    return contributions\n",
    "\n",
    "\n",
    "# SIMPER plot\n",
    "def plot_simper(simper_result, selected_group1, selected_group2):\n",
    "    simper_result_sorted = simper_result.sort_values(ascending=False).head(20)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    simper_result_sorted.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Contribution to Dissimilarity (%)\")\n",
    "    plt.title(\n",
    "        f\"SIMPER: Top 20 Features Dissimilarity between {selected_group1} and {selected_group2}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Interactive part begins ---\n",
    "\n",
    "# Step 8: Define widgets\n",
    "distance_metric_dropdown = widgets.Dropdown(\n",
    "    options=[\"braycurtis\", \"euclidean\", \"jaccard\", \"cityblock\"],\n",
    "    description=\"Distance Metric:\",\n",
    ")\n",
    "\n",
    "factor_dropdown = widgets.Dropdown(\n",
    "    options=[\n",
    "        col for col in data.columns if col not in numeric_columns and col != \"ref_code\"\n",
    "    ],\n",
    "    description=\"PCoA Factor:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "simper_dropdown_1 = widgets.Dropdown(\n",
    "    options=[], description=\"SIMPER: Condition 1:\", disabled=True\n",
    ")\n",
    "\n",
    "simper_dropdown_2 = widgets.Dropdown(\n",
    "    options=[], description=\"SIMPER: Condition 2:\", disabled=True\n",
    ")\n",
    "\n",
    "# Output areas\n",
    "output_permanova = widgets.Output()\n",
    "output_pcoa = widgets.Output()\n",
    "output_simper = widgets.Output()\n",
    "output_shannon = widgets.Output()\n",
    "\n",
    "\n",
    "# Step 9: Function to run analysis after distance metric is selected\n",
    "def on_distance_change(change):\n",
    "    with output_permanova:\n",
    "        output_permanova.clear_output(wait=True)\n",
    "        with output_pcoa:\n",
    "            output_pcoa.clear_output(wait=True)\n",
    "            with output_shannon:\n",
    "                output_shannon.clear_output(wait=True)\n",
    "\n",
    "                selected_metric = distance_metric_dropdown.value\n",
    "                distance_matrix = compute_distance_matrix(selected_metric)\n",
    "\n",
    "                # PERMANOVA and initial PCoA\n",
    "                global permanova_summary, pcoa_df\n",
    "                permanova_summary, pcoa_df = run_permanova_and_pcoa(distance_matrix)\n",
    "\n",
    "                # Plot PCoA colored by Shannon index\n",
    "                plot_pcoa_black(pcoa_df)\n",
    "\n",
    "                # Update factor and SIMPER dropdowns\n",
    "                available_factors = [\n",
    "                    col\n",
    "                    for col in data.columns\n",
    "                    if col not in numeric_columns and col != \"ref_code\"\n",
    "                ]\n",
    "                factor_dropdown.options = available_factors\n",
    "                factor_dropdown.disabled = False\n",
    "\n",
    "                # Initialize SIMPER dropdowns\n",
    "                if factor_dropdown.value:\n",
    "                    current_factor = factor_dropdown.value\n",
    "                    simper_options = sorted(data[current_factor].unique())\n",
    "                    simper_dropdown_1.options = simper_options\n",
    "                    simper_dropdown_2.options = simper_options\n",
    "                    simper_dropdown_1.disabled = False\n",
    "                    simper_dropdown_2.disabled = False\n",
    "\n",
    "\n",
    "distance_metric_dropdown.observe(on_distance_change, names=\"value\")\n",
    "\n",
    "\n",
    "# Function to update PCoA plot after factor is selected\n",
    "def on_factor_change(change):\n",
    "    with output_pcoa:\n",
    "        output_pcoa.clear_output(wait=True)\n",
    "\n",
    "        # Plot the PCoA with colors based on the selected factor\n",
    "        selected_factor = factor_dropdown.value\n",
    "        plot_pcoa_with_factor(pcoa_df, selected_factor)\n",
    "\n",
    "        # Enable and update the SIMPER condition dropdowns based on the selected factor\n",
    "        simper_dropdown_1.options = data[selected_factor].unique()\n",
    "        simper_dropdown_2.options = data[selected_factor].unique()\n",
    "        simper_dropdown_1.disabled = False\n",
    "        simper_dropdown_2.disabled = False\n",
    "\n",
    "\n",
    "# Function to run SIMPER after both conditions are selected\n",
    "def on_simper_change(change):\n",
    "    if simper_dropdown_1.value != simper_dropdown_2.value:\n",
    "        with output_simper:\n",
    "            output_simper.clear_output(wait=True)\n",
    "\n",
    "            # Filter data for the two selected conditions\n",
    "            selected_factor = factor_dropdown.value\n",
    "            group1 = data[data[selected_factor] == simper_dropdown_1.value]\n",
    "            group2 = data[data[selected_factor] == simper_dropdown_2.value]\n",
    "\n",
    "            # Calculate SIMPER and plot\n",
    "            simper_result = calculate_simper(group1, group2)\n",
    "            plot_simper(simper_result, simper_dropdown_1.value, simper_dropdown_2.value)\n",
    "\n",
    "\n",
    "# Step 10: Link widget events to functions\n",
    "distance_metric_dropdown.observe(on_distance_change, names=\"value\")\n",
    "factor_dropdown.observe(on_factor_change, names=\"value\")\n",
    "simper_dropdown_1.observe(on_simper_change, names=\"value\")\n",
    "simper_dropdown_2.observe(on_simper_change, names=\"value\")\n",
    "\n",
    "# Display widgets and output areas\n",
    "display(\n",
    "    distance_metric_dropdown,\n",
    "    output_permanova,\n",
    "    output_pcoa,\n",
    "    factor_dropdown,\n",
    "    simper_dropdown_1,\n",
    "    simper_dropdown_2,\n",
    "    output_simper,\n",
    "    output_shannon,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45aac140-dafa-4481-bd95-ae467636e363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32f7c75-bfa3-4fae-b5b0-3ccbcfb0975f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Load the table from DuckDB into a DataFrame\n",
    "data = merged_data  # Use 'merged_data'\n",
    "\n",
    "# Ensure that we only use numeric data for distance calculation\n",
    "numeric_columns = [\n",
    "    col for col in data.columns if col.startswith(\"class_\")\n",
    "]  # Columns with the prefix 'class_'\n",
    "metadata = [\n",
    "    col for col in data.columns if not col.startswith(\"class_\") and col != \"ref_code\"\n",
    "]\n",
    "\n",
    "# Convert the columns to numeric (if they are not already) and replace NaN with 0\n",
    "data[numeric_columns] = (\n",
    "    data[numeric_columns].apply(pd.to_numeric, errors=\"coerce\").fillna(0)\n",
    ")\n",
    "# data[numeric_columns] = data[numeric_columns].astype(float).fillna(0)\n",
    "\n",
    "\n",
    "# Now all 'NaN' values in columns with names starting with 'class_' have been replaced with 0.\n",
    "\n",
    "\n",
    "# Now compute the distance matrix\n",
    "def compute_distance_matrix(metric):\n",
    "    numeric_data = data[numeric_columns]  # Use updated data with NaNs replaced by 0\n",
    "\n",
    "    if metric == \"braycurtis\":\n",
    "        return squareform(pdist(numeric_data, metric=\"braycurtis\"))\n",
    "    elif metric == \"euclidean\":\n",
    "        return squareform(pdist(numeric_data, metric=\"euclidean\"))\n",
    "    elif metric == \"jaccard\":\n",
    "        return squareform(pdist(numeric_data, metric=\"jaccard\"))\n",
    "    elif metric == \"cityblock\":\n",
    "        return squareform(pdist(numeric_data, metric=\"cityblock\"))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported distance metric selected\")\n",
    "\n",
    "\n",
    "# Continue with the rest of the code\n",
    "\n",
    "\n",
    "# Step 3: Perform PERMANOVA and PCoA based on the selected distance metric\n",
    "def run_permanova_and_pcoa(distance_matrix):\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        permanova_summary = pd.DataFrame(columns=[\"Factor\", \"Pseudo-F\", \"p-value\"])\n",
    "        factors_to_analyze = [\n",
    "            col\n",
    "            for col in data.columns\n",
    "            if col not in numeric_columns and col != \"ref_code\"\n",
    "        ]\n",
    "        for factor in factors_to_analyze:\n",
    "            unique_values = data[factor].nunique()\n",
    "            if unique_values > 1:\n",
    "                try:\n",
    "                    dist_matrix_obj = DistanceMatrix(distance_matrix, ids=data.index)\n",
    "                    permanova_results = permanova(\n",
    "                        dist_matrix_obj, data[factor].values, permutations=999\n",
    "                    )\n",
    "                    pseudo_F = (\n",
    "                        permanova_results[\"test statistic\"]\n",
    "                        if \"test statistic\" in permanova_results\n",
    "                        else None\n",
    "                    )\n",
    "                    p_value = (\n",
    "                        permanova_results[\"p-value\"]\n",
    "                        if \"p-value\" in permanova_results\n",
    "                        else None\n",
    "                    )\n",
    "                    new_row = pd.DataFrame(\n",
    "                        {\n",
    "                            \"Factor\": [factor],\n",
    "                            \"Pseudo-F\": [pseudo_F],\n",
    "                            \"p-value\": [p_value],\n",
    "                        }\n",
    "                    )\n",
    "                    permanova_summary = pd.concat(\n",
    "                        [permanova_summary, new_row], ignore_index=True\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    pass\n",
    "        permanova_summary = permanova_summary.sort_values(\n",
    "            by=\"Pseudo-F\", ascending=False\n",
    "        )\n",
    "        print(\"\\nPERMANOVA Summary:\")\n",
    "        print(permanova_summary.to_string(index=False))\n",
    "\n",
    "        # PCoA (PCA as approximation)\n",
    "        pcoa = PCA(n_components=2)\n",
    "        pcoa_coords = pcoa.fit_transform(distance_matrix)\n",
    "        pcoa_df = pd.DataFrame(\n",
    "            pcoa_coords, columns=[\"PCoA1\", \"PCoA2\"], index=data.index\n",
    "        )\n",
    "\n",
    "        return permanova_summary, pcoa_df\n",
    "\n",
    "\n",
    "# Plot the PCoA with optional coloring by Shannon index\n",
    "def plot_pcoa_black(pcoa_df, color_by=None):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    if color_by is not None:\n",
    "        scatter = plt.scatter(\n",
    "            pcoa_df[\"PCoA1\"], pcoa_df[\"PCoA2\"], c=color_by, cmap=\"RdYlGn\", edgecolor=\"k\"\n",
    "        )\n",
    "        plt.colorbar(scatter, label=\"Shannon Index\")\n",
    "    else:\n",
    "        plt.scatter(pcoa_df[\"PCoA1\"], pcoa_df[\"PCoA2\"], color=\"black\")\n",
    "\n",
    "    plt.xlabel(\"PCoA1\")\n",
    "    plt.ylabel(\"PCoA2\")\n",
    "    plt.title(\"PCoA Plot\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Plot the PCoA with the selected factor\n",
    "def plot_pcoa_with_factor(pcoa_df, selected_factor):\n",
    "    if selected_factor not in data.columns:\n",
    "        print(f\"Factor '{selected_factor}' not found in the metadata.\")\n",
    "        return\n",
    "\n",
    "    pcoa_df[selected_factor] = data[selected_factor].values\n",
    "    colors_list = [\"r\", \"g\", \"b\", \"c\", \"m\", \"y\", \"k\", \"orange\", \"purple\", \"pink\"]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    unique_values = pcoa_df[selected_factor].unique()\n",
    "    for i, value in enumerate(unique_values):\n",
    "        color = colors_list[i % len(colors_list)]\n",
    "        subset = pcoa_df[pcoa_df[selected_factor] == value]\n",
    "        plt.scatter(subset[\"PCoA1\"], subset[\"PCoA2\"], label=value, color=color)\n",
    "\n",
    "    plt.xlabel(\"PCoA1\")\n",
    "    plt.ylabel(\"PCoA2\")\n",
    "    plt.title(f\"PCoA Plot Grouped by {selected_factor}\")\n",
    "    plt.legend(title=selected_factor)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# SIMPER calculation\n",
    "def calculate_simper(group1, group2):\n",
    "    group1_filtered = group1[numeric_columns]\n",
    "    group2_filtered = group2[numeric_columns]\n",
    "\n",
    "    mean_group1 = group1_filtered.mean()\n",
    "    mean_group2 = group2_filtered.mean()\n",
    "\n",
    "    differences = abs(mean_group1 - mean_group2)\n",
    "    total_difference = differences.sum()\n",
    "    contributions = (differences / total_difference) * 100\n",
    "\n",
    "    return contributions\n",
    "\n",
    "\n",
    "# SIMPER plot\n",
    "def plot_simper(simper_result, selected_group1, selected_group2):\n",
    "    simper_result_sorted = simper_result.sort_values(ascending=False).head(20)\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    simper_result_sorted.plot(kind=\"bar\")\n",
    "    plt.xlabel(\"Features\")\n",
    "    plt.ylabel(\"Contribution to Dissimilarity (%)\")\n",
    "    plt.title(\n",
    "        f\"SIMPER: Top 20 Features Dissimilarity between {selected_group1} and {selected_group2}\"\n",
    "    )\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Interactive part begins ---\n",
    "\n",
    "# Step 8: Define widgets\n",
    "\n",
    "# Adding \"please select\" as the first option for the distance metric dropdown\n",
    "distance_metric_dropdown = widgets.Dropdown(\n",
    "    options=[\"please select\", \"braycurtis\", \"euclidean\", \"jaccard\", \"cityblock\"],\n",
    "    description=\"Distance Metric:\",\n",
    ")\n",
    "\n",
    "# Adding \"please select\" as the first option for the factor dropdown\n",
    "factor_dropdown = widgets.Dropdown(\n",
    "    options=[\"please select\"]\n",
    "    + [col for col in data.columns if col not in numeric_columns and col != \"ref_code\"],\n",
    "    description=\"PCoA Factor:\",\n",
    "    disabled=True,\n",
    ")\n",
    "\n",
    "# SIMPER dropdowns with \"please select\" as the default\n",
    "simper_dropdown_1 = widgets.Dropdown(\n",
    "    options=[\"please select\"], description=\"SIMPER: Condition 1:\", disabled=True\n",
    ")\n",
    "\n",
    "simper_dropdown_2 = widgets.Dropdown(\n",
    "    options=[\"please select\"], description=\"SIMPER: Condition 2:\", disabled=True\n",
    ")\n",
    "\n",
    "\n",
    "# Step 9: Function to run analysis after distance metric is selected\n",
    "def on_distance_change(change):\n",
    "    if (\n",
    "        distance_metric_dropdown.value != \"please select\"\n",
    "    ):  # Only run if a valid option is selected\n",
    "        with output_permanova:\n",
    "            output_permanova.clear_output(wait=True)\n",
    "            with output_pcoa:\n",
    "                output_pcoa.clear_output(wait=True)\n",
    "                with output_shannon:\n",
    "                    output_shannon.clear_output(wait=True)\n",
    "\n",
    "                    selected_metric = distance_metric_dropdown.value\n",
    "                    distance_matrix = compute_distance_matrix(selected_metric)\n",
    "\n",
    "                    # PERMANOVA and initial PCoA\n",
    "                    global permanova_summary, pcoa_df\n",
    "                    permanova_summary, pcoa_df = run_permanova_and_pcoa(distance_matrix)\n",
    "\n",
    "                    # Plot PCoA colored by Shannon index\n",
    "                    plot_pcoa_black(pcoa_df)\n",
    "\n",
    "                    # Update factor and SIMPER dropdowns\n",
    "                    available_factors = [\"please select\"] + [\n",
    "                        col\n",
    "                        for col in data.columns\n",
    "                        if col not in numeric_columns and col != \"ref_code\"\n",
    "                    ]\n",
    "                    factor_dropdown.options = available_factors\n",
    "                    factor_dropdown.disabled = False\n",
    "\n",
    "                    # Initialize SIMPER dropdowns\n",
    "                    if (\n",
    "                        factor_dropdown.value\n",
    "                        and factor_dropdown.value != \"please select\"\n",
    "                    ):\n",
    "                        current_factor = factor_dropdown.value\n",
    "                        simper_options = [\"please select\"] + sorted(\n",
    "                            data[current_factor].unique()\n",
    "                        )\n",
    "                        simper_dropdown_1.options = simper_options\n",
    "                        simper_dropdown_2.options = simper_options\n",
    "                        simper_dropdown_1.disabled = False\n",
    "                        simper_dropdown_2.disabled = False\n",
    "\n",
    "\n",
    "# Step 9b: Function to handle factor change and plot PCoA with selected factor\n",
    "def on_factor_change(change):\n",
    "    if factor_dropdown.value != \"please select\":  # Ensure a valid factor is selected\n",
    "        with output_pcoa:\n",
    "            output_pcoa.clear_output(wait=True)\n",
    "            selected_factor = factor_dropdown.value\n",
    "            plot_pcoa_with_factor(pcoa_df, selected_factor)\n",
    "\n",
    "        # Enable and populate SIMPER dropdowns\n",
    "        simper_options = [\"please select\"] + sorted(data[selected_factor].unique())\n",
    "        simper_dropdown_1.options = simper_options\n",
    "        simper_dropdown_2.options = simper_options\n",
    "        simper_dropdown_1.disabled = False\n",
    "        simper_dropdown_2.disabled = False\n",
    "    else:\n",
    "        # Disable SIMPER dropdowns if no valid factor is selected\n",
    "        simper_dropdown_1.disabled = True\n",
    "        simper_dropdown_2.disabled = True\n",
    "\n",
    "\n",
    "# Function to run SIMPER after both conditions are selected\n",
    "def on_simper_change(change):\n",
    "    if simper_dropdown_1.value != simper_dropdown_2.value:\n",
    "        with output_simper:\n",
    "            output_simper.clear_output(wait=True)\n",
    "\n",
    "            # Filter data for the two selected conditions\n",
    "            selected_factor = factor_dropdown.value\n",
    "            group1 = merged_data[\n",
    "                merged_data[selected_factor] == simper_dropdown_1.value\n",
    "            ]\n",
    "            group2 = merged_data[\n",
    "                merged_data[selected_factor] == simper_dropdown_2.value\n",
    "            ]\n",
    "\n",
    "            # Calculate SIMPER and plot\n",
    "            simper_result = calculate_simper(group1, group2)\n",
    "            plot_simper(simper_result, simper_dropdown_1.value, simper_dropdown_2.value)\n",
    "\n",
    "\n",
    "# Step 10: Link widget events to functions\n",
    "distance_metric_dropdown.observe(on_distance_change, names=\"value\")\n",
    "factor_dropdown.observe(\n",
    "    on_factor_change, names=\"value\"\n",
    ")  # Now linked to the correct function\n",
    "simper_dropdown_1.observe(\n",
    "    on_simper_change, names=\"value\"\n",
    ")  # Assuming on_simper_change is defined\n",
    "simper_dropdown_2.observe(\n",
    "    on_simper_change, names=\"value\"\n",
    ")  # Assuming on_simper_change is defined\n",
    "\n",
    "\n",
    "# Display widgets and output areas\n",
    "display(\n",
    "    distance_metric_dropdown,\n",
    "    output_permanova,\n",
    "    output_pcoa,\n",
    "    factor_dropdown,\n",
    "    simper_dropdown_1,\n",
    "    simper_dropdown_2,\n",
    "    output_simper,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
