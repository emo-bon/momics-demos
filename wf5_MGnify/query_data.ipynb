{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic interaction with MGnify\n",
    "1. search data\n",
    "2. get data\n",
    "3. compare to your data\n",
    "4. compare to ncbi data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connection to MGnify API\n",
    "\n",
    "# this repo is not maintained, or less than jsonapi-requests\n",
    "# consider a dep change\n",
    "from jsonapi_client import Session as APISession\n",
    "from jsonapi_client import Modifier\n",
    "import requests\n",
    "\n",
    "# Dataframes and display\n",
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Data transformation\n",
    "from functools import reduce\n",
    "\n",
    "# Plots\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# import plotly.graph_objects as go\n",
    "%matplotlib inline \n",
    "\n",
    "# Create signature of MAGs for comparison against database\n",
    "import sourmash\n",
    "import glob\n",
    "import time\n",
    "from pathlib import PurePath as pp\n",
    "from Bio import SeqIO\n",
    "\n",
    "# Warning verbosity\n",
    "import warnings \n",
    "warnings.filterwarnings(action=\"ignore\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query genomes\n",
    "\n",
    "For the genome dataset, use genomes endpoint.\n",
    "A complete list of endpoints can be found at https://www.ebi.ac.uk/metagenomics/api/v1/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name = 'genomes'\n",
    "r = requests.get(f\"https://www.ebi.ac.uk/metagenomics/api/v1/{endpoint_name}\")\n",
    "r.json()['data'][0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Get information for a specific genus or species\n",
    "\n",
    "Examples: Search for available ressources for a specific genus or species of interest.\n",
    "\n",
    "- Listeria\n",
    "- Listeria monocytogenes\n",
    "\n",
    "The taxon-lineage field contains domain, phylum, class, order, family, genus, species, subspecies as\n",
    "\n",
    "`d__Bacteria;p__Firmicutes;c__Bacilli;o__Lactobacillales;f__Listeriaceae;g__Listeria;s__Listeria` monocytogenes(example for Listeria monocytogenes).\n",
    "\n",
    "The filter can use the full lineage\n",
    "`d__Bacteria;p__Firmicutes;c__Bacilli;o__Lactobacillales;f__Listeriaceae;g__Listeria` or only part of it `g__Listeria` or `Listeria`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genus_filter = 'Listeria'\n",
    "species_filter = 'Listeria monocytogenes'\n",
    "\n",
    "with APISession(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    search_filter = Modifier(f\"taxon_lineage={genus_filter}\")\n",
    "    resources = map(lambda r: r.json, mgnify.iterate(endpoint_name, filter=search_filter))\n",
    "    resources_df = pd.json_normalize(resources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the table containing the results of the query\n",
    "resources_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Query the database with the 'Listeria monocytogenes' filter and store the results in a Pandas DataFrame.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with APISession(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    search_filter_2 = Modifier(f\"taxon_lineage={species_filter}\")\n",
    "    resources_2 = map(lambda r: r.json, mgnify.iterate(endpoint_name, filter=search_filter_2))\n",
    "    resources_df_2 = pd.json_normalize(resources_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_df_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resources_df.to_parquet('Listeria_resources.parquet')\n",
    "listeria_df = pd.read_parquet('Listeria_resources.parquet')\n",
    "listeria_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Query and save the dataset as parquet file\n",
    "\n",
    "To query the whole dataset, we can use the same method as previously. The only difference is that no filter is passed to the query.\n",
    "\n",
    "Warning: Querying without filter is computationally expensive and will take time.\n",
    "\n",
    "A pre-fetched copy of the data (as of 8 November 2022) is available in ../example-data/genomes/all_genome_resources.parquet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Careful, this takes a while to run\n",
    "# on the order level, the query takes \n",
    "domain_filter = 'Bacteria'\n",
    "order_filter = 'Lactobacillales'\n",
    "\n",
    "with APISession(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    search_filter_3 = Modifier(f\"taxon_lineage={order_filter}\")\n",
    "    resources_all = map(lambda r: r.json, mgnify.iterate(endpoint_name, filter=search_filter_3))\n",
    "    resources_all_df = pd.json_normalize(resources_all)\n",
    "\n",
    "resources_all_df\n",
    "resources_all_df.to_parquet('latest_genome_resources.parquet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark session to load all that data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyspark.conf.SparkConf().set(\"spark.sql.debug.maxToStringFields\", 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genomes_df = spark.read.parquet('latest_genome_resources.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genomes_df.count(), len(all_genomes_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genomes_df.describe().show(truncate=False, vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get most represented genus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see a sample of taxon-lineages present in the dataset:\n",
    "all_genomes_df.select(f'`attributes.taxon-lineage`').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The total number of genomes in the dataset:\n",
    "all_genomes_df.select('`id`').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of distinct lineages:\n",
    "all_genomes_df.select('`attributes.taxon-lineage`').distinct().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Split taxon-lineage column into 7 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['domain', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "\n",
    "all_genomes_tax_df = reduce(lambda df, i: df.withColumn(features[i], F.col('lineage_split')[i]),\n",
    "    range(len(features)),\n",
    "    all_genomes_df.withColumn('lineage_split', F.split(F.col('`attributes.taxon-lineage`'), ';')),\n",
    ")\n",
    "\n",
    "all_genomes_tax_df.select(features).show(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Query examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To search the most represented taxon:\n",
    "all_genomes_tax_df.groupby('`attributes.taxon-lineage`').count().filter(F.col('count')>100).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To search for a particular lineage and count how many times it appears:\n",
    "all_genomes_tax_df.filter(F.col('`attributes.taxon-lineage`').startswith('d__Bacteria;p__Actinobacteriota;c__Coriobacteriia;o__Coriobacteriales;f__Coriobacteriaceae;g__Collinsella')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To search for a particular genus and count how many times it appears:\n",
    "all_genomes_tax_df.filter(F.col('`attributes.taxon-lineage`').contains('Collinsella')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To search for the most or least represented genus, species, ... in this dataset for example. The search is more flexible than for the full taxon.\n",
    "all_genomes_tax_df.groupby('genus').count().filter(F.col('count')>100).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genomes_tax_df.filter(F.col('genus').isin('g__Prevotella', 'g__RC9', 'g__Collinsella')).groupby('genus').agg(F.countDistinct('species')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To see some of the Collinsella species in the dataset:\n",
    "all_genomes_tax_df.filter(F.col('genus')=='g__Collinsella').select('species').distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graphics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genomes_tax_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_genomes_tax_df.select([F.count_distinct(x).alias(f'{features[i]}_count') for i, x in enumerate([*features])]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import holoviews as hv\n",
    "hv.extension('bokeh')\n",
    "\n",
    "def get_sankey(df, cat_cols=[], value_cols='', title='Sankey Diagram'):\n",
    "    # Colors\n",
    "    colorPalette = ['rgba(31, 119, 180, 0.8)',\n",
    "     'rgba(255, 127, 14, 0.8)',\n",
    "     'rgba(44, 160, 44, 0.8)',\n",
    "     'rgba(214, 39, 40, 0.8)',\n",
    "     'rgba(148, 103, 189, 0.8)',\n",
    "     'rgba(140, 86, 75, 0.8)',\n",
    "     'rgba(227, 119, 194, 0.8)',\n",
    "     'rgba(127, 127, 127, 0.8)']\n",
    "    labelList = []\n",
    "    colorNumList = []\n",
    "    for catCol in cat_cols:\n",
    "        labelListTemp =  list(set(df[catCol].values))\n",
    "        colorNumList.append(len(labelListTemp))\n",
    "        labelList = labelList + labelListTemp\n",
    " \n",
    "    # remove duplicates from labelList\n",
    "    labelList = list(dict.fromkeys(labelList))\n",
    " \n",
    "    # define colors based on number of levels\n",
    "    colorList = []\n",
    "    for idx, colorNum in enumerate(colorNumList):\n",
    "        colorList = colorList + [colorPalette[idx]]*colorNum\n",
    "\n",
    "    # transform df into a source-target pair\n",
    "    for i in range(len(cat_cols)-1):\n",
    "        if i==0:\n",
    "            sourceTargetDf = df[[cat_cols[i],cat_cols[i+1],value_cols]]\n",
    "            sourceTargetDf.columns = ['source','target','count']\n",
    "        else:\n",
    "            tempDf = df[[cat_cols[i],cat_cols[i+1],value_cols]]\n",
    "            tempDf.columns = ['source','target','count']\n",
    "            sourceTargetDf = pd.concat([sourceTargetDf,tempDf])\n",
    "        sourceTargetDf = sourceTargetDf.groupby(['source','target']).agg({'count':'sum'}).reset_index()\n",
    " \n",
    "    # add index for source-target pair\n",
    "    sourceTargetDf['sourceID'] = sourceTargetDf['source'].apply(lambda x: labelList.index(x))\n",
    "    sourceTargetDf['targetID'] = sourceTargetDf['target'].apply(lambda x: labelList.index(x))\n",
    " \n",
    "    # creating data for the sankey diagram\n",
    "    data = dict(\n",
    "        type='sankey',\n",
    "        node = dict(\n",
    "            pad = 15,\n",
    "            thickness = 20,\n",
    "            line = dict(\n",
    "                color = \"black\",\n",
    "                width = 0.5\n",
    "            ),\n",
    "            label = labelList,\n",
    "            color = colorList\n",
    "        ),\n",
    "        link = dict(\n",
    "            source = sourceTargetDf['sourceID'],\n",
    "            target = sourceTargetDf['targetID'],\n",
    "            value = sourceTargetDf['count']\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    # override gray link colors with 'source' colors\n",
    "    opacity = 0.4\n",
    "    # change 'magenta' to its 'rgba' value to add opacity\n",
    "    data['node']['color'] = ['rgba(255,0,255, 0.8)' if color == \"magenta\" else color for color in data['node']['color']]\n",
    "    data['link']['color'] = [data['node']['color'][src].replace(\"0.8\", str(opacity))\n",
    "                                        for src in data['link']['source']]\n",
    "    \n",
    "    \n",
    "    fig = go.Figure(data=[go.Sankey(\n",
    "    # Define nodes\n",
    "    node = dict(\n",
    "      pad = 15,\n",
    "      thickness = 15,\n",
    "      line = dict(color = \"black\", width = 0.5),\n",
    "      label =  data['node']['label'],\n",
    "      color =  data['node']['color']\n",
    "    ),\n",
    "    # Add links\n",
    "    link = dict(\n",
    "      source =  data['link']['source'],\n",
    "      target =  data['link']['target'],\n",
    "      value =  data['link']['value'],\n",
    "      color =  data['link']['color']\n",
    "    ))])\n",
    "    \n",
    "    fig.update_layout(title_text=title, font_size=10)\n",
    "    \n",
    "    return fig.show(renderer='iframe')\n",
    "\n",
    "\n",
    "# To create a Sankey diagram for the taxon lineage:\n",
    "def sankey_hv():\n",
    "    sankey_df = all_genomes_tax_df.groupby(features).count().toPandas()\n",
    "    sankey_df = sankey_df.rename(columns={'count': 'value'})\n",
    "    edges = pd.read_csv('data/health-breakup2.csv')\n",
    "    sankey = hv.Sankey(edges, label='A Breakout of National Health Care Expenditures')\n",
    "    sankey.opts(label_position='left', edge_color='target', node_color='index', cmap='tab20')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sankey_df = all_genomes_tax_df.groupby(features).count().toPandas()\n",
    "sankey_df = sankey_df.rename(columns={'count': 'value'})\n",
    "sankey_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sankey = hv.Sankey(sankey_df[['phylum', \"species\", \"value\"]], label='Taxon Lineage')\n",
    "sankey.opts(width=600, height=400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Spark DataFrame to Pandas DataFrame:\n",
    "pdf = all_genomes_tax_df.select(features).groupby(features).count().toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Representation of a sample of the genomes present in the dataset: example from the order of the Lactobacillales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_lactobacillales = all_genomes_tax_df.filter(F.col('order')=='o__Lactobacillales').select(features).groupby(features).count().toPandas()\n",
    "# fig_l = get_sankey(pdf_lactobacillales,cat_cols=features[0:6], value_cols='count',title='Genomes from the Lactobacillales order')\n",
    "\n",
    "# Note that there are too many distinct species in the Lactobacillales order to show individually:\n",
    "all_genomes_tax_df.filter(F.col('order')=='o__Lactobacillales').select('species').distinct().count()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Information such as genome length or GC-content can also be represented\n",
    "\n",
    "We can group and visualise these at different levels like family, genus, species... depending on the number of sequences available and on the biological significance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lactobacillales_df = all_genomes_tax_df.filter(F.col('order')=='o__Lactobacillales').orderBy('family').toPandas()\n",
    "lactobacillales_count = all_genomes_tax_df.filter(F.col('order')=='o__Lactobacillales').groupby('family').count().orderBy('family').toPandas()\n",
    "lactobacillales_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 10), layout=\"constrained\")\n",
    "spec = fig.add_gridspec(3, 1)\n",
    "\n",
    "ax00 = fig.add_subplot(spec[0, 0])\n",
    "sns.barplot(data=lactobacillales_count, x='family', y='count')\n",
    "plt.ylabel(\"Number of genome available\")\n",
    "\n",
    "ax10 = fig.add_subplot(spec[1, 0])\n",
    "sns.boxplot(data=lactobacillales_df, x='family', y='attributes.length')\n",
    "plt.ylabel(\"Genome length (bp)\")\n",
    "#plt.xlabel(\"Family of the Lactobacillales order\")\n",
    "\n",
    "ax20 = fig.add_subplot(spec[2, 0])\n",
    "sns.boxplot(data=lactobacillales_df, x='family', y='attributes.gc-content')\n",
    "plt.ylabel(\"GC-content (%)\")\n",
    "plt.xlabel(\"Family of the Lactobacillales order\")\n",
    "\n",
    "\n",
    "fig.suptitle('Number of genomes avalaible, genome length and GC-content of bacteria belonging the Lactobacillales order')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(20, 5))\n",
    "spec = fig.add_gridspec(1, 2)\n",
    "\n",
    "#ax00 = fig.add_subplot(spec[0, 0])\n",
    "#lactobacillales_df['relationships.biome.data.id'].hist()\n",
    "#plt.xlabel(\"Biome\")\n",
    "\n",
    "ax01 = fig.add_subplot(spec[0:])\n",
    "lactobacillales_df['relationships.catalogue.data.id'].hist()\n",
    "plt.xlabel(\"Catalogue\")\n",
    "ax01.grid(False)\n",
    "\n",
    "fig.suptitle('Biome and Catalogue related to bacteria belonging the Lactobacillales order')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another example: produce a quality control figure similar to Extended Data Fig. 4a of [Almeida et al 2020](https://www.nature.com/articles/s41587-020-0603-3/figures/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_df = all_genomes_tax_df.toPandas()\n",
    "qc_df[['attributes.completeness', 'attributes.contamination']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(5, 10), layout=\"constrained\")\n",
    "spec = fig.add_gridspec(1, 1)\n",
    "\n",
    "ax00 = fig.add_subplot(spec[0, 0])\n",
    "sns.boxplot(data=qc_df[['attributes.completeness', 'attributes.contamination']])\n",
    "plt.ylabel(\"%\")\n",
    "\n",
    "\n",
    "fig.suptitle('Quality of genomes avalaible')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Find out whether your own MAGs are novel compared to the MGnify catalogues\n",
    "\n",
    "Another use for the MGnify genomes resource is to query your own MAG against MGnify's MAG catalogues, to see whether they are novel or already represented.\n",
    "List directories of the files to be analysed:\n",
    "\n",
    "Replace the str with your own path to folder containing your files. * allows to query all the file with the .fa extension.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('../data/input_gecco/*.fa')\n",
    "files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Compute a sourmash sketch for each MAG\n",
    "\n",
    "Create \"sketches\" for each MAG using Sourmash\n",
    "\n",
    "A sketch goes into a signature, that we will use for searching. The signature is a sort of collection of hashes that are well suited for calculating the containment of your MAGs within the catalogue's MAGs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for mag in files:\n",
    "    # The sourmash parameters are chosen to match those used within MGnify\n",
    "    sketch = sourmash.MinHash(n=0, ksize=31, scaled=1000)\n",
    "    \n",
    "    # A fasta file may have multiple records in it. Add them all to the sourmash signature.\n",
    "    for index, record in enumerate(SeqIO.parse(mag, 'fasta')):\n",
    "        sketch.add_sequence(str(record.seq))\n",
    "        \n",
    "    # Save the sourmash sketch as a \"signature\" file\n",
    "    signature = sourmash.SourmashSignature(sketch, name=record.name)\n",
    "    with open(pp(pp(mag).name).stem + '.sig', 'wt') as fp:\n",
    "        sourmash.save_signatures([signature], fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Fetch all of the catalogue IDs currently available on MGnify\n",
    "\n",
    "To fetch the catalogue IDs to the MGnify API, use the following endpoint: https://www.ebi.ac.uk/metagenomics/api/v1/genome-catalogues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_endpoint = \"genome-catalogues\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "with APISession(\"https://www.ebi.ac.uk/metagenomics/api/v1\") as mgnify:\n",
    "    catalogues = map(lambda r: r.json, mgnify.iterate(catalogue_endpoint))\n",
    "    catalogues = pd.json_normalize(catalogues)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "catalogue_ids = list(catalogues['id'])\n",
    "catalogue_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Submit a search job to the MGnify API\n",
    "\n",
    "Tosubmit a job to the MGnify API, use the following endpoint: https://www.ebi.ac.uk/metagenomics/api/v1/genomes-search/gather.\n",
    "Data will be send to the API, which is called \"POST\"ing data in the API world.\n",
    "This part of the API is quite specialized and so is not a formal JSON:API, the requests Python packageìs therefore used to communicate with it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = 'https://www.ebi.ac.uk/metagenomics/api/v1/genomes-search/gather'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of file uploads, and attach them to the API request\n",
    "signatures = [open(sig, 'rb') for sig in glob.glob('*.sig')]\n",
    "sketch_uploads = [('file_uploaded', signature) for signature in signatures]\n",
    "\n",
    "# Send the API request - it specifies which catalogue to search against and attaches all of the signature files.\n",
    "submitted_job = requests.post(endpoint, data={'mag_catalogues': catalogue_ids}, files=sketch_uploads).json()\n",
    "\n",
    "\n",
    "map(lambda fp: fp.close(), signatures)  # tidy up open file pointers\n",
    "\n",
    "print(submitted_job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_done = False\n",
    "while not job_done:\n",
    "    print('Checking status...')\n",
    "    # The status_URL is another API endpoint that's unique for the submitted search job\n",
    "    query_result = None\n",
    "    \n",
    "    while not query_result:\n",
    "        query_result = requests.get(submitted_job['data']['status_URL'])\n",
    "        print('Still waiting for jobs to complete. Current status of jobs')\n",
    "        print('Will check again in 2 seconds')\n",
    "        time.sleep(2) \n",
    "        \n",
    "    queries_status = {sig['job_id']: sig['status'] for sig in query_result.json()['data']['signatures']}\n",
    "    job_done = all(map(lambda q: q == 'SUCCESS', queries_status.values()))\n",
    "    \n",
    "print('Job done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result_df = pd.json_normalize(query_result.json()['data']['signatures'])\n",
    "query_result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = query_result_df.dropna(subset=['result.match'])\n",
    "matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momicsdem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
