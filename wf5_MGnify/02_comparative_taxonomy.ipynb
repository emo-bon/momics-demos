{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f53f4f5e",
   "metadata": {},
   "source": [
    "## Compare EMO-BON metaGOflow outputs to other MGnify studies\n",
    "\n",
    "- This is also published as jupyter book [chapter](https://lab.fairease.eu/book-marine-omics-observation/comparative-taxonomy-campaigns/).\n",
    "\n",
    "1. Study MGYS00006608, 16S rRNA amplicon sequencing from the Ocean Sampling Day (OSD) campaign June 2018 (ERP124424_taxonomy_abundances_SSU_v5.0)\n",
    "2. Study MGYS00006607, 16S rRNA amplicon sequencing from the Ocean Sampling Day (OSD) campaign June 2019 (ERP124431_taxonomy_abundances_SSU_v5.0)\n",
    "3. Study MGYS00000492, Amplicon sequencing of Tara Oceans DNA samples corresponding to size fractions for prokaryotes or protist. (ERP003634_taxonomy_abundances_SSU_v5.0)\n",
    "4. Study MGYS00006680, SOLA sampling point Raw sequence reads (SRP237882_taxonomy_abundances_SSU_v5.0)\n",
    "5. Study MGYS00006682, Vertical stratification of environmental DNA in the open ocean captures ecological patterns and behavior of deep-sea fishes (SRP334933_taxonomy_abundances_SSU_v5.0)\n",
    "6. Study MGYS00006678, Dataset on spatiotemporal variation of microbial plankton communities in the Baltic Sea (ERP140185_taxonomy_abundances_SSU_v5.0)\n",
    "7. Study MGYS00006675, 16S rRNA gene amplicon time-series in Blanes Bay Microbial Observatory (BBMO) (ERP122219_taxonomy_abundances_SSU_v5.0)\n",
    "8. Study MGYS00003725, Arctic microbiome along Svalbard Cross Shelf transects (ERP106348_taxonomy_abundances_SSU_v5.0)\n",
    "9. Study MGYS00006686, Environmental DNA and zooplankton samples taken at Helgoland Roads in June 2019 (ERP144826_taxonomy_abundances_SSU_v5.0)\n",
    "10. Study MGYS00006714, Regional and vertical patterns in microbial communities across Fram Strait (2015-2019) (ERP151329_taxonomy_abundances_SSU_v5.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f0f2e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This needs to be repeated here for the Pannel dashboard to work, WEIRD\n",
    "# TODO: report as possible bug\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "from functools import partial\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "import panel as pn\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# All low level functions are imported from the momics package\n",
    "from momics.loader import load_parquets_udal\n",
    "from momics.metadata import get_metadata_udal, enhance_metadata\n",
    "\n",
    "from momics.taxonomy import (\n",
    "    fill_taxonomy_placeholders,\n",
    "    pivot_taxonomic_data,\n",
    "    prevalence_cutoff,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0935f",
   "metadata": {},
   "source": [
    "## Loading EMO-BON (meta)data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ff6b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parquet files\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    root_folder = os.path.abspath(os.path.join('/content/momics-demos'))\n",
    "else:\n",
    "    root_folder = os.path.abspath(os.path.join('../'))\n",
    "\n",
    "assets_folder = os.path.join(root_folder, 'assets')\n",
    "data_folder = os.path.join(root_folder, 'data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daef4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.cache()\n",
    "def get_data():\n",
    "    return load_parquets_udal()\n",
    "\n",
    "# Load and merge metadata\n",
    "@pn.cache()\n",
    "def get_full_metadata():\n",
    "    return get_metadata_udal()\n",
    "\n",
    "@pn.cache()\n",
    "def get_valid_samples():\n",
    "    df_valid = pd.read_csv(\n",
    "        os.path.join(root_folder, 'data/shipment_b1b2_181.csv')\n",
    "    )\n",
    "    return df_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f22dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load metadata\n",
    "full_metadata = get_full_metadata()\n",
    "\n",
    "# filter the metadata only for valid 181 samples\n",
    "valid_samples = get_valid_samples()\n",
    "full_metadata = enhance_metadata(full_metadata, valid_samples)\n",
    "\n",
    "# LOADing data\n",
    "mgf_parquet_dfs = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cf6d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only ssu\n",
    "ssu = mgf_parquet_dfs['ssu'].copy()\n",
    "\n",
    "del mgf_parquet_dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c081652",
   "metadata": {},
   "source": [
    "### Pivot EMO-BON data\n",
    "MGnify tables are already abundance pivoted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf63f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tax_row(row):\n",
    "    \"\"\"\n",
    "    Cleans the taxonomic row by removing empty strings and replacing spaces with underscores.\n",
    "    \"\"\"\n",
    "    # replace string with underscores\n",
    "    row = row.replace('_', '__')\n",
    "    split_row = row.split(';')\n",
    "    res = [split_row[1]]\n",
    "    # print(split_row)\n",
    "    for tax in split_row[3:]:\n",
    "        if tax[-1] == '_':\n",
    "            break\n",
    "        res.append(tax)\n",
    "    return ';'.join(res)\n",
    "\n",
    "def clean_tax_row_mgnify(row):\n",
    "    \"\"\"\n",
    "    Cleans the taxonomic row by removing empty strings and replacing spaces with underscores.\n",
    "    \"\"\"\n",
    "    split_row = row.split(';')\n",
    "    res = [split_row[0]]\n",
    "    # print(split_row)\n",
    "    for tax in split_row[2:]:\n",
    "        if tax[-1] == '_':\n",
    "            break\n",
    "        res.append(tax)\n",
    "    return ';'.join(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d7df08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "TAXONOMY_RANKS = ['superkingdom', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species']\n",
    "ssu_filt = fill_taxonomy_placeholders(ssu, TAXONOMY_RANKS)\n",
    "ssu_filt = pivot_taxonomic_data(ssu_filt, normalize=None, rarefy_depth=None)\n",
    "\n",
    "# remove tax id\n",
    "ssu_filt = ssu_filt.drop(columns=['ncbi_tax_id'])\n",
    "\n",
    "ssu_filt['taxonomic_concat'] = ssu_filt['taxonomic_concat'].apply(clean_tax_row)\n",
    "# rename columns\n",
    "ssu_filt = ssu_filt.rename(columns={\n",
    "    'taxonomic_concat': '#SampleID',\n",
    "})\n",
    "ssu_filt.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c0d5b5",
   "metadata": {},
   "source": [
    "## Load Pre-downloaded MGnify datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9f87e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_list = {\n",
    "    'OSD-2018': 'mgnify_data/ERP124424_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "    'OSD-2019': 'mgnify_data/ERP124431_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "    'Tara': 'mgnify_data/ERP003634_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "    'Sola': 'mgnify_data/SRP237882_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "    'Biscay': 'mgnify_data/SRP334933_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "    'Baltic': 'mgnify_data/ERP140185_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "    'BBMO': 'mgnify_data/ERP122219_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "    'Svalbard': 'mgnify_data/ERP106348_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "    'Helgoland': 'mgnify_data/ERP144826_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "    'Fram': 'mgnify_data/ERP151329_taxonomy_abundances_SSU_v5.0.tsv',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c5ae4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = {}\n",
    "for key, value in ds_list.items():\n",
    "    df = pd.read_csv(os.path.join(data_folder, value), sep='\\t')\n",
    "    df['#SampleID'] = df['#SampleID'].apply(clean_tax_row_mgnify)\n",
    "    print(key, df.shape)\n",
    "    ds[key] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bcb6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: split pivot_taxonomic_data function in the marine_omics_methods, it is a subfunc of that function\n",
    "def normalize_taxonomy(df, method: str = 'tss'):\n",
    "    \"\"\"\n",
    "    Normalize the taxonomy dataframe by removing high taxa and applying prevalence cutoff.\n",
    "    \"\"\"\n",
    "    if method == 'tss':\n",
    "        df.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: x / x.sum())\n",
    "    elif method == 'tss_sqrt':\n",
    "        df.iloc[:, 1:] = df.iloc[:, 1:].apply(lambda x: (x / x.sum()) ** 0.5)\n",
    "    else:\n",
    "        raise ValueError(\"Normalization method not recognized. Use 'tss' or 'tss_sqrt'.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f94f0e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add emo-bon\n",
    "ds['EMO-BON'] = ssu_filt.copy()\n",
    "\n",
    "ds_normalized = {}\n",
    "for key, value in ds.items():\n",
    "    df = value.copy()\n",
    "    df = prevalence_cutoff(df, percent=0.1, skip_columns=1)\n",
    "    df = normalize_taxonomy(df, method='tss')\n",
    "    ds_normalized[key] = df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3de7b88",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "### Stacked barplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e979898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "chash = []\n",
    "for i, (label, df_orig) in enumerate(ds_normalized.items()):\n",
    "    df = df_orig.copy()\n",
    "    # first sum off the samples ie columns\n",
    "    df['sum'] = df.iloc[:, 1:].sum(axis=1) / (len(df.columns)-1) *100\n",
    "    # then sort by sum\n",
    "    df = df.sort_values(by='sum', ascending=False)\n",
    "\n",
    "    # keep only the top X species\n",
    "    df = df.head(5)\n",
    "\n",
    "    for j, val in enumerate(df['#SampleID']):\n",
    "        chash.append(val.split(\";\")[-1])\n",
    "chash = list(set(chash))\n",
    "chash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db16ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose a colormap and normalize the color range\n",
    "cmap = plt.get_cmap('jet')\n",
    "norm = plt.Normalize(0, len(chash) - 1)\n",
    "\n",
    "# Map each item to a color\n",
    "color_dict = {name: cmap(norm(i)) for i, name in enumerate(chash)}\n",
    "\n",
    "# Example: print or use a color\n",
    "print(color_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5847d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots()\n",
    "x_positions = np.arange(len(ds_normalized))  # one bar per DataFrame\n",
    "bar_width = 0.5\n",
    "\n",
    "xlabels = []\n",
    "for i, (label, df_orig) in enumerate(ds_normalized.items()):\n",
    "    bottom = 0\n",
    "    df = df_orig.copy()\n",
    "    # first sum off the samples ie columns\n",
    "    df['sum'] = df.iloc[:, 1:].sum(axis=1) / (len(df.columns)-1) *100\n",
    "    # print(df['sum'].sum(), len(df.columns)-2)\n",
    "    # then sort by sum\n",
    "    df = df.sort_values(by='sum', ascending=False)\n",
    "\n",
    "    # keep only the top X species\n",
    "    df = df.head(5)\n",
    "\n",
    "    for j, val in enumerate(df['sum']):\n",
    "        ax.bar(x_positions[i], val, bottom=bottom, width=bar_width,\n",
    "               color=color_dict[df['#SampleID'].iloc[j].split(\";\")[-1]],\n",
    "        )\n",
    "        bottom += val\n",
    "    xlabels.append(label)\n",
    "\n",
    "# manual legend\n",
    "handles = [plt.Rectangle((0,0),1,1, color=color_dict[name]) for name in chash]\n",
    "ax.legend(handles, chash, loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "ax.set_xticks(x_positions)\n",
    "ax.set_xticklabels(xlabels, rotation=45, ha='right')\n",
    "\n",
    "ax.set_ylabel('Relative abundance in all samples pooled [%]')\n",
    "ax.set_xlabel('Dataset')\n",
    "ax.set_title('Most abundant taxa in each dataset')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99361f9e",
   "metadata": {},
   "source": [
    "### Alpha diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85988536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start from the DS dictionary\n",
    "from momics.diversity import calculate_shannon_index\n",
    "\n",
    "for k, v in ds.items():\n",
    "    df = v.copy().T\n",
    "    shannon_vals = calculate_shannon_index(df)\n",
    "    plt.plot(shannon_vals, 'o', alpha=0.5, label=f'{k}-av {shannon_vals.mean():.2f}')\n",
    "plt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "plt.xticks([])\n",
    "plt.xlabel('Sample')\n",
    "plt.ylabel('Shannon index')\n",
    "plt.title('Alpha diversity (Shannon index)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b156020a",
   "metadata": {},
   "source": [
    "### Beta diversity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79557335",
   "metadata": {},
   "outputs": [],
   "source": [
    "from skbio.diversity import beta_diversity\n",
    "import seaborn as sns\n",
    "from skbio.stats.ordination import pcoa\n",
    "\n",
    "pcoa_res, explained_var = {}, {}\n",
    "\n",
    "fig, ax = plt.subplots(4, 3, figsize=(18, 10))\n",
    "# starts from the normalized DS dictionary\n",
    "for i, (k, v) in enumerate(ds.items()):\n",
    "    df = v.set_index('#SampleID').copy().T\n",
    "    beta = beta_diversity('braycurtis', df)\n",
    "    #order beta\n",
    "    df_beta = beta.to_data_frame()\n",
    "\n",
    "    # this is for later use in PCoA\n",
    "    pcoa_result = pcoa(df_beta, method=\"eigh\")\n",
    "    pcoa_res[k] = pcoa_result\n",
    "    explained_var[k] = (\n",
    "        pcoa_result.proportion_explained[0],\n",
    "        pcoa_result.proportion_explained[1],\n",
    "    )\n",
    "\n",
    "    sums = df_beta.sum(axis=1)\n",
    "\n",
    "    # Sort index by sum\n",
    "    sorted_idx = sums.sort_values(ascending=False).index\n",
    "\n",
    "    # Reorder both rows and columns\n",
    "    corr_sorted = df_beta.loc[sorted_idx, sorted_idx]\n",
    "    curr_ax = ax.flatten()[i]\n",
    "    sns.heatmap(corr_sorted, cmap=\"YlGnBu\", ax=curr_ax)\n",
    "    curr_ax.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5))\n",
    "    curr_ax.set_xticks([])\n",
    "    curr_ax.set_ylabel('Sample')\n",
    "    curr_ax.set_title(f'Beta div., {k}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334be831",
   "metadata": {},
   "source": [
    "### PCoA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e355e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 4, figsize=(25, 16))\n",
    "\n",
    "for i, (k, v) in enumerate(pcoa_res.items()):\n",
    "    curr_ax = ax.flatten()[i]\n",
    "\n",
    "    sns.scatterplot(\n",
    "        data=v.samples,\n",
    "        x=\"PC1\",\n",
    "        y=\"PC2\",\n",
    "        ax=curr_ax,\n",
    "    )\n",
    "    curr_ax.set_xlabel(f\"PC1 ({explained_var[k][0]*100:.2f})\")\n",
    "    curr_ax.set_ylabel(f\"PC2 ({explained_var[k][1]*100:.2f})\")\n",
    "    curr_ax.set_title(f\"PCoA, Bray-Curtis - {k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df80efc",
   "metadata": {},
   "source": [
    "### Metric multidimensional scaling (MDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b80fdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from scipy.spatial.distance import pdist, squareform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77e1a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 4, figsize=(25, 16))\n",
    "\n",
    "for i, (k, v) in enumerate(ds.items()):\n",
    "    curr_ax = ax.flatten()[i]\n",
    "    df = v.set_index('#SampleID').copy().T\n",
    "\n",
    "    # Step 1: Calculate Bray-Curtis distance matrix\n",
    "    # Note: pdist expects a 2D array, so we use df.values\n",
    "    dist_matrix = pdist(df.values, metric='braycurtis')\n",
    "    dist_square = squareform(dist_matrix)\n",
    "\n",
    "    # Step 3: Run MDS\n",
    "    mds = MDS(n_components=2, dissimilarity='precomputed', random_state=42)\n",
    "    coords = mds.fit_transform(dist_square)\n",
    "\n",
    "    # Step 4: Plot\n",
    "    curr_ax.scatter(coords[:, 0], coords[:, 1], s=50)\n",
    "\n",
    "    # Optional: label samples\n",
    "    # for i, sample_id in enumerate(df.index):\n",
    "    #     curr_ax.text(coords[i, 0], coords[i, 1], sample_id, fontsize=8)\n",
    "\n",
    "    curr_ax.set_title(\"MDS of Bray-Curtis - \" + k)\n",
    "    curr_ax.set_xlabel(\"MDS1\")\n",
    "    curr_ax.set_ylabel(\"MDS2\")\n",
    "    # curr_ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e87287",
   "metadata": {},
   "source": [
    "### Non-metric multidimensional scaling (NMDS)\n",
    "- does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc2696",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, 4, figsize=(25, 16))\n",
    "\n",
    "for i, (k, v) in enumerate(ds.items()):\n",
    "    curr_ax = ax.flatten()[i]\n",
    "    df = v.set_index('#SampleID').copy().T\n",
    "\n",
    "    # Step 1: Calculate Bray-Curtis distance matrix\n",
    "    # Note: pdist expects a 2D array, so we use df.values\n",
    "    dist_matrix = pdist(df.values, metric='braycurtis')\n",
    "    dist_square = squareform(dist_matrix)\n",
    "\n",
    "    # Step 3: Run NMDS\n",
    "    nmds = MDS(n_components=2, dissimilarity='precomputed',\n",
    "               metric=False,  # this is the non-metric part\n",
    "               random_state=42)\n",
    "    coords = nmds.fit_transform(dist_square)\n",
    "    coords *= np.sqrt((df.values**2).sum()) / np.sqrt((coords**2).sum())\n",
    "\n",
    "    # Step 4: Plot\n",
    "    curr_ax.scatter(coords[:, 0], coords[:, 1], s=50)\n",
    "\n",
    "    curr_ax.set_title(\"NMDS of Bray-Curtis - \" + k)\n",
    "    curr_ax.set_xlabel(\"NMDS1\")\n",
    "    curr_ax.set_ylabel(\"NMDS2\")\n",
    "    # curr_ax.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ec33b92",
   "metadata": {},
   "source": [
    "## Venn diagram of taxonomic IDs overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69095d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from venny4py.venny4py import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f18f308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict of sets\n",
    "sets = {\n",
    "    'EMO-BON': set(ds_normalized['EMO-BON']['#SampleID'].values),\n",
    "    'OSD 2018': set(ds_normalized['OSD-2018']['#SampleID'].values),\n",
    "    'OSD 2019': set(ds_normalized['OSD-2019']['#SampleID'].values),\n",
    "    'SOLA': set(ds_normalized['Sola']['#SampleID'].values)\n",
    "}\n",
    "\n",
    "fig = venny4py(sets=sets, out = 'venn4_studies')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f094d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "momics-demos",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
